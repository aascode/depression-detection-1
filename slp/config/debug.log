[2020-01-06 19:26:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 19:26:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 19:26:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 19:26:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 19:29:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 19:29:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 19:31:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 19:31:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 19:32:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 19:32:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 19:32:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-06 19:33:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-06 19:33:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f86dc98b278>,), **{}) took: 35.44786787033081 sec
[2020-01-06 19:33:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-06 19:33:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-06 19:41:55] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-06 19:41:55] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-06 19:41:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 19:41:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 19:42:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-06 19:42:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-06 19:42:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5aa5a47278>,), **{}) took: 2.201265811920166 sec
[2020-01-06 19:42:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-06 19:42:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-06 19:45:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-06 19:45:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-06 20:45:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:45:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:45:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:45:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:47:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:47:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:49:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:49:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:49:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-06 20:49:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-06 20:49:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f17bdab12b0>,), **{}) took: 2.080927610397339 sec
[2020-01-06 20:50:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:50:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:50:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-06 20:50:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-06 20:50:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbf2ef0d2e8>,), **{}) took: 2.0268912315368652 sec
[2020-01-06 20:50:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-06 20:50:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-06 20:50:33] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '../../../data/lexicons_kate/AFINN/AFINN-111.txt'.
[2020-01-06 20:50:33] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '../../../data/lexicons_kate/AFINN/AFINN-111.txt'.
[2020-01-06 20:52:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:52:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:52:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-06 20:52:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-06 20:52:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5f3dbba8d0>,), **{}) took: 1.9730510711669922 sec
[2020-01-06 20:53:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-06 20:53:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-06 20:53:01] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '../../../data/lexicons_kate/AFINN/AFINN-111.txt'.
[2020-01-06 20:53:01] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '../../../data/lexicons_kate/AFINN/AFINN-111.txt'.
[2020-01-06 20:54:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:54:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:54:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-06 20:54:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-06 20:54:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f27f7e19898>,), **{}) took: 2.0707197189331055 sec
[2020-01-06 20:54:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-06 20:54:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-06 20:54:34] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-06 20:54:34] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-06 20:55:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-06 20:55:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-06 20:55:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-06 20:55:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-06 20:55:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1b23b3d898>,), **{}) took: 2.047318696975708 sec
[2020-01-06 20:55:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-06 20:55:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-06 20:55:14] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '/data/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-06 20:55:14] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '/data/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-07 19:30:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:30:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:42:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:42:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:42:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:42:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:43:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 19:43:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 19:43:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f30def004e0>,), **{}) took: 7.853026628494263 sec
[2020-01-07 19:43:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 19:43:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 19:43:27] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '/data/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-07 19:43:27] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '/data/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-07 19:52:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:52:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:52:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:52:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:52:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 19:52:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 19:52:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f18318f6c50>,), **{}) took: 10.58799409866333 sec
[2020-01-07 19:52:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 19:52:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 19:52:56] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '/data/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-07 19:52:56] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '/data/lexicon_kate/AFINN/AFINN-111.txt'.
[2020-01-07 19:54:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:54:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:54:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 19:54:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 19:54:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fac316c82e8>,), **{}) took: 16.303908824920654 sec
[2020-01-07 19:55:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 19:55:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 19:55:22] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: 'BASE_DIR/AFINN-111.txt'.
[2020-01-07 19:55:22] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: 'BASE_DIR/AFINN-111.txt'.
[2020-01-07 19:55:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:55:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:55:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 19:56:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 19:56:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5167b932e8>,), **{}) took: 13.769773721694946 sec
[2020-01-07 19:56:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 19:56:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 19:56:25] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 19:56:25] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 19:59:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 19:59:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 19:59:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 19:59:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 19:59:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3405d213c8>,), **{}) took: 10.718981981277466 sec
[2020-01-07 20:00:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:00:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:00:07] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 20:00:07] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 20:02:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:02:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:02:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:02:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:02:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa449bb2f98>,), **{}) took: 10.814828395843506 sec
[2020-01-07 20:02:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:02:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:02:37] - slp - ERROR -- Current run is terminating due to exception: name 'file' is not defined.
[2020-01-07 20:02:37] - slp - ERROR -- Engine run is terminating due to exception: name 'file' is not defined.
[2020-01-07 20:03:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:03:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:03:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:03:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:03:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f85e1e793c8>,), **{}) took: 10.238869428634644 sec
[2020-01-07 20:03:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:03:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:03:32] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 20:03:32] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 20:07:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:07:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:07:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:07:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:07:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0a67b203c8>,), **{}) took: 12.046745300292969 sec
[2020-01-07 20:07:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:07:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:07:37] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 20:07:37] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '../../data/AFINN-111.txt'.
[2020-01-07 20:09:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:09:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:09:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:09:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:09:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f85527e5390>,), **{}) took: 8.516173839569092 sec
[2020-01-07 20:09:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:09:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:09:22] - slp - ERROR -- Current run is terminating due to exception: list index out of range.
[2020-01-07 20:09:22] - slp - ERROR -- Engine run is terminating due to exception: list index out of range.
[2020-01-07 20:20:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:20:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:20:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:20:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:20:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0c517cc3c8>,), **{}) took: 2.265092372894287 sec
[2020-01-07 20:21:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:21:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:21:03] - slp - ERROR -- Current run is terminating due to exception: list index out of range.
[2020-01-07 20:21:03] - slp - ERROR -- Engine run is terminating due to exception: list index out of range.
[2020-01-07 20:27:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:27:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:27:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:28:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:28:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff85bf6e3c8>,), **{}) took: 14.205770492553711 sec
[2020-01-07 20:28:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:28:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:28:35] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: '../data/lexicons_kate/Bing_Liu_opinion_lex/negative-words.txt'.
[2020-01-07 20:28:35] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: '../data/lexicons_kate/Bing_Liu_opinion_lex/negative-words.txt'.
[2020-01-07 20:32:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:32:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:32:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:32:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:32:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7aa1e143c8>,), **{}) took: 1.9534120559692383 sec
[2020-01-07 20:32:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:32:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:32:48] - slp - ERROR -- Current run is terminating due to exception: [Errno 2] No such file or directory: './negative-words.txt'.
[2020-01-07 20:32:48] - slp - ERROR -- Engine run is terminating due to exception: [Errno 2] No such file or directory: './negative-words.txt'.
[2020-01-07 20:33:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:33:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:33:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:33:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:33:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7feb6f27a390>,), **{}) took: 1.8949470520019531 sec
[2020-01-07 20:33:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:33:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:34:30] - slp - ERROR -- Current run is terminating due to exception: forward() takes 4 positional arguments but 5 were given.
[2020-01-07 20:34:30] - slp - ERROR -- Engine run is terminating due to exception: forward() takes 4 positional arguments but 5 were given.
[2020-01-07 20:37:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:37:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:37:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:37:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:37:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f48b9cf93c8>,), **{}) took: 1.867140293121338 sec
[2020-01-07 20:37:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:37:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:39:39] - slp - ERROR -- Current run is terminating due to exception: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time..
[2020-01-07 20:39:39] - slp - ERROR -- Engine run is terminating due to exception: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time..
[2020-01-07 20:41:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:41:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:41:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:41:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:41:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f235a80a3c8>,), **{}) took: 1.8703951835632324 sec
[2020-01-07 20:41:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:41:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:43:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-07 20:43:37] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-07 20:50:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:50:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:51:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 20:51:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 20:51:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 20:51:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 20:51:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6f4979d2e8>,), **{}) took: 1.8961498737335205 sec
[2020-01-07 20:51:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 20:51:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 20:51:11] - slp - ERROR -- Current run is terminating due to exception: 'LexiconFeatures' object is not callable.
[2020-01-07 20:51:11] - slp - ERROR -- Engine run is terminating due to exception: 'LexiconFeatures' object is not callable.
[2020-01-07 21:07:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 21:07:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 21:07:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 21:07:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 21:07:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe40d0a42e8>,), **{}) took: 1.8697335720062256 sec
[2020-01-07 21:07:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 21:07:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 21:07:35] - slp - ERROR -- Current run is terminating due to exception: 'LexiconFeatures' object is not callable.
[2020-01-07 21:07:35] - slp - ERROR -- Engine run is terminating due to exception: 'LexiconFeatures' object is not callable.
[2020-01-07 21:11:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 21:11:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 21:11:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 21:11:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 21:11:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f466cac02e8>,), **{}) took: 1.8698744773864746 sec
[2020-01-07 21:11:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 21:11:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 21:11:28] - slp - ERROR -- Current run is terminating due to exception: name 'afinn' is not defined.
[2020-01-07 21:11:28] - slp - ERROR -- Engine run is terminating due to exception: name 'afinn' is not defined.
[2020-01-07 21:12:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 21:12:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 21:12:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 21:12:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 21:12:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4d114b32e8>,), **{}) took: 1.8685622215270996 sec
[2020-01-07 21:12:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 21:12:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 21:14:34] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-07 21:14:34] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-07 21:14:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 21:14:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 21:14:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 21:14:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 21:14:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f40863342e8>,), **{}) took: 1.8672335147857666 sec
[2020-01-07 21:15:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 21:15:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 21:16:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-07 21:16:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-07 21:16:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-07 21:16:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-07 21:16:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-07 21:17:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-07 21:17:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f91dc2d02e8>,), **{}) took: 1.8926939964294434 sec
[2020-01-07 21:17:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-07 21:17:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-07 21:18:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-07 21:18:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 00:54:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 00:54:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 00:54:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 00:54:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 00:54:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 00:54:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 00:54:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd47834d2e8>,), **{}) took: 1.8967807292938232 sec
[2020-01-08 00:54:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 00:54:54] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 00:55:12] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 00:55:12] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 00:55:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 00:55:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 00:55:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 00:55:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 00:55:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f235e92c2e8>,), **{}) took: 2.1976475715637207 sec
[2020-01-08 00:55:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 00:55:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 00:56:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 00:56:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 00:56:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 00:56:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 00:56:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 00:56:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 00:56:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f81293112e8>,), **{}) took: 2.389650821685791 sec
[2020-01-08 00:57:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 00:57:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 00:58:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 00:58:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 00:59:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 00:59:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 00:59:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 00:59:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 00:59:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5dd93412e8>,), **{}) took: 2.4255263805389404 sec
[2020-01-08 00:59:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 00:59:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:00:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:00:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:01:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:01:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:01:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:01:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:01:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7c125c33c8>,), **{}) took: 2.6924264430999756 sec
[2020-01-08 01:01:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:01:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:01:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:01:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:01:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:01:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:01:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:01:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:01:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f27e6b772e8>,), **{}) took: 2.6221201419830322 sec
[2020-01-08 01:02:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:02:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:06:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:06:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:07:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:07:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:07:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:07:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:07:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f844be1d2e8>,), **{}) took: 3.0540788173675537 sec
[2020-01-08 01:07:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:07:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:07:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:07:56] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:07:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:07:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:08:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:08:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:08:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f77f6b3a2e8>,), **{}) took: 3.048631191253662 sec
[2020-01-08 01:08:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:08:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:10:58] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:10:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:14:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:14:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:14:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:14:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:14:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0f6dd8b2e8>,), **{}) took: 1.8995246887207031 sec
[2020-01-08 01:14:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:14:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:15:26] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:15:26] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:17:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:17:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:17:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:17:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:17:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:17:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:17:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7f9f7ab2e8>,), **{}) took: 1.8692350387573242 sec
[2020-01-08 01:17:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:17:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:17:29] - slp - ERROR -- Current run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-08 01:17:29] - slp - ERROR -- Engine run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-08 01:17:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:17:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:17:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:17:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:17:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdf1b56e2e8>,), **{}) took: 1.895169973373413 sec
[2020-01-08 01:18:06] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:18:06] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:19:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:19:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:19:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:19:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:19:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:19:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:19:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f83fe5762e8>,), **{}) took: 1.8995931148529053 sec
[2020-01-08 01:19:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:19:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:19:32] - slp - ERROR -- Current run is terminating due to exception: an integer is required (got type list).
[2020-01-08 01:19:32] - slp - ERROR -- Engine run is terminating due to exception: an integer is required (got type list).
[2020-01-08 01:20:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:20:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:20:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:20:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:20:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe8e36076a0>,), **{}) took: 1.86903977394104 sec
[2020-01-08 01:20:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:20:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:20:20] - slp - ERROR -- Current run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-08 01:20:20] - slp - ERROR -- Engine run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-08 01:20:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:20:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:20:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:20:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:20:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbb9c5a12e8>,), **{}) took: 1.888012170791626 sec
[2020-01-08 01:20:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:20:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:20:39] - slp - ERROR -- Current run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-08 01:20:39] - slp - ERROR -- Engine run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-08 01:24:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:24:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:24:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:24:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:24:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f33bbbea2e8>,), **{}) took: 3.022538185119629 sec
[2020-01-08 01:24:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:24:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:33:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:33:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:35:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:35:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:35:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:35:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:35:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f86f43b42b0>,), **{}) took: 2.031759738922119 sec
[2020-01-08 01:35:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:35:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:35:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:35:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:36:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:36:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:36:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:36:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:36:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe2cc5392e8>,), **{}) took: 2.027578592300415 sec
[2020-01-08 01:36:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:36:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:37:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:37:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:37:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:37:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:37:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:37:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:37:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f94d67f52e8>,), **{}) took: 2.017474889755249 sec
[2020-01-08 01:37:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:37:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:37:55] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:37:55] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:38:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:38:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:38:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:38:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:38:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efe934df2e8>,), **{}) took: 2.047548294067383 sec
[2020-01-08 01:39:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:39:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:39:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:39:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:40:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:40:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:40:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:40:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:40:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd4bab582e8>,), **{}) took: 2.0550875663757324 sec
[2020-01-08 01:40:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:40:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:42:57] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:42:57] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:43:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:43:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:43:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:43:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:43:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f120b9f42e8>,), **{}) took: 2.0753016471862793 sec
[2020-01-08 01:43:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:43:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:43:25] - slp - ERROR -- Current run is terminating due to exception: an integer is required (got type list).
[2020-01-08 01:43:25] - slp - ERROR -- Engine run is terminating due to exception: an integer is required (got type list).
[2020-01-08 01:44:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:44:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:44:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:44:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:44:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f66522f92e8>,), **{}) took: 2.043043613433838 sec
[2020-01-08 01:44:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:44:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:44:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:44:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:44:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:44:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:44:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:44:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:44:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f329a98de80>,), **{}) took: 2.0630459785461426 sec
[2020-01-08 01:45:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:45:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:46:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:46:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:47:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:47:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:47:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:47:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:47:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efd3f4f92e8>,), **{}) took: 2.0530316829681396 sec
[2020-01-08 01:47:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:47:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:47:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:47:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:47:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:47:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:47:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:47:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:47:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fabb8125eb8>,), **{}) took: 2.0855507850646973 sec
[2020-01-08 01:47:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:47:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:54:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:54:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:54:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:54:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:54:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:54:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:54:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0633a422b0>,), **{}) took: 2.011251449584961 sec
[2020-01-08 01:54:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:54:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:55:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 01:55:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 01:56:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:56:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:56:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:56:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:56:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8a8043ceb8>,), **{}) took: 1.9852619171142578 sec
[2020-01-08 01:56:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:56:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:56:58] - slp - ERROR -- Current run is terminating due to exception: an integer is required (got type list).
[2020-01-08 01:56:58] - slp - ERROR -- Engine run is terminating due to exception: an integer is required (got type list).
[2020-01-08 01:58:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 01:58:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 01:58:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 01:58:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 01:58:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0a1e9e52e8>,), **{}) took: 2.0399298667907715 sec
[2020-01-08 01:58:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 01:58:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 01:58:38] - slp - ERROR -- Current run is terminating due to exception: 'int' object has no attribute 'size'.
[2020-01-08 01:58:38] - slp - ERROR -- Engine run is terminating due to exception: 'int' object has no attribute 'size'.
[2020-01-08 02:00:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:00:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:00:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:00:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:00:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f91ce2292e8>,), **{}) took: 2.029433250427246 sec
[2020-01-08 02:00:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:00:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:20:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:20:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:21:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:21:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:21:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:21:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:21:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f84f623a2e8>,), **{}) took: 2.0603675842285156 sec
[2020-01-08 02:21:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:21:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:28:18] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:28:18] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:28:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:28:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:28:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:28:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:28:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7996ce7668>,), **{}) took: 2.0174355506896973 sec
[2020-01-08 02:28:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:28:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:29:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:29:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:30:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:30:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:30:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:30:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:30:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fab28a60dd8>,), **{}) took: 2.071800470352173 sec
[2020-01-08 02:30:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:30:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:30:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:30:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:30:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:30:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:30:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:30:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:30:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ffb3e3e99e8>,), **{}) took: 2.1035315990448 sec
[2020-01-08 02:30:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:30:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:32:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:32:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:32:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:32:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:32:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:32:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:32:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc89c0ebef0>,), **{}) took: 2.086413860321045 sec
[2020-01-08 02:32:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:32:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:33:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:33:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:33:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:33:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:33:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:33:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:33:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcd6e3d92e8>,), **{}) took: 2.130162239074707 sec
[2020-01-08 02:33:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:33:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:34:58] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:34:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:35:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:35:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:35:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:35:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:35:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa73b6322b0>,), **{}) took: 2.1181936264038086 sec
[2020-01-08 02:35:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:35:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:35:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:35:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:36:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:36:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:36:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:36:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:36:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd145fbe2b0>,), **{}) took: 2.125977039337158 sec
[2020-01-08 02:36:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:36:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:36:39] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:36:39] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:36:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:36:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:36:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:36:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:36:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f636a6c32b0>,), **{}) took: 2.119476079940796 sec
[2020-01-08 02:37:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:37:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:37:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:37:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:37:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:37:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:37:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:37:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:37:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff1d16652b0>,), **{}) took: 2.104783296585083 sec
[2020-01-08 02:37:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:37:42] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:38:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:38:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:38:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:38:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:38:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:38:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:38:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f67c2f2a9e8>,), **{}) took: 2.152463674545288 sec
[2020-01-08 02:38:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:38:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:39:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:39:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:39:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:39:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:39:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:39:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:39:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcb35bca2e8>,), **{}) took: 2.223353147506714 sec
[2020-01-08 02:39:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:39:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:41:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:41:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:41:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:41:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:41:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:41:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:41:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe0d3d542e8>,), **{}) took: 2.2308802604675293 sec
[2020-01-08 02:41:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:41:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:42:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:42:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:42:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:42:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:42:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:42:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:42:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fed5ad002e8>,), **{}) took: 2.2337141036987305 sec
[2020-01-08 02:42:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:42:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:42:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:42:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:43:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:43:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:44:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:44:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:44:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:44:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:44:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2e7e61ff28>,), **{}) took: 2.1898856163024902 sec
[2020-01-08 02:44:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:44:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:46:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:46:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:46:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:46:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:46:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:46:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:46:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f65a16492e8>,), **{}) took: 2.229834794998169 sec
[2020-01-08 02:46:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:46:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:47:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:47:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 02:47:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:47:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:47:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:47:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:47:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7937dd52e8>,), **{}) took: 2.2595272064208984 sec
[2020-01-08 02:48:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:48:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:48:08] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 02:48:08] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 02:49:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:49:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:49:49] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:49:51] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:49:51] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fca4da4b2e8>,), **{}) took: 2.0835070610046387 sec
[2020-01-08 02:50:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:50:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:50:00] - slp - ERROR -- Current run is terminating due to exception: an integer is required (got type str).
[2020-01-08 02:50:00] - slp - ERROR -- Engine run is terminating due to exception: an integer is required (got type str).
[2020-01-08 02:50:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 02:50:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 02:50:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 02:50:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 02:50:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1a8d8c02e8>,), **{}) took: 2.0724573135375977 sec
[2020-01-08 02:51:06] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 02:51:06] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 02:55:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 02:55:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 13:06:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:06:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:06:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:06:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:06:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdb3152d320>,), **{}) took: 2.172349214553833 sec
[2020-01-08 13:06:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:06:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:13:26] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 13:13:26] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 13:13:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:13:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:13:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:13:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:13:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd2ae64a320>,), **{}) took: 2.2500221729278564 sec
[2020-01-08 13:13:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:13:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:13:53] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:13:53] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:15:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:15:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:15:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:15:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:15:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7fc2a672e8>,), **{}) took: 2.2792718410491943 sec
[2020-01-08 13:15:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:15:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:15:16] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:15:16] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:15:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:15:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:15:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:15:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:15:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f89033a3320>,), **{}) took: 2.316110849380493 sec
[2020-01-08 13:15:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:15:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:15:31] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:15:31] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:15:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:15:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:15:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:15:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:15:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f054f004320>,), **{}) took: 2.341118574142456 sec
[2020-01-08 13:15:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:15:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:32:45] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 13:32:45] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 13:32:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:32:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:32:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:32:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:32:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcffe610320>,), **{}) took: 2.3879458904266357 sec
[2020-01-08 13:33:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:33:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:38:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 13:38:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 13:38:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:38:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:38:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:38:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:38:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe0263b8320>,), **{}) took: 2.428218364715576 sec
[2020-01-08 13:38:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:38:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:38:17] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:38:17] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:38:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:38:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:38:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:38:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:38:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f471bbc0320>,), **{}) took: 2.4423890113830566 sec
[2020-01-08 13:38:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:38:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:38:36] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:38:36] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 380.
[2020-01-08 13:38:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:38:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:38:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:38:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:38:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fefc79d9320>,), **{}) took: 2.5464582443237305 sec
[2020-01-08 13:38:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:38:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:39:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 13:39:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 13:48:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:48:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:48:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:48:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:48:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faa8e608748>,), **{}) took: 2.472026824951172 sec
[2020-01-08 13:48:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:48:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:48:20] - slp - ERROR -- Current run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-01-08 13:48:20] - slp - ERROR -- Engine run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-01-08 13:51:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 13:51:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 13:51:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 13:51:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 13:51:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f18add132e8>,), **{}) took: 2.4616856575012207 sec
[2020-01-08 13:51:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 13:51:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 13:51:20] - slp - ERROR -- Current run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-01-08 13:51:20] - slp - ERROR -- Engine run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-01-08 14:02:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:02:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:02:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:02:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:02:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0f4b156320>,), **{}) took: 11.074701070785522 sec
[2020-01-08 14:02:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:02:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:02:34] - slp - ERROR -- Current run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-01-08 14:02:34] - slp - ERROR -- Engine run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-01-08 14:02:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:02:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:02:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:03:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:03:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f87701dcf98>,), **{}) took: 8.148883581161499 sec
[2020-01-08 14:03:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:03:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:05:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:05:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:05:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:05:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:05:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:05:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:05:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff12de432e8>,), **{}) took: 8.064634561538696 sec
[2020-01-08 14:05:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:05:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:09:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:09:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:10:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:10:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:10:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:10:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:10:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1b4ce3f320>,), **{}) took: 8.181501865386963 sec
[2020-01-08 14:10:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:10:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:23:03] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:23:03] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:23:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:23:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:23:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:23:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:23:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa49b105320>,), **{}) took: 9.541974067687988 sec
[2020-01-08 14:23:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:23:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:23:57] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:23:57] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:24:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:24:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:24:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:24:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:24:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f307bc7e320>,), **{}) took: 6.483628988265991 sec
[2020-01-08 14:24:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:24:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:25:28] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:25:28] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:25:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:25:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:25:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:25:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:25:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f51ea853320>,), **{}) took: 6.5606396198272705 sec
[2020-01-08 14:25:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:25:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:26:45] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:26:45] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:27:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:27:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:27:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:27:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:27:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f88ed9ba320>,), **{}) took: 6.901930809020996 sec
[2020-01-08 14:27:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:27:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:27:29] - slp - ERROR -- Current run is terminating due to exception: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 25 and 41 in dimension 1 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71.
[2020-01-08 14:27:29] - slp - ERROR -- Engine run is terminating due to exception: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 25 and 41 in dimension 1 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71.
[2020-01-08 14:28:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:28:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:28:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:28:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:28:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb8b3f88320>,), **{}) took: 6.934762001037598 sec
[2020-01-08 14:28:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:28:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:31:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:31:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:32:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:32:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:32:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:32:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:32:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:32:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:32:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdab0a3ca58>,), **{}) took: 7.379705905914307 sec
[2020-01-08 14:32:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:32:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:35:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:35:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:35:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:35:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:35:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:36:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:36:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdb4838f320>,), **{}) took: 7.41419243812561 sec
[2020-01-08 14:36:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:36:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:40:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:40:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:42:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:42:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:42:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:42:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:42:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f46aa919320>,), **{}) took: 7.520919322967529 sec
[2020-01-08 14:42:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:42:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:46:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:46:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:47:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:47:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:47:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:47:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:47:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa9b8676320>,), **{}) took: 7.979674577713013 sec
[2020-01-08 14:47:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:47:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:50:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:50:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:51:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:51:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:51:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:51:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:51:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f682d080320>,), **{}) took: 7.481612205505371 sec
[2020-01-08 14:51:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:51:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:51:45] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:51:45] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:52:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:52:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:52:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:52:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:52:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0d1c6cf320>,), **{}) took: 4.694054365158081 sec
[2020-01-08 14:52:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:52:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:53:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:53:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:54:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:54:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:54:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:54:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:54:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3f50eb4ef0>,), **{}) took: 6.942025423049927 sec
[2020-01-08 14:54:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:54:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:57:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:57:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:58:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:58:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:58:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:58:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:58:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7ed0f88320>,), **{}) took: 10.632250308990479 sec
[2020-01-08 14:58:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:58:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 14:59:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 14:59:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 14:59:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 14:59:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 14:59:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 14:59:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 14:59:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f689e0ba320>,), **{}) took: 8.933200597763062 sec
[2020-01-08 14:59:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 14:59:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 15:00:26] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 15:00:26] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-08 15:12:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 15:12:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 15:12:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 15:12:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 15:12:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff904886320>,), **{}) took: 7.10677170753479 sec
[2020-01-08 15:12:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 15:12:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 15:25:25] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 15:25:25] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 15:25:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 15:25:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 15:25:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 15:25:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 15:25:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fee5fd3f320>,), **{}) took: 7.098797559738159 sec
[2020-01-08 15:25:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 15:25:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 15:26:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 15:26:59] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 15:27:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 15:27:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 15:27:49] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 15:27:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 15:27:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa462418320>,), **{}) took: 7.089869737625122 sec
[2020-01-08 15:28:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 15:28:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 15:38:45] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 15:38:45] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 15:38:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 15:38:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 15:39:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 15:39:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 15:39:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9a423a2f28>,), **{}) took: 7.1452765464782715 sec
[2020-01-08 15:39:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 15:39:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 15:39:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 15:39:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 15:39:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 15:39:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 15:39:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 15:39:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 15:39:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f27402e8f28>,), **{}) took: 7.145263671875 sec
[2020-01-08 15:40:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 15:40:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 15:42:08] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 15:42:08] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-08 15:42:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 15:42:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 15:42:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 15:42:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 15:42:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efe33e012e8>,), **{}) took: 7.033553838729858 sec
[2020-01-08 15:43:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 15:43:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 21:02:26] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 21:02:26] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-08 21:02:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 21:02:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 21:56:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 21:56:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 21:56:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 21:56:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 21:56:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbe96c482b0>,), **{}) took: 2.8996970653533936 sec
[2020-01-08 21:56:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 21:56:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 22:19:57] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 22:19:57] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 22:55:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 22:55:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 22:55:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 22:55:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 22:55:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcf6aeb5278>,), **{}) took: 2.0555291175842285 sec
[2020-01-08 22:55:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 22:55:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 22:55:45] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 380, got 300.
[2020-01-08 22:55:45] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 380, got 300.
[2020-01-08 22:56:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 22:56:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 22:56:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 22:56:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 22:56:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f11c50e8dd8>,), **{}) took: 1.8870317935943604 sec
[2020-01-08 22:56:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 22:56:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 22:56:39] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 380, got 300.
[2020-01-08 22:56:39] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 380, got 300.
[2020-01-08 22:57:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 22:57:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 22:57:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 22:57:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 22:57:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbb98ee22b0>,), **{}) took: 1.8894338607788086 sec
[2020-01-08 22:57:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 22:57:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:01:08] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:01:08] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:01:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:01:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:01:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:01:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:01:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4d482ee278>,), **{}) took: 1.880861520767212 sec
[2020-01-08 23:01:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:01:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:06:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:06:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:06:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:06:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:06:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:06:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:06:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2a2c3fc2b0>,), **{}) took: 1.8865156173706055 sec
[2020-01-08 23:06:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:06:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:06:58] - slp - ERROR -- Current run is terminating due to exception: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 150 and 1 in dimension 1 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71.
[2020-01-08 23:06:58] - slp - ERROR -- Engine run is terminating due to exception: invalid argument 0: Sizes of tensors must match except in dimension 2. Got 150 and 1 in dimension 1 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:71.
[2020-01-08 23:07:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:07:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:07:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:07:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:07:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:07:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:07:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4f95afc2b0>,), **{}) took: 1.8870458602905273 sec
[2020-01-08 23:07:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:07:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:08:45] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:08:45] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:08:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:08:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:08:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:08:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:08:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f531be252b0>,), **{}) took: 1.8898344039916992 sec
[2020-01-08 23:08:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:08:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:10:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:10:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:10:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:10:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:10:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:10:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:10:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f33c3fdf278>,), **{}) took: 1.8889641761779785 sec
[2020-01-08 23:10:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:10:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:13:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:13:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:14:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:14:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:14:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:14:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:14:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcb8daeb2b0>,), **{}) took: 1.8884172439575195 sec
[2020-01-08 23:14:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:14:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:17:39] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:17:39] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:17:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:17:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:17:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:17:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:17:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff6edae3278>,), **{}) took: 1.8883066177368164 sec
[2020-01-08 23:17:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:17:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:17:53] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [8 x 680], m2: [600 x 600] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-01-08 23:17:53] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [8 x 680], m2: [600 x 600] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-01-08 23:35:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:35:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:35:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:35:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:35:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f49bbcfd2b0>,), **{}) took: 6.989874839782715 sec
[2020-01-08 23:36:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:36:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:36:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:36:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:36:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2e8a8972b0>,), **{}) took: 6.75066351890564 sec
[2020-01-08 23:38:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:38:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:38:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:38:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:38:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc2864112b0>,), **{}) took: 6.864372491836548 sec
[2020-01-08 23:38:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:38:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:38:42] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 600, got 680.
[2020-01-08 23:38:42] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 600, got 680.
[2020-01-08 23:39:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:39:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:39:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:39:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:39:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f36dd9eb278>,), **{}) took: 6.835591793060303 sec
[2020-01-08 23:39:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:39:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:41:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:41:02] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:42:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:42:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:42:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:42:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:42:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6c865d02b0>,), **{}) took: 5.201568841934204 sec
[2020-01-08 23:42:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:42:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:44:12] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:44:12] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:44:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:44:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:44:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:44:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:44:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9ecd416ef0>,), **{}) took: 4.758049011230469 sec
[2020-01-08 23:44:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:44:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:53:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:53:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-08 23:53:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-08 23:53:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-08 23:53:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-08 23:53:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-08 23:53:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f23777b12b0>,), **{}) took: 4.846994876861572 sec
[2020-01-08 23:53:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-08 23:53:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 460, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-08 23:56:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-08 23:56:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 05:39:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 05:39:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 09:13:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 09:13:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:11:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:11:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:11:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:11:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:12:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:12:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:12:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:13:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:13:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f48111eb2e8>,), **{}) took: 44.84918975830078 sec
[2020-01-09 12:13:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:13:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:14:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:14:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:14:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:14:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:14:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:14:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:14:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f89e0d522e8>,), **{}) took: 13.925175428390503 sec
[2020-01-09 12:15:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:15:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:15:33] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:15:33] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:15:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:15:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:15:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:16:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:16:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f09ab1f32e8>,), **{}) took: 13.8371102809906 sec
[2020-01-09 12:16:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:16:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:16:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:16:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:17:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:17:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:17:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:17:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:17:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3065c1b278>,), **{}) took: 13.764134645462036 sec
[2020-01-09 12:17:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:17:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:18:20] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:18:20] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:18:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:18:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:18:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:18:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:18:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f60e5da9da0>,), **{}) took: 13.718528985977173 sec
[2020-01-09 12:19:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:19:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:19:45] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:19:45] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:19:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:19:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:20:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:20:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:20:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7df84a5278>,), **{}) took: 13.778078079223633 sec
[2020-01-09 12:20:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:20:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:21:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:21:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:21:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:21:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:21:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:21:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:21:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f672bc5c278>,), **{}) took: 13.798137664794922 sec
[2020-01-09 12:21:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:21:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:22:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:22:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:22:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:22:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:22:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:22:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:22:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd83e783240>,), **{}) took: 13.746027946472168 sec
[2020-01-09 12:23:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:23:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:24:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:24:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:26:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:26:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:26:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:26:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:26:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa8dabf1240>,), **{}) took: 13.782023668289185 sec
[2020-01-09 12:27:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:27:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:27:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:27:52] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:29:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:29:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:29:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:29:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:29:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb8b759a2e8>,), **{}) took: 13.635617017745972 sec
[2020-01-09 12:29:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:29:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(680, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=680, out_features=680, bias=True)
    (context): Linear(in_features=680, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:32:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:32:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:32:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:32:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:32:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:33:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:33:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5454cc7a20>,), **{}) took: 13.661686897277832 sec
[2020-01-09 12:33:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:33:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 399, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:37:12] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:37:12] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:37:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:37:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:37:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:37:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:37:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f46001e62e8>,), **{}) took: 13.563096284866333 sec
[2020-01-09 12:37:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:37:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 399, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:38:00] - slp - ERROR -- Current run is terminating due to exception: 'l'.
[2020-01-09 12:38:00] - slp - ERROR -- Engine run is terminating due to exception: 'l'.
[2020-01-09 12:39:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:39:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:39:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:39:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:39:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f96acfb6e80>,), **{}) took: 13.546035766601562 sec
[2020-01-09 12:39:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:39:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 399, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:39:57] - slp - ERROR -- Current run is terminating due to exception: 't'.
[2020-01-09 12:39:57] - slp - ERROR -- Engine run is terminating due to exception: 't'.
[2020-01-09 12:41:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:41:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:41:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:41:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:41:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2374a08e48>,), **{}) took: 13.54730486869812 sec
[2020-01-09 12:41:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:41:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 399, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:42:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:42:23] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:42:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:42:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:42:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:42:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:42:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efe0afe6278>,), **{}) took: 13.565789461135864 sec
[2020-01-09 12:43:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:43:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 399, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:46:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:46:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:46:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:46:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:46:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:46:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:46:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff769c54278>,), **{}) took: 2.7770397663116455 sec
[2020-01-09 12:47:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:47:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:48:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:48:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:50:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:50:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:50:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:50:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:50:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd08ee47278>,), **{}) took: 1.8970730304718018 sec
[2020-01-09 12:50:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:50:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:50:30] - slp - ERROR -- Current run is terminating due to exception: local variable 'output_lex' referenced before assignment.
[2020-01-09 12:50:30] - slp - ERROR -- Engine run is terminating due to exception: local variable 'output_lex' referenced before assignment.
[2020-01-09 12:51:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:51:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:51:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:51:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:51:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fee4deeb278>,), **{}) took: 1.9416162967681885 sec
[2020-01-09 12:52:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:52:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:52:03] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-09 12:52:03] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-09 12:53:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:53:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:53:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:53:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:53:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3adb76e278>,), **{}) took: 1.8834116458892822 sec
[2020-01-09 12:53:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:53:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:53:31] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:53:31] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:53:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:53:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:53:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:53:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:53:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe6fbf78278>,), **{}) took: 1.89583158493042 sec
[2020-01-09 12:53:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:53:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:54:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:54:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:55:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:55:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:55:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:55:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:55:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5bc49e4278>,), **{}) took: 1.890080213546753 sec
[2020-01-09 12:55:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:55:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:56:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:56:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:56:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:56:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:56:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:56:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:56:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4941e17278>,), **{}) took: 1.893101692199707 sec
[2020-01-09 12:56:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:56:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:57:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 12:57:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 12:57:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 12:57:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 12:57:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 12:57:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 12:57:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fac4f6bc278>,), **{}) took: 1.8903639316558838 sec
[2020-01-09 12:57:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 12:57:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 12:57:50] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [256 x 900], m2: [699 x 699] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:752.
[2020-01-09 12:57:50] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [256 x 900], m2: [699 x 699] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:752.
[2020-01-09 13:00:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:00:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:00:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:00:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:00:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f43bf982278>,), **{}) took: 1.8962042331695557 sec
[2020-01-09 13:00:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:00:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:02:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:02:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 13:03:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:03:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:03:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:03:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:03:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc068ec0240>,), **{}) took: 1.8948454856872559 sec
[2020-01-09 13:03:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:03:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:03:34] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [304 x 600], m2: [699 x 699] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:752.
[2020-01-09 13:03:34] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [304 x 600], m2: [699 x 699] at /pytorch/aten/src/TH/generic/THTensorMath.cpp:752.
[2020-01-09 13:04:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:04:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:04:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:04:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:04:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f70fceea278>,), **{}) took: 1.904505968093872 sec
[2020-01-09 13:04:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:04:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:11:20] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:11:20] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 13:11:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:11:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:11:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:11:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:11:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4c08dbb9b0>,), **{}) took: 1.899768352508545 sec
[2020-01-09 13:11:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:11:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:11:40] - slp - ERROR -- Current run is terminating due to exception: name 'output_title' is not defined.
[2020-01-09 13:11:40] - slp - ERROR -- Engine run is terminating due to exception: name 'output_title' is not defined.
[2020-01-09 13:13:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:13:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:13:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:13:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:13:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f64e3959278>,), **{}) took: 1.898488998413086 sec
[2020-01-09 13:13:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:13:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:14:03] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:14:03] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-09 13:14:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:14:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:14:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:14:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:14:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbf66caf278>,), **{}) took: 1.8954496383666992 sec
[2020-01-09 13:14:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:14:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:15:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:15:50] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-09 13:18:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:18:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:18:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:18:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:18:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9368da1278>,), **{}) took: 1.8897147178649902 sec
[2020-01-09 13:18:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:18:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:18:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:18:59] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-09 13:19:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:19:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:19:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:19:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:19:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4721044a20>,), **{}) took: 1.8881428241729736 sec
[2020-01-09 13:19:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:19:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:20:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:20:41] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-09 13:21:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:21:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:21:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:21:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:21:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0a251702e8>,), **{}) took: 2.5596237182617188 sec
[2020-01-09 13:21:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:21:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:43:08] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:43:08] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-09 13:43:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:43:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 13:43:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 13:43:17] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-09 13:46:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:46:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:46:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-09 13:46:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-09 13:46:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-09 13:46:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-09 13:46:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc2863ac2e8>,), **{}) took: 1.8979885578155518 sec
[2020-01-09 13:46:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 13:46:42] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 15:50:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 15:50:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 380, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(380, 380, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 16:58:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 16:58:22] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-09 16:58:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 16:58:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 20:54:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-09 20:54:47] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-09 20:54:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 20:54:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-09 23:38:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-09 23:38:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-10 03:46:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-10 03:46:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-11 18:16:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:16:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:16:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:16:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:17:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:17:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:17:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 18:18:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 18:18:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb792b3f278>,), **{}) took: 40.575958251953125 sec
[2020-01-11 18:18:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:18:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:18:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 18:19:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 18:19:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff1d4646278>,), **{}) took: 23.41627526283264 sec
[2020-01-11 18:38:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:38:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:38:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 18:38:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 18:38:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f76cadef240>,), **{}) took: 18.181288957595825 sec
[2020-01-11 18:42:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:42:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:42:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 18:43:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 18:43:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faced431b70>,), **{}) took: 17.795042276382446 sec
[2020-01-11 18:45:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:45:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:45:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 18:45:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 18:45:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2042821208>,), **{}) took: 17.8999240398407 sec
[2020-01-11 18:45:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-11 18:45:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-11 18:47:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-11 18:47:35] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-11 18:48:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:48:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:48:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 18:48:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 18:48:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9029a58208>,), **{}) took: 17.917794227600098 sec
[2020-01-11 18:49:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-11 18:49:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-11 18:51:53] - slp - ERROR -- Current run is terminating due to exception: list index out of range.
[2020-01-11 18:51:53] - slp - ERROR -- Engine run is terminating due to exception: list index out of range.
[2020-01-11 18:55:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 18:55:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 18:55:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 18:55:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 18:55:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9f150f1208>,), **{}) took: 20.120972633361816 sec
[2020-01-11 18:55:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-11 18:55:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-11 19:26:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-11 19:26:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-11 19:27:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 19:27:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 19:27:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 19:27:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 19:27:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1184614d68>,), **{}) took: 17.748340368270874 sec
[2020-01-11 19:28:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-11 19:28:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-11 19:33:55] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-11 19:33:55] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-11 19:35:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-11 19:35:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-11 19:35:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-11 19:35:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-11 19:35:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0ff30aed68>,), **{}) took: 18.089232206344604 sec
[2020-01-11 19:35:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-11 19:35:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-11 22:44:47] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 5.58 GiB already allocated; 9.19 MiB free; 8.75 MiB cached).
[2020-01-11 22:44:47] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 5.58 GiB already allocated; 9.19 MiB free; 8.75 MiB cached).
[2020-01-15 02:46:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-15 02:46:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:10:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:10:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:11:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:11:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:11:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:12:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:12:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa5d7d02240>,), **{}) took: 47.06324076652527 sec
[2020-01-18 15:12:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-18 15:12:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-18 15:16:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-18 15:16:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-18 15:20:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:20:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:20:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:20:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:20:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f08cc9ea208>,), **{}) took: 23.770172357559204 sec
[2020-01-18 15:21:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-18 15:21:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-18 15:21:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-18 15:21:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-18 15:24:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:24:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:24:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:24:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:24:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe6704a0240>,), **{}) took: 24.132283926010132 sec
[2020-01-18 15:25:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:25:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:25:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:25:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:25:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7fd0e67240>,), **{}) took: 26.57350254058838 sec
[2020-01-18 15:26:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:26:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:26:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:26:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:26:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1256e142b0>,), **{}) took: 25.035491943359375 sec
[2020-01-18 15:27:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:27:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:27:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:27:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:27:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1b496a32b0>,), **{}) took: 25.89329504966736 sec
[2020-01-18 15:31:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:31:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:31:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:31:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:31:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4407636278>,), **{}) took: 26.04488253593445 sec
[2020-01-18 15:32:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:32:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:32:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:33:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:33:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff24dd6f278>,), **{}) took: 25.87161350250244 sec
[2020-01-18 15:34:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:34:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:34:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:35:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:35:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f38230462b0>,), **{}) took: 26.442490339279175 sec
[2020-01-18 15:37:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:37:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:37:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:38:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:38:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:38:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:38:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:38:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f84b2ecd2b0>,), **{}) took: 25.362969160079956 sec
[2020-01-18 15:41:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:41:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:41:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:42:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:42:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6726822240>,), **{}) took: 25.263832807540894 sec
[2020-01-18 15:42:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:42:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:42:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:43:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:43:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6cc5ac2278>,), **{}) took: 25.358371257781982 sec
[2020-01-18 15:45:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:45:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:45:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:45:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:45:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fca0e2c6278>,), **{}) took: 25.676605224609375 sec
[2020-01-18 15:49:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:49:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:49:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:49:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:49:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faaad812278>,), **{}) took: 25.59701704978943 sec
[2020-01-18 15:54:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:54:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:54:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:55:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:55:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:55:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:56:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:56:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb6bb6152b0>,), **{}) took: 27.70869779586792 sec
[2020-01-18 15:56:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 15:56:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 15:56:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 15:56:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 15:56:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f527a0e5a90>,), **{}) took: 25.80538320541382 sec
[2020-01-18 15:57:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-18 15:57:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-18 16:00:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-18 16:00:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-18 16:01:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-18 16:01:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-18 16:01:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-18 16:01:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-18 16:01:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe1fe9112b0>,), **{}) took: 26.040929794311523 sec
[2020-01-18 16:02:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-18 16:02:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-18 16:04:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-18 16:04:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 18:40:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 18:40:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 18:41:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 18:41:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 18:41:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 18:42:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 18:42:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5b2c6dd278>,), **{}) took: 10.545470714569092 sec
[2020-01-19 18:42:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 18:42:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 18:47:57] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 18:47:57] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 18:56:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 18:56:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 18:56:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 18:56:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 18:56:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f524a13f2b0>,), **{}) took: 1.9330103397369385 sec
[2020-01-19 18:56:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 18:56:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:19:42] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:19:42] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:19:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:19:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:19:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:19:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:19:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f92686552b0>,), **{}) took: 1.924384593963623 sec
[2020-01-19 19:19:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:19:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:24:39] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:24:39] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:30:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:30:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:30:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:30:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:30:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb5b60792b0>,), **{}) took: 2.029454469680786 sec
[2020-01-19 19:30:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:30:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:32:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:32:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:34:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:34:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:34:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:34:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:34:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb2f17382b0>,), **{}) took: 1.9232869148254395 sec
[2020-01-19 19:34:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:34:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:34:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:34:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:35:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:35:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:35:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:35:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:35:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd2fcc132b0>,), **{}) took: 1.933396577835083 sec
[2020-01-19 19:35:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:35:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:37:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:37:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:38:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:38:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:38:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:38:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:38:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc16ff772b0>,), **{}) took: 1.9187352657318115 sec
[2020-01-19 19:38:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:38:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:40:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:40:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:40:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:40:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:40:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:40:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:40:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f42376112b0>,), **{}) took: 1.931091070175171 sec
[2020-01-19 19:40:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:40:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:45:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:45:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:45:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:45:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:45:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:45:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:45:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1281d99278>,), **{}) took: 2.052349805831909 sec
[2020-01-19 19:45:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:45:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:50:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:50:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:50:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:50:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:50:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:50:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:50:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6fcffdef98>,), **{}) took: 2.0564544200897217 sec
[2020-01-19 19:51:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:51:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:53:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 19:53:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 19:53:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:53:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:53:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:54:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:54:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc3758e22b0>,), **{}) took: 2.057424306869507 sec
[2020-01-19 19:54:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:54:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:54:10] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-01-19 19:54:10] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-01-19 19:55:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:55:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:55:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:55:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:55:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa8d01982b0>,), **{}) took: 1.9551763534545898 sec
[2020-01-19 19:56:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:56:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 19:56:09] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-01-19 19:56:09] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-01-19 19:57:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 19:57:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 19:57:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 19:57:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 19:57:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efe915bf2b0>,), **{}) took: 1.9978461265563965 sec
[2020-01-19 19:57:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 19:57:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:02:28] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:02:28] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:02:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:02:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:02:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:02:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:02:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7c499a1cc0>,), **{}) took: 1.9937522411346436 sec
[2020-01-19 20:02:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:02:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:02:52] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-01-19 20:02:52] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-01-19 20:04:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:04:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:04:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:04:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:04:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9664f002b0>,), **{}) took: 1.966048002243042 sec
[2020-01-19 20:04:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:04:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:06:03] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:06:03] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:06:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:06:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:06:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:06:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:06:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f64154b02b0>,), **{}) took: 2.0266776084899902 sec
[2020-01-19 20:06:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:06:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:07:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:07:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:07:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:07:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:07:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:07:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:07:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7feab81f42b0>,), **{}) took: 1.9838130474090576 sec
[2020-01-19 20:08:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:08:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:08:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:08:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:09:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:09:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:09:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:09:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:09:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f105c08a2b0>,), **{}) took: 1.9749128818511963 sec
[2020-01-19 20:09:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:09:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:09:27] - slp - ERROR -- Current run is terminating due to exception: too many values to unpack (expected 5).
[2020-01-19 20:09:27] - slp - ERROR -- Engine run is terminating due to exception: too many values to unpack (expected 5).
[2020-01-19 20:16:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:16:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:16:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:16:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:16:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcabd0172b0>,), **{}) took: 1.9484503269195557 sec
[2020-01-19 20:16:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:16:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:16:43] - slp - ERROR -- Current run is terminating due to exception: name 'nnumber_of_sentences' is not defined.
[2020-01-19 20:16:43] - slp - ERROR -- Engine run is terminating due to exception: name 'nnumber_of_sentences' is not defined.
[2020-01-19 20:17:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:17:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:17:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:17:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:17:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc84671b2b0>,), **{}) took: 2.027534246444702 sec
[2020-01-19 20:17:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:17:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:38:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:38:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:38:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:38:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:38:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:38:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:38:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7feb0bd942b0>,), **{}) took: 1.9226570129394531 sec
[2020-01-19 20:38:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:38:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:44:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:44:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:45:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:45:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:45:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:45:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:45:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f948373d2b0>,), **{}) took: 1.918936014175415 sec
[2020-01-19 20:45:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:45:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:48:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:48:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:48:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:48:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:48:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:48:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:48:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8c18aee2b0>,), **{}) took: 1.919330358505249 sec
[2020-01-19 20:48:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:48:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:49:25] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:49:25] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:49:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:49:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:49:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:49:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:49:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6ba670f2b0>,), **{}) took: 1.9170217514038086 sec
[2020-01-19 20:50:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:50:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:50:28] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:50:28] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:51:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:51:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:51:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:51:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:51:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9625f8d2b0>,), **{}) took: 1.914984941482544 sec
[2020-01-19 20:51:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:51:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:55:06] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:55:06] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:55:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:55:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:55:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:55:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:55:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff4e05e82e8>,), **{}) took: 1.9182379245758057 sec
[2020-01-19 20:55:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:55:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:55:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:55:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:56:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:56:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:56:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:56:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:56:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f682ab742e8>,), **{}) took: 1.969897747039795 sec
[2020-01-19 20:56:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:56:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:56:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:56:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 20:57:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 20:57:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 20:57:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 20:57:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 20:57:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6e567132e8>,), **{}) took: 1.9190075397491455 sec
[2020-01-19 20:57:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 20:57:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 20:57:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 20:57:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:00:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:00:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:01:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:01:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:01:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc165df62e8>,), **{}) took: 1.9580111503601074 sec
[2020-01-19 21:01:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:01:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:02:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:02:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:03:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:03:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:03:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:03:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:03:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9d592722e8>,), **{}) took: 1.9678514003753662 sec
[2020-01-19 21:03:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:03:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:03:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:03:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:05:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:05:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:05:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:05:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:05:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc7b62b4240>,), **{}) took: 1.9120290279388428 sec
[2020-01-19 21:05:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:05:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:05:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:05:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:06:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:06:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:06:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:06:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:06:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5620861278>,), **{}) took: 1.915926456451416 sec
[2020-01-19 21:06:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:06:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:29:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:29:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:40:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:40:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:40:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:40:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:40:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0d6b914240>,), **{}) took: 1.9644579887390137 sec
[2020-01-19 21:40:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:40:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:41:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:41:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:43:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:43:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:43:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:43:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:43:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdeb2a41240>,), **{}) took: 2.1851513385772705 sec
[2020-01-19 21:43:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:43:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:44:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:44:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:45:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:45:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:45:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:45:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:45:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fef55d8c240>,), **{}) took: 2.1065962314605713 sec
[2020-01-19 21:45:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:45:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:46:30] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:46:30] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:51:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:51:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:51:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:51:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:51:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faa9bcdb208>,), **{}) took: 2.217446804046631 sec
[2020-01-19 21:51:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:51:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:53:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:53:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:53:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:53:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:53:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:53:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:53:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9c4cfe2240>,), **{}) took: 2.04154634475708 sec
[2020-01-19 21:53:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:53:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:54:39] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:54:39] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 21:55:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 21:55:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 21:55:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 21:55:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 21:55:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f738c373240>,), **{}) took: 1.9717366695404053 sec
[2020-01-19 21:55:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 21:55:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 21:58:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 21:58:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:13:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:13:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:13:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:13:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:13:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc95250c240>,), **{}) took: 1.941727876663208 sec
[2020-01-19 22:13:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:13:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 22:16:03] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 22:16:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:16:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:16:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:16:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:16:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:16:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f22f8bb2208>,), **{}) took: 1.9196093082427979 sec
[2020-01-19 22:16:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:16:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 22:17:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 22:17:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:19:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:19:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:19:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:19:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:19:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1002c589e8>,), **{}) took: 1.9179131984710693 sec
[2020-01-19 22:19:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:19:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 22:31:30] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 22:31:30] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:31:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:31:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:31:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:31:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:31:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f15e26f5240>,), **{}) took: 1.9078624248504639 sec
[2020-01-19 22:31:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:31:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 22:33:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 22:33:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:33:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:33:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:33:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:33:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:33:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f201fc4f240>,), **{}) took: 1.9099106788635254 sec
[2020-01-19 22:33:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:33:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 22:34:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 22:34:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:34:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:34:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:34:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:34:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:34:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f003d71f240>,), **{}) took: 1.9198546409606934 sec
[2020-01-19 22:35:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:35:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 22:36:08] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 22:36:08] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:36:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:36:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:36:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:36:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:36:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efe4de7a240>,), **{}) took: 1.9685122966766357 sec
[2020-01-19 22:36:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:36:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 22:37:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 22:37:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 22:38:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 22:38:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 22:38:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 22:38:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 22:38:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ffb80d08240>,), **{}) took: 1.9073963165283203 sec
[2020-01-19 22:38:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 22:38:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:29:20] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:29:20] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:29:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:29:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:29:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:29:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:29:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2438ac6240>,), **{}) took: 2.0716474056243896 sec
[2020-01-19 23:29:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:29:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:33:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:33:02] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:35:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:35:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:35:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:35:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:35:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0869e82240>,), **{}) took: 1.8883521556854248 sec
[2020-01-19 23:35:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:35:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:45:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:45:02] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:45:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:45:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:45:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:45:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:45:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f18c5966240>,), **{}) took: 1.9183597564697266 sec
[2020-01-19 23:45:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:45:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:46:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:46:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:46:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:46:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:46:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:46:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:46:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9a549a7208>,), **{}) took: 2.527317523956299 sec
[2020-01-19 23:46:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:46:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:46:48] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-19 23:46:48] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-19 23:47:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:47:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:47:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:47:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:47:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9aea31f240>,), **{}) took: 1.9204716682434082 sec
[2020-01-19 23:47:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:47:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:48:07] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:48:07] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:48:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:48:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:48:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:48:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:48:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb103530240>,), **{}) took: 1.9377808570861816 sec
[2020-01-19 23:48:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:48:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:48:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:48:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:48:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:48:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:48:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:48:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:48:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1ef19c6240>,), **{}) took: 1.9134550094604492 sec
[2020-01-19 23:48:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:48:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:48:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:48:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:48:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:48:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:48:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:48:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:48:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f039cd6e240>,), **{}) took: 1.9264910221099854 sec
[2020-01-19 23:49:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:49:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:49:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:49:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:49:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:49:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:49:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:49:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:49:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0bdc01b240>,), **{}) took: 1.938190221786499 sec
[2020-01-19 23:49:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:49:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:49:53] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-19 23:49:53] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-19 23:50:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:50:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:50:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:50:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:50:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f12cd29c240>,), **{}) took: 1.9151947498321533 sec
[2020-01-19 23:50:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:50:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:51:18] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:51:18] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-19 23:51:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-19 23:51:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-19 23:51:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-19 23:51:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-19 23:51:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7e9bf6a240>,), **{}) took: 1.9149444103240967 sec
[2020-01-19 23:51:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-19 23:51:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-19 23:53:06] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-19 23:53:06] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:04:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:04:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:04:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:05:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:05:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc48e6ff208>,), **{}) took: 45.097962856292725 sec
[2020-01-20 16:05:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:05:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:07:34] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:07:34] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:07:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:07:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:07:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:08:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:08:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f59c7c92208>,), **{}) took: 22.829987049102783 sec
[2020-01-20 16:08:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:08:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:10:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:10:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:10:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:10:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:10:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:11:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:11:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdac7bee208>,), **{}) took: 14.983691930770874 sec
[2020-01-20 16:11:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:11:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:12:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:12:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:12:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:12:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:12:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:12:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:12:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd203310208>,), **{}) took: 14.077233076095581 sec
[2020-01-20 16:12:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:12:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:13:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:13:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:13:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:13:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:13:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:13:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:13:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f653d4ad208>,), **{}) took: 18.58330988883972 sec
[2020-01-20 16:13:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:13:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:16:55] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:16:55] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:20:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:20:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:20:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:20:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:20:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdcb53fd1d0>,), **{}) took: 4.121323347091675 sec
[2020-01-20 16:20:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:20:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:36:48] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:36:48] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:36:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:36:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:36:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:37:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:37:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa6e16191d0>,), **{}) took: 36.0945246219635 sec
[2020-01-20 16:37:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:37:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:38:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:38:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:38:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:38:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:38:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:38:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:38:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fabe9fe21d0>,), **{}) took: 11.072265863418579 sec
[2020-01-20 16:38:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:38:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:39:05] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:39:05] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:39:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:39:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:39:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:39:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:39:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9f93b5d198>,), **{}) took: 10.142004013061523 sec
[2020-01-20 16:39:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:39:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:39:48] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:39:48] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:39:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:39:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:39:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:40:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:40:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fadf329b1d0>,), **{}) took: 10.242838382720947 sec
[2020-01-20 16:40:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:40:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:40:29] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:40:29] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 16:42:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:42:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:42:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:43:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:43:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc8c1a4d1d0>,), **{}) took: 10.950636386871338 sec
[2020-01-20 16:43:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:43:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:48:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:48:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:51:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:51:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:51:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:51:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:51:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f30bf2a3dd8>,), **{}) took: 11.529096841812134 sec
[2020-01-20 16:51:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:51:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:52:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:52:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:53:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:53:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:53:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:54:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:54:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc3bd8fb198>,), **{}) took: 17.304877281188965 sec
[2020-01-20 16:54:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:54:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:54:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:54:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:54:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:54:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:54:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:55:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:55:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f476346e198>,), **{}) took: 16.235639333724976 sec
[2020-01-20 16:55:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:55:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:55:57] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:55:57] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:56:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:56:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:56:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:56:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:56:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe3a11381d0>,), **{}) took: 16.17003297805786 sec
[2020-01-20 16:56:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:56:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:57:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:57:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:57:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:57:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:57:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:57:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:57:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6f6d8a41d0>,), **{}) took: 16.670260667800903 sec
[2020-01-20 16:58:06] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 16:58:06] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 16:58:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 16:58:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 16:59:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 16:59:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 16:59:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 16:59:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 16:59:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4ee81ad1d0>,), **{}) took: 12.26001238822937 sec
[2020-01-20 17:00:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:00:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:00:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:00:52] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:01:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:01:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:01:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:01:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:01:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f70929171d0>,), **{}) took: 11.046269655227661 sec
[2020-01-20 17:01:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:01:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:06:00] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:06:00] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:06:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:06:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:06:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:06:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:06:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fec5cf021d0>,), **{}) took: 26.624462366104126 sec
[2020-01-20 17:07:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:07:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:07:22] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:07:22] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:07:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:07:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:07:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:07:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:07:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f05dcb1a1d0>,), **{}) took: 4.011552095413208 sec
[2020-01-20 17:07:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:07:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:08:01] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:08:01] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:08:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:08:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:08:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:08:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:08:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6bf200e1d0>,), **{}) took: 4.109265565872192 sec
[2020-01-20 17:08:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:08:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:09:08] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:09:08] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:09:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:09:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:09:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:09:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:09:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2a048d21d0>,), **{}) took: 4.09241247177124 sec
[2020-01-20 17:09:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:09:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:09:48] - slp - ERROR -- Current run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:09:48] - slp - ERROR -- Engine run is terminating due to exception: local variable 'f_output' referenced before assignment.
[2020-01-20 17:09:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:09:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:09:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:09:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:09:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3a1b8bc1d0>,), **{}) took: 4.25793719291687 sec
[2020-01-20 17:10:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:10:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:10:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:10:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:15:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:15:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:15:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:15:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:15:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc4c5bbf1d0>,), **{}) took: 5.654765605926514 sec
[2020-01-20 17:15:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:15:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:18:03] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:18:03] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:18:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:18:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:18:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:18:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:18:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5faa7f2198>,), **{}) took: 10.469241857528687 sec
[2020-01-20 17:18:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:18:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:19:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:19:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:19:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:19:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:19:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:19:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:19:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff556216e80>,), **{}) took: 6.164921045303345 sec
[2020-01-20 17:19:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:19:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:28:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:28:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:28:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:28:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:28:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:28:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:28:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3f4c9d81d0>,), **{}) took: 7.727129697799683 sec
[2020-01-20 17:28:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:28:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:29:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:29:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:29:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:29:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:29:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:29:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:29:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff880750198>,), **{}) took: 7.9173619747161865 sec
[2020-01-20 17:29:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:29:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:30:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:30:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:30:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:30:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:30:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:31:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:31:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc9882d4e10>,), **{}) took: 8.013483047485352 sec
[2020-01-20 17:31:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:31:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:39:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:39:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:39:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:39:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:39:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:39:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:39:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0a9e5a6198>,), **{}) took: 1.9942796230316162 sec
[2020-01-20 17:39:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:39:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:47:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:47:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:47:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:47:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:47:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:47:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:47:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f083a6971d0>,), **{}) took: 39.9200713634491 sec
[2020-01-20 17:48:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:48:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:51:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:51:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-20 17:51:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-20 17:51:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-20 17:51:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-20 17:51:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-20 17:51:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb8aa63a198>,), **{}) took: 2.037829637527466 sec
[2020-01-20 17:51:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-20 17:51:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-20 17:57:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-20 17:57:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 19:36:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 19:36:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 19:36:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 19:37:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 19:37:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f05b1b6d208>,), **{}) took: 35.183863401412964 sec
[2020-01-23 19:37:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 19:37:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 19:41:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 19:41:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 19:48:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 19:48:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 19:48:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 19:48:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 19:48:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f22b3d1e208>,), **{}) took: 1.9432191848754883 sec
[2020-01-23 19:48:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 19:48:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 19:53:55] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 19:53:55] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 19:54:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 19:54:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 19:54:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 19:54:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 19:54:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff571379cc0>,), **{}) took: 1.8883998394012451 sec
[2020-01-23 19:54:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 19:54:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 19:54:12] - slp - ERROR -- Current run is terminating due to exception: module 'time' has no attribute 'start'.
[2020-01-23 19:54:12] - slp - ERROR -- Engine run is terminating due to exception: module 'time' has no attribute 'start'.
[2020-01-23 19:57:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 19:57:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 19:57:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 19:57:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 19:57:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fed53669208>,), **{}) took: 1.888465166091919 sec
[2020-01-23 19:57:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 19:57:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 20:00:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 20:00:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 20:02:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 20:02:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 20:02:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 20:02:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 20:02:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3ebef13208>,), **{}) took: 1.9072647094726562 sec
[2020-01-23 20:02:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 20:02:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 20:03:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 20:03:10] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-23 20:07:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 20:07:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 20:07:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 20:07:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 20:07:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2d62347208>,), **{}) took: 1.8904445171356201 sec
[2020-01-23 20:08:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 20:08:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 20:09:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 20:09:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 20:10:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 20:10:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 20:10:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 20:10:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 20:10:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb4319c4208>,), **{}) took: 1.9107997417449951 sec
[2020-01-23 20:10:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 20:10:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 20:16:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 20:16:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 20:16:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 20:16:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 20:16:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 20:16:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 20:16:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2d366a1208>,), **{}) took: 1.8892297744750977 sec
[2020-01-23 20:16:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 20:16:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 20:16:54] - slp - ERROR -- Current run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-23 20:16:54] - slp - ERROR -- Engine run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-23 21:56:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 21:56:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 21:56:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 21:56:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 21:56:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f10cb6cb1d0>,), **{}) took: 1.9127094745635986 sec
[2020-01-23 21:56:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 21:56:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 21:56:12] - slp - ERROR -- Current run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-23 21:56:12] - slp - ERROR -- Engine run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-23 21:57:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 21:57:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 21:57:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 21:57:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 21:57:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f15ed1d4208>,), **{}) took: 1.8821415901184082 sec
[2020-01-23 21:58:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 21:58:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 21:59:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 21:59:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 21:59:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 21:59:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:00:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:00:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:00:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:00:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:00:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f566e9401d0>,), **{}) took: 1.8837919235229492 sec
[2020-01-23 22:00:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:00:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:00:25] - slp - ERROR -- Current run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-23 22:00:25] - slp - ERROR -- Engine run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-23 22:03:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:03:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:03:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:03:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:03:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3cd1fdb278>,), **{}) took: 1.9191222190856934 sec
[2020-01-23 22:04:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:04:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:04:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:04:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:04:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faf4c569f98>,), **{}) took: 1.8810160160064697 sec
[2020-01-23 22:04:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:04:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:05:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 22:05:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 22:05:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:05:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:05:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:06:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:06:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5dfe9972b0>,), **{}) took: 1.8830962181091309 sec
[2020-01-23 22:06:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:06:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:06:44] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 11.93 GiB total capacity; 11.29 GiB already allocated; 8.06 MiB free; 88.07 MiB cached).
[2020-01-23 22:06:44] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 40.00 MiB (GPU 0; 11.93 GiB total capacity; 11.29 GiB already allocated; 8.06 MiB free; 88.07 MiB cached).
[2020-01-23 22:07:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:07:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:07:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:07:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:07:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faceac342b0>,), **{}) took: 1.8910198211669922 sec
[2020-01-23 22:07:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:07:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:17:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 22:17:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 22:17:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:17:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:17:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:17:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:17:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcea843b2b0>,), **{}) took: 2.2413954734802246 sec
[2020-01-23 22:17:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:17:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:19:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 22:19:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 22:19:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:19:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:19:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:19:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:19:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faf6de802b0>,), **{}) took: 1.8873310089111328 sec
[2020-01-23 22:20:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:20:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:22:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 22:22:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 22:22:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:22:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:22:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:22:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:22:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff7441d2a90>,), **{}) took: 1.8802461624145508 sec
[2020-01-23 22:23:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:23:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:23:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 22:23:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 22:24:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:24:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:24:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:24:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:24:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3ae4522e80>,), **{}) took: 1.882761001586914 sec
[2020-01-23 22:24:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:24:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:24:56] - slp - ERROR -- Current run is terminating due to exception: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time..
[2020-01-23 22:24:56] - slp - ERROR -- Engine run is terminating due to exception: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time..
[2020-01-23 22:26:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:26:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:26:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:26:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:26:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa84e9e62b0>,), **{}) took: 1.9373540878295898 sec
[2020-01-23 22:26:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:26:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:30:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 22:30:04] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-23 22:31:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:31:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:31:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:31:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:31:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb5a17102b0>,), **{}) took: 1.8817176818847656 sec
[2020-01-23 22:31:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:31:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 22:34:34] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 22:34:34] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 22:34:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 22:34:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 22:34:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 22:34:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 22:34:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdb021887b8>,), **{}) took: 1.9272735118865967 sec
[2020-01-23 22:34:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 22:34:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:23:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:23:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 23:23:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 23:23:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 23:23:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 23:23:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 23:23:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f91f31e92b0>,), **{}) took: 1.9126536846160889 sec
[2020-01-23 23:23:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 23:23:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:32:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:32:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 23:32:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 23:32:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 23:32:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 23:32:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 23:32:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f817fa8d2b0>,), **{}) took: 1.9102659225463867 sec
[2020-01-23 23:33:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 23:33:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:33:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:33:02] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-23 23:33:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 23:33:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 23:33:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 23:33:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 23:33:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0ba3b632b0>,), **{}) took: 1.897003173828125 sec
[2020-01-23 23:33:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 23:33:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:39:06] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:39:06] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 23:39:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 23:39:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 23:39:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 23:39:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 23:39:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc2d07f52b0>,), **{}) took: 1.9907450675964355 sec
[2020-01-23 23:39:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 23:39:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:41:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:41:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 23:41:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 23:41:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 23:41:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 23:41:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 23:41:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5b3c0bb2b0>,), **{}) took: 1.8873212337493896 sec
[2020-01-23 23:41:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-23 23:41:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:42:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:42:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 23:43:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 23:43:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 23:43:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 23:43:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 23:43:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f96f6d76240>,), **{}) took: 1.8820810317993164 sec
[2020-01-23 23:43:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 23:43:54] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:45:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:45:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-23 23:45:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-23 23:45:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-23 23:45:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-23 23:45:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-23 23:45:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f23e02ba240>,), **{}) took: 1.918544054031372 sec
[2020-01-23 23:45:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-23 23:45:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-23 23:48:30] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-23 23:48:30] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:08:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:08:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:08:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:08:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:08:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbf8fdc3710>,), **{}) took: 1.9094243049621582 sec
[2020-01-24 17:08:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:08:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:18:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:18:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:19:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:19:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:19:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:19:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:19:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faa7b232208>,), **{}) took: 1.907287359237671 sec
[2020-01-24 17:19:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:19:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:21:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:21:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:21:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:21:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:21:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:21:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:21:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb948c26208>,), **{}) took: 1.9108610153198242 sec
[2020-01-24 17:21:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:21:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:21:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:21:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:22:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:22:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:22:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:22:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:22:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f78dc8ed208>,), **{}) took: 1.901933193206787 sec
[2020-01-24 17:23:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:23:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:24:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:24:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:24:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:24:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:24:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:24:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:24:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5d94948208>,), **{}) took: 1.9142248630523682 sec
[2020-01-24 17:24:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:24:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:41:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:41:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:42:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:42:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:42:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:42:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:42:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f18bd3a7208>,), **{}) took: 1.9115865230560303 sec
[2020-01-24 17:42:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:42:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:42:33] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:42:33] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:42:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:42:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:42:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:42:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:42:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efef967e208>,), **{}) took: 1.9081943035125732 sec
[2020-01-24 17:42:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:42:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:48:06] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:48:06] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:48:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:48:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:48:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:48:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:48:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe192a06208>,), **{}) took: 1.8861334323883057 sec
[2020-01-24 17:48:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:48:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:48:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:48:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:49:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:49:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:49:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:49:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:49:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1b7567d208>,), **{}) took: 1.9131932258605957 sec
[2020-01-24 17:49:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:49:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:49:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:49:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:49:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:49:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:49:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:49:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:49:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff2d745e208>,), **{}) took: 2.0000712871551514 sec
[2020-01-24 17:50:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:50:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:50:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:50:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:51:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:51:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:51:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:51:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:51:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f991953f208>,), **{}) took: 1.9043588638305664 sec
[2020-01-24 17:51:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:51:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 17:52:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 17:52:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 17:52:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 17:52:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 17:52:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 17:52:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 17:52:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9639a49208>,), **{}) took: 1.889206886291504 sec
[2020-01-24 17:52:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 17:52:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:10:26] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 18:10:26] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 18:11:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 18:11:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 18:11:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 18:11:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 18:11:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcf389d0208>,), **{}) took: 2.0171358585357666 sec
[2020-01-24 18:11:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 18:11:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:13:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 18:13:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 18:14:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 18:14:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 18:14:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 18:14:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 18:14:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1514116208>,), **{}) took: 1.968595266342163 sec
[2020-01-24 18:14:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 18:14:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:16:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 18:16:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 18:16:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 18:16:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 18:17:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 18:17:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 18:17:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f94feb5d208>,), **{}) took: 1.920140027999878 sec
[2020-01-24 18:17:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 18:17:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:17:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 18:17:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 18:18:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 18:18:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 18:18:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 18:19:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 18:19:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1cb30c2208>,), **{}) took: 1.9262974262237549 sec
[2020-01-24 18:19:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 18:19:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:19:05] - slp - ERROR -- Current run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-24 18:19:05] - slp - ERROR -- Engine run is terminating due to exception: invalid argument 3: out of range at /pytorch/aten/src/TH/generic/THTensor.cpp:318.
[2020-01-24 18:19:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 18:19:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 18:19:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 18:19:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 18:19:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f28c2f29208>,), **{}) took: 1.9270613193511963 sec
[2020-01-24 18:19:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 18:19:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:20:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 18:20:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 18:20:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 18:20:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 18:20:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 18:20:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 18:20:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f36e0f101d0>,), **{}) took: 1.9211056232452393 sec
[2020-01-24 18:20:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 18:20:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:22:42] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 18:22:42] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 18:23:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 18:23:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 18:23:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 18:23:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 18:23:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7f74359208>,), **{}) took: 1.981299877166748 sec
[2020-01-24 18:23:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 18:23:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 18:25:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 18:25:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 19:11:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 19:11:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 19:11:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 19:12:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 19:12:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe6c814e208>,), **{}) took: 1.8806486129760742 sec
[2020-01-24 19:12:06] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 19:12:06] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 19:26:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 19:26:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 19:26:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 19:26:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 19:26:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 19:26:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 19:26:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa423424208>,), **{}) took: 1.8885200023651123 sec
[2020-01-24 19:26:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 19:26:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 19:28:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 19:28:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 19:28:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 19:28:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 19:28:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 19:28:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 19:28:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f47df757208>,), **{}) took: 1.8818848133087158 sec
[2020-01-24 19:28:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 19:28:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 19:46:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 19:46:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 19:46:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 19:46:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 19:46:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 19:46:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 19:46:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff8268ea208>,), **{}) took: 1.9144015312194824 sec
[2020-01-24 19:46:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 19:46:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 19:48:18] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 19:48:18] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 19:48:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 19:48:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 19:48:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 19:48:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 19:48:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f587f288208>,), **{}) took: 1.9126472473144531 sec
[2020-01-24 19:48:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 19:48:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 19:48:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 19:48:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 19:48:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 19:48:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 19:48:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 19:49:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 19:49:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3b3a1e59e8>,), **{}) took: 1.9118216037750244 sec
[2020-01-24 19:49:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 19:49:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 19:49:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 19:49:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 19:49:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 19:49:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 19:49:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 19:49:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 19:49:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f85f7bec898>,), **{}) took: 1.8843178749084473 sec
[2020-01-24 19:50:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 19:50:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 19:54:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 19:54:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 20:06:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 20:06:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 20:06:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 20:06:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 20:06:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7375e361d0>,), **{}) took: 1.9101974964141846 sec
[2020-01-24 20:06:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 20:06:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 20:07:25] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 20:07:25] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 20:07:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 20:07:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 20:07:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 20:07:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 20:07:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4e4d2fb1d0>,), **{}) took: 1.905792236328125 sec
[2020-01-24 20:07:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 20:07:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 20:08:34] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 20:08:34] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 20:09:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 20:09:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 20:09:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 20:09:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 20:09:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f97505a8208>,), **{}) took: 1.8838822841644287 sec
[2020-01-24 20:09:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 20:09:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 20:19:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 20:19:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 20:19:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 20:19:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 20:19:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 20:19:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 20:19:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f614e20f208>,), **{}) took: 1.8873155117034912 sec
[2020-01-24 20:20:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 20:20:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 20:20:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 20:20:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 20:20:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 20:20:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 20:20:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 20:20:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 20:20:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2534be9208>,), **{}) took: 1.8899714946746826 sec
[2020-01-24 20:20:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 20:20:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 21:32:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 21:32:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 21:32:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 21:32:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 21:32:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdb8a06cda0>,), **{}) took: 1.880176305770874 sec
[2020-01-24 21:32:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 21:32:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 21:37:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 21:37:02] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 21:37:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 21:37:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 21:37:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 21:37:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 21:37:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8ed5797208>,), **{}) took: 1.9075539112091064 sec
[2020-01-24 21:37:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 21:37:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 22:05:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 22:05:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 22:07:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 22:07:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 22:07:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 22:07:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 22:07:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5394199208>,), **{}) took: 1.909013032913208 sec
[2020-01-24 22:07:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 22:07:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 22:11:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 22:11:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 22:11:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 22:11:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 22:11:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 22:11:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 22:11:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb07ef8e208>,), **{}) took: 1.8793718814849854 sec
[2020-01-24 22:11:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 22:11:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 22:11:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 22:11:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 22:26:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 22:26:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 22:26:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 22:26:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 22:26:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f613855f208>,), **{}) took: 1.9260144233703613 sec
[2020-01-24 22:26:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 22:26:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 22:40:48] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 22:40:48] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 22:42:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 22:42:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 22:42:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 22:42:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 22:42:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb0c24c1208>,), **{}) took: 1.9064345359802246 sec
[2020-01-24 22:42:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 22:42:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 23:02:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 23:02:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 23:02:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 23:02:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 23:02:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 23:02:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 23:02:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f715f48bef0>,), **{}) took: 1.9097888469696045 sec
[2020-01-24 23:02:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 23:02:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 23:11:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 23:11:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 23:12:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 23:12:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 23:12:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 23:12:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 23:12:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f95af49d208>,), **{}) took: 1.908944845199585 sec
[2020-01-24 23:12:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 23:12:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 23:22:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 23:22:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 23:22:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 23:22:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 23:22:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 23:22:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 23:22:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4551c6c208>,), **{}) took: 1.8818714618682861 sec
[2020-01-24 23:23:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 23:23:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 23:25:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 23:25:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 23:28:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 23:28:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 23:28:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 23:28:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 23:28:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f73d3481208>,), **{}) took: 1.9114468097686768 sec
[2020-01-24 23:28:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 23:28:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 23:41:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 23:41:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 23:42:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 23:42:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 23:42:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 23:42:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 23:42:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 23:42:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 23:42:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc1313b71d0>,), **{}) took: 1.9059698581695557 sec
[2020-01-24 23:42:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 23:42:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-24 23:48:57] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-24 23:48:57] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-24 23:49:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-24 23:49:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-24 23:49:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-24 23:49:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-24 23:49:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f03271cf208>,), **{}) took: 1.88392972946167 sec
[2020-01-24 23:49:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-24 23:49:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:06:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:06:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:06:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:06:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:06:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:06:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:06:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0f57fcb0f0>,), **{}) took: 1.9059810638427734 sec
[2020-01-25 00:06:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:06:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:08:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:08:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:09:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:09:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:09:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:09:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:09:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f840ef189e8>,), **{}) took: 1.881659746170044 sec
[2020-01-25 00:09:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:09:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:12:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:12:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:12:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:12:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:12:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:12:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:12:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa61537b208>,), **{}) took: 1.9187719821929932 sec
[2020-01-25 00:12:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:12:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:16:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:16:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:17:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:17:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:17:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:17:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:17:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd9ca319208>,), **{}) took: 1.8865549564361572 sec
[2020-01-25 00:17:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:17:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:17:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:17:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:17:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:17:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:17:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:17:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:17:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fecb5725208>,), **{}) took: 1.8854427337646484 sec
[2020-01-25 00:18:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:18:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:19:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:19:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:19:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:19:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:19:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:19:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:19:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f05cb6a3208>,), **{}) took: 1.9049334526062012 sec
[2020-01-25 00:19:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:19:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:25:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:25:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:25:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:25:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:25:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:25:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:25:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff233c89208>,), **{}) took: 1.8999547958374023 sec
[2020-01-25 00:25:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:25:42] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:28:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:28:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:30:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:30:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:30:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:30:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:30:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4f8ac19208>,), **{}) took: 1.9152157306671143 sec
[2020-01-25 00:30:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:30:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:32:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:32:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:37:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:37:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:37:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:37:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:37:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f019814d208>,), **{}) took: 1.884364128112793 sec
[2020-01-25 00:37:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:37:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:43:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:43:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:43:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:43:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:43:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:43:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:43:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4c25e01208>,), **{}) took: 1.8806419372558594 sec
[2020-01-25 00:43:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:43:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:44:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:44:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:45:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:45:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:45:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:45:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:45:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f50e31b0da0>,), **{}) took: 1.8852686882019043 sec
[2020-01-25 00:45:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:45:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:45:20] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:45:20] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 00:45:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 00:45:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 00:45:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 00:45:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 00:45:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f97a64c0208>,), **{}) took: 1.9120020866394043 sec
[2020-01-25 00:45:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 00:45:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 00:48:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 00:48:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 12:49:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 12:49:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 12:49:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 12:49:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 12:49:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3c099eb208>,), **{}) took: 1.8794941902160645 sec
[2020-01-25 12:49:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 12:49:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 12:50:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 12:50:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 12:58:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 12:58:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 12:58:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 12:58:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 12:58:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6efd271208>,), **{}) took: 1.8817811012268066 sec
[2020-01-25 12:58:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 12:58:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 12:59:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 12:59:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 12:59:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 12:59:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 12:59:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 12:59:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 12:59:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f780d641208>,), **{}) took: 1.8797435760498047 sec
[2020-01-25 12:59:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 12:59:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 13:00:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 13:00:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 13:01:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 13:01:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 13:01:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 13:01:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 13:01:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ffab42d1208>,), **{}) took: 2.042351484298706 sec
[2020-01-25 13:01:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 13:01:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 13:09:48] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 13:09:48] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 13:10:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 13:10:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 13:10:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 13:10:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 13:10:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faf52b83208>,), **{}) took: 1.9082860946655273 sec
[2020-01-25 13:10:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 13:10:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 13:11:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 13:11:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 13:19:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 13:19:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 13:19:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 13:19:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 13:19:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff91a86d208>,), **{}) took: 1.9043142795562744 sec
[2020-01-25 13:19:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 13:19:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 13:19:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 13:19:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 13:19:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 13:19:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 13:19:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 13:19:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 13:19:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f87e9833e10>,), **{}) took: 1.909787654876709 sec
[2020-01-25 13:20:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 13:20:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 13:38:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 13:38:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 13:38:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 13:38:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 13:38:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 13:38:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 13:38:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa28aef3208>,), **{}) took: 1.9091672897338867 sec
[2020-01-25 13:38:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 13:38:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 13:52:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 13:52:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 13:55:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 13:55:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 13:55:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 13:55:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 13:55:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fabec755208>,), **{}) took: 1.9067051410675049 sec
[2020-01-25 13:55:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 13:55:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 13:56:26] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 13:56:26] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 13:56:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 13:56:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 13:56:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 13:56:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 13:56:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f48111541d0>,), **{}) took: 1.8862924575805664 sec
[2020-01-25 13:56:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 13:56:42] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:00:07] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:00:07] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 14:00:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:00:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:00:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:00:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:00:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb5837f3208>,), **{}) took: 1.908517837524414 sec
[2020-01-25 14:00:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:00:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:01:20] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:01:20] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 14:01:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:01:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:01:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:01:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:01:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7febb77719e8>,), **{}) took: 1.9042236804962158 sec
[2020-01-25 14:02:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:02:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:02:22] - slp - ERROR -- Current run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-25 14:02:22] - slp - ERROR -- Engine run is terminating due to exception: only one element tensors can be converted to Python scalars.
[2020-01-25 14:03:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:03:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:03:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:03:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:03:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7ac8ee31d0>,), **{}) took: 1.890151023864746 sec
[2020-01-25 14:03:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:03:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:06:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:06:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 14:07:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:07:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:07:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:07:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:07:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbb8fc20208>,), **{}) took: 1.8812017440795898 sec
[2020-01-25 14:07:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:07:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:09:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:09:37] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-25 14:11:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:11:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:11:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:11:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:11:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2bcceb1278>,), **{}) took: 2.0543863773345947 sec
[2020-01-25 14:11:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:11:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:11:24] - slp - ERROR -- Current run is terminating due to exception: Expected object of backend CUDA but got backend CPU for argument #3 'index'.
[2020-01-25 14:11:24] - slp - ERROR -- Engine run is terminating due to exception: Expected object of backend CUDA but got backend CPU for argument #3 'index'.
[2020-01-25 14:13:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:13:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:13:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:13:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:13:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ffb17d66278>,), **{}) took: 2.052361249923706 sec
[2020-01-25 14:13:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:13:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:13:31] - slp - ERROR -- Current run is terminating due to exception: Expected object of backend CUDA but got backend CPU for argument #3 'index'.
[2020-01-25 14:13:31] - slp - ERROR -- Engine run is terminating due to exception: Expected object of backend CUDA but got backend CPU for argument #3 'index'.
[2020-01-25 14:17:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:17:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:17:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:17:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:17:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4b6316d240>,), **{}) took: 1.9900965690612793 sec
[2020-01-25 14:17:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:17:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:17:44] - slp - ERROR -- Current run is terminating due to exception: Expected object of backend CUDA but got backend CPU for argument #3 'index'.
[2020-01-25 14:17:44] - slp - ERROR -- Engine run is terminating due to exception: Expected object of backend CUDA but got backend CPU for argument #3 'index'.
[2020-01-25 14:18:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:18:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:18:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:18:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:18:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc8dae8a240>,), **{}) took: 1.901085376739502 sec
[2020-01-25 14:18:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:18:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:20:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:20:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 14:20:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:20:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:20:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:20:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:20:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f52c2175240>,), **{}) took: 2.036938428878784 sec
[2020-01-25 14:20:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:20:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:21:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:21:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 14:21:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:21:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:21:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:21:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:21:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1c9cc70208>,), **{}) took: 2.044203758239746 sec
[2020-01-25 14:21:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:21:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:22:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:22:23] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-25 14:22:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:22:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:22:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:22:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:22:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6a5a111240>,), **{}) took: 2.040839433670044 sec
[2020-01-25 14:22:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:22:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:23:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:23:54] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-25 14:24:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:24:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:24:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:24:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:24:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff1a0cf3208>,), **{}) took: 1.9014177322387695 sec
[2020-01-25 14:24:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:24:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 14:26:42] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-25 14:26:42] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-25 14:27:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-25 14:27:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-25 14:27:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-25 14:27:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-25 14:27:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd40c416240>,), **{}) took: 2.041234016418457 sec
[2020-01-25 14:27:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 14:27:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 18:58:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 18:58:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-25 22:53:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-25 22:53:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-26 00:58:08] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-26 00:58:08] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-27 22:00:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:00:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:01:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:01:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:01:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:01:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:01:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:02:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:02:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6e42779240>,), **{}) took: 35.045124769210815 sec
[2020-01-27 22:02:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:02:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:02:34] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-01-27 22:02:34] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-01-27 22:09:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:09:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:09:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:09:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:09:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f19c15e6240>,), **{}) took: 2.0788776874542236 sec
[2020-01-27 22:09:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:09:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:12:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 22:12:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 22:12:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:12:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:12:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:12:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:12:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5f6e534da0>,), **{}) took: 2.031697988510132 sec
[2020-01-27 22:12:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:12:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:12:47] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-01-27 22:12:47] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-01-27 22:27:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:27:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:27:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:27:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:27:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb0858eb240>,), **{}) took: 1.9910576343536377 sec
[2020-01-27 22:28:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:28:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:28:05] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-01-27 22:28:05] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-01-27 22:28:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:28:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:28:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:29:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:29:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb20d6a7240>,), **{}) took: 2.0661747455596924 sec
[2020-01-27 22:29:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:29:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:30:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 22:30:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 22:37:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:37:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:37:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:37:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:37:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff27e0d6240>,), **{}) took: 2.0416269302368164 sec
[2020-01-27 22:37:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:37:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:47:18] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 22:47:18] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 22:49:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:49:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:49:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:49:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:49:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3bee837240>,), **{}) took: 2.0351157188415527 sec
[2020-01-27 22:49:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:49:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:52:07] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 22:52:07] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 22:52:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:52:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:52:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:52:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:52:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc8ceb5c240>,), **{}) took: 2.0533924102783203 sec
[2020-01-27 22:52:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:52:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:54:14] - slp - ERROR -- Current run is terminating due to exception: local variable 'output_list_text' referenced before assignment.
[2020-01-27 22:54:14] - slp - ERROR -- Engine run is terminating due to exception: local variable 'output_list_text' referenced before assignment.
[2020-01-27 22:55:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:55:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:55:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:55:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:55:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9f73064240>,), **{}) took: 2.0591602325439453 sec
[2020-01-27 22:55:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:55:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 22:59:12] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 22:59:12] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 22:59:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 22:59:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 22:59:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 22:59:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 22:59:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f13ecdfa208>,), **{}) took: 2.022819995880127 sec
[2020-01-27 22:59:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 22:59:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 23:10:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 23:10:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 23:10:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 23:10:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 23:10:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 23:10:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 23:10:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0b45412240>,), **{}) took: 2.0595362186431885 sec
[2020-01-27 23:11:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 23:11:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 23:11:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 23:11:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 23:11:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6dab76e240>,), **{}) took: 2.0479493141174316 sec
[2020-01-27 23:11:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 23:11:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 23:13:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 23:13:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 23:16:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 23:16:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 23:16:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 23:16:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 23:16:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4184d86240>,), **{}) took: 2.0616610050201416 sec
[2020-01-27 23:16:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 23:16:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 23:25:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 23:25:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 23:36:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 23:36:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 23:36:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 23:36:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 23:36:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2e86c84240>,), **{}) took: 2.0592715740203857 sec
[2020-01-27 23:36:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 23:36:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 23:36:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 23:36:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 23:36:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f03182dc240>,), **{}) took: 2.052051305770874 sec
[2020-01-27 23:36:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 23:36:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 23:37:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 23:37:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-27 23:38:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 23:38:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 23:38:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 23:38:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 23:38:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa604524e10>,), **{}) took: 2.05646014213562 sec
[2020-01-27 23:38:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 23:38:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-27 23:38:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-27 23:38:56] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-27 23:39:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-27 23:39:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-27 23:39:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-27 23:39:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-27 23:39:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9b7c37e240>,), **{}) took: 2.046464204788208 sec
[2020-01-27 23:39:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-27 23:39:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-28 07:44:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-28 07:44:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-28 12:54:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-28 12:54:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-29 01:15:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-29 01:15:21] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-29 01:15:21] - slp - ERROR -- Engine run is terminating due to exception: 'loss'.
[2020-01-29 01:15:21] - slp - ERROR -- Engine run is terminating due to exception: 'loss'.
[2020-01-29 01:16:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-29 01:16:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-29 01:16:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-29 01:16:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-29 01:16:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd324c19f60>,), **{}) took: 2.3637776374816895 sec
[2020-01-29 01:16:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-29 01:16:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-29 22:48:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-29 22:48:54] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-29 22:49:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-29 22:49:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-30 07:49:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-30 07:49:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-30 14:34:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-30 14:34:46] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-30 14:34:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-30 14:34:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 00:02:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 00:02:37] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-31 00:02:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 00:02:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 14:54:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 14:54:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 14:55:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 14:55:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 14:55:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 14:55:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 14:55:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5bd684b278>,), **{}) took: 42.6510591506958 sec
[2020-01-31 14:56:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 14:56:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:02:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:02:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:04:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:04:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:04:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:04:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:04:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f55bfef3278>,), **{}) took: 9.058682918548584 sec
[2020-01-31 15:05:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:05:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:05:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:05:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:05:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:05:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:05:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:06:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:06:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6cfba51278>,), **{}) took: 10.198540210723877 sec
[2020-01-31 15:06:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:06:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:09:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:09:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:09:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:09:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:09:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:09:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:09:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa238887278>,), **{}) took: 10.532362937927246 sec
[2020-01-31 15:09:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:09:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:13:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:13:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:15:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:15:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:15:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:15:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:15:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe488bcc278>,), **{}) took: 9.008930683135986 sec
[2020-01-31 15:15:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:15:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:18:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:18:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:18:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:18:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:18:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:18:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:18:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f627d816278>,), **{}) took: 7.016849756240845 sec
[2020-01-31 15:19:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:19:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:19:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:19:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:19:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:19:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:19:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:19:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:19:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa6564f6278>,), **{}) took: 7.061891555786133 sec
[2020-01-31 15:20:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:20:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:21:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:21:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:22:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:22:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:22:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:22:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:22:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd38aefd278>,), **{}) took: 6.945385456085205 sec
[2020-01-31 15:22:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:22:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:23:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:23:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:25:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:25:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:25:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:25:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:25:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f176afb8278>,), **{}) took: 2.0365521907806396 sec
[2020-01-31 15:25:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:25:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:27:03] - slp - ERROR -- Current run is terminating due to exception: list index out of range.
[2020-01-31 15:27:03] - slp - ERROR -- Engine run is terminating due to exception: list index out of range.
[2020-01-31 15:27:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:28:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:28:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:28:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:28:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0e1bfb5e10>,), **{}) took: 2.0272011756896973 sec
[2020-01-31 15:28:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:28:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:31:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:31:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:31:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:31:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:31:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:31:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:31:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faf8c985278>,), **{}) took: 2.039583921432495 sec
[2020-01-31 15:31:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:31:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:37:48] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:37:48] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:37:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:37:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:37:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:37:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:37:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f435ac9b278>,), **{}) took: 1.9276745319366455 sec
[2020-01-31 15:38:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:38:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:48:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:48:52] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:48:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:48:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:48:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:48:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:48:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f22b74fe278>,), **{}) took: 2.002213954925537 sec
[2020-01-31 15:49:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:49:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:49:07] - slp - ERROR -- Current run is terminating due to exception: local variable 'items' referenced before assignment.
[2020-01-31 15:49:07] - slp - ERROR -- Engine run is terminating due to exception: local variable 'items' referenced before assignment.
[2020-01-31 15:49:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:49:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:49:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:49:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:49:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5ffe880240>,), **{}) took: 2.047189712524414 sec
[2020-01-31 15:49:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:49:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:51:25] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:51:25] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:51:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:51:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:51:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:51:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:51:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f071c5d3240>,), **{}) took: 2.0456960201263428 sec
[2020-01-31 15:51:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:51:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:52:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:52:52] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:53:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:53:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:54:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:54:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:54:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa4c98a7278>,), **{}) took: 2.064312696456909 sec
[2020-01-31 15:54:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:54:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 15:57:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 15:57:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 15:57:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 15:57:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 15:57:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 15:57:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 15:57:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f901a360278>,), **{}) took: 1.9553539752960205 sec
[2020-01-31 15:57:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 15:57:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 16:18:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 16:18:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 16:18:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 16:18:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 16:18:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 16:18:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 16:18:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4d54a05278>,), **{}) took: 2.1083409786224365 sec
[2020-01-31 16:18:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 16:18:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 16:32:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 16:32:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 16:32:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 16:32:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 16:32:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 16:32:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 16:32:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 16:32:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2412273278>,), **{}) took: 1.9683644771575928 sec
[2020-01-31 16:32:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 16:32:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 16:48:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 16:48:14] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-31 16:48:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 16:48:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 16:48:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 16:48:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 16:48:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3f0c847278>,), **{}) took: 1.991323471069336 sec
[2020-01-31 16:48:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 16:48:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 16:51:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 16:51:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 16:51:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 16:51:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 16:51:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 16:51:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 16:51:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f413d8b5240>,), **{}) took: 1.9994807243347168 sec
[2020-01-31 16:51:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 16:51:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 16:51:58] - slp - ERROR -- Current run is terminating due to exception: local variable 're' referenced before assignment.
[2020-01-31 16:51:58] - slp - ERROR -- Engine run is terminating due to exception: local variable 're' referenced before assignment.
[2020-01-31 16:52:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 16:52:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 16:52:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 16:52:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 16:52:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f037e237278>,), **{}) took: 1.9656782150268555 sec
[2020-01-31 16:52:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 16:52:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 16:52:36] - slp - ERROR -- Current run is terminating due to exception: 'NoneType' object has no attribute 'groups'.
[2020-01-31 16:52:36] - slp - ERROR -- Engine run is terminating due to exception: 'NoneType' object has no attribute 'groups'.
[2020-01-31 16:53:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 16:53:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 16:53:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 16:53:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 16:53:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8332468278>,), **{}) took: 2.0040700435638428 sec
[2020-01-31 16:53:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 16:53:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 16:59:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 16:59:23] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 16:59:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 16:59:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 16:59:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 16:59:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 16:59:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2d678ce240>,), **{}) took: 2.031184196472168 sec
[2020-01-31 16:59:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 16:59:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 17:08:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 17:08:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 17:09:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 17:09:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 17:09:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 17:09:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 17:09:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f607b66c278>,), **{}) took: 1.9607503414154053 sec
[2020-01-31 17:09:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 17:09:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 17:12:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 17:12:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-01-31 17:12:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 17:12:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 17:12:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 17:12:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 17:12:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff756911278>,), **{}) took: 1.979306936264038 sec
[2020-01-31 17:12:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 17:12:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 17:12:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 17:12:56] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-31 17:13:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-01-31 17:13:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-01-31 17:13:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-01-31 17:13:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-01-31 17:13:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1ffa701278>,), **{}) took: 1.9967937469482422 sec
[2020-01-31 17:13:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 17:13:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-01-31 20:59:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-01-31 20:59:14] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-01-31 20:59:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-01-31 20:59:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-01 01:55:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-01 01:55:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-01 06:18:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-01 06:18:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-01 10:51:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-01 10:51:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-03 22:29:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 22:29:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 22:29:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 22:29:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 22:29:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6167f22240>,), **{}) took: 30.856504440307617 sec
[2020-02-03 22:30:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-03 22:30:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-03 22:35:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-03 22:35:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-03 22:36:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 22:36:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 22:36:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 22:37:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 22:37:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe981718208>,), **{}) took: 8.855782985687256 sec
[2020-02-03 22:37:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-03 22:37:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-03 22:40:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-03 22:40:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-03 22:40:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 22:40:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 22:40:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 22:40:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 22:40:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5cecb64240>,), **{}) took: 8.917181491851807 sec
[2020-02-03 22:41:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-03 22:41:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-03 22:43:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-03 22:43:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-03 22:43:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 22:43:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 22:43:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 22:43:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 22:43:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f30ca91f240>,), **{}) took: 8.946137428283691 sec
[2020-02-03 22:44:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-03 22:44:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-03 22:53:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-03 22:53:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-03 22:54:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 22:54:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 22:54:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 22:54:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 22:54:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6fc5b05da0>,), **{}) took: 9.093705654144287 sec
[2020-02-03 22:54:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-03 22:54:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-03 22:56:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-03 22:56:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-03 22:56:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 22:56:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 22:56:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 22:56:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 22:56:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efdcd7bcda0>,), **{}) took: 9.007044315338135 sec
[2020-02-03 22:56:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-03 22:56:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-03 23:07:33] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-03 23:07:33] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-03 23:08:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 23:08:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 23:08:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 23:09:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 23:09:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6359717240>,), **{}) took: 8.705314874649048 sec
[2020-02-03 23:18:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 23:18:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 23:18:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 23:19:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 23:19:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4029bf0ef0>,), **{}) took: 8.85993218421936 sec
[2020-02-03 23:41:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 23:41:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 23:41:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 23:42:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 23:42:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc09f9b8240>,), **{}) took: 8.913468837738037 sec
[2020-02-03 23:50:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 23:50:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 23:50:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 23:50:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 23:50:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f890a182240>,), **{}) took: 8.905367374420166 sec
[2020-02-03 23:50:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-03 23:50:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-03 23:50:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-03 23:50:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-03 23:50:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f08e52b8e48>,), **{}) took: 8.948598146438599 sec
[2020-02-03 23:51:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-03 23:51:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-04 00:03:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-04 00:03:36] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-04 00:09:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-04 00:09:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-04 00:09:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-04 00:09:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-04 00:09:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcf92bda240>,), **{}) took: 9.342505693435669 sec
[2020-02-04 00:09:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-04 00:09:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-04 05:12:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-04 05:12:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-04 09:05:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-04 09:05:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-04 12:48:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-04 12:48:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-04 15:43:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-04 15:43:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 01:08:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:08:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:08:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:08:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:08:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7facaeb12240>,), **{}) took: 30.866979837417603 sec
[2020-02-05 01:13:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:13:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:14:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:14:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:14:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc9c270b240>,), **{}) took: 2.068309783935547 sec
[2020-02-05 01:14:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:14:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:15:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:15:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:15:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37424c6208>,), **{}) took: 2.0631563663482666 sec
[2020-02-05 01:15:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:15:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:15:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:15:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:15:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f024fb81208>,), **{}) took: 2.044461965560913 sec
[2020-02-05 01:15:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 01:15:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 01:18:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 01:18:56] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-05 01:19:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:19:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:19:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:19:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:19:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc57f566f60>,), **{}) took: 2.035407066345215 sec
[2020-02-05 01:19:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 01:19:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 01:24:18] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 01:24:18] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-05 01:25:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:25:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:25:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:25:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:25:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f49bd8a9240>,), **{}) took: 2.05027174949646 sec
[2020-02-05 01:25:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 01:25:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 01:28:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 01:28:04] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-05 01:28:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:28:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:29:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:29:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:29:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:29:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:29:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbcd2c2b1d0>,), **{}) took: 2.0432655811309814 sec
[2020-02-05 01:29:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 01:29:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 01:30:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 01:30:37] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-05 01:32:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:32:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:32:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:32:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:32:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc6831aecc0>,), **{}) took: 2.0040853023529053 sec
[2020-02-05 01:32:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 01:32:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 01:32:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 01:32:59] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-05 01:33:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:33:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:33:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:33:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:33:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fad9c1d21d0>,), **{}) took: 2.0478515625 sec
[2020-02-05 01:33:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 01:33:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 01:33:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 01:33:46] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-05 01:34:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 01:34:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 01:34:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 01:34:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 01:34:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fba6cb49198>,), **{}) took: 2.1742894649505615 sec
[2020-02-05 01:34:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 01:34:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 03:08:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 03:08:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 04:22:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 04:22:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 06:11:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 06:11:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 08:15:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 08:15:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 23:26:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 23:26:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 23:26:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 23:27:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 23:27:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f96bcd4e240>,), **{}) took: 44.58410906791687 sec
[2020-02-05 23:27:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 23:27:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 23:31:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 23:31:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-05 23:31:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 23:31:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 23:31:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 23:31:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 23:31:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f714e7ee240>,), **{}) took: 19.45603895187378 sec
[2020-02-05 23:32:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 23:32:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 23:54:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 23:54:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-05 23:54:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 23:54:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 23:54:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 23:54:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 23:54:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f086e64b208>,), **{}) took: 14.647901773452759 sec
[2020-02-05 23:55:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 23:55:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-05 23:55:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-05 23:55:23] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-05 23:55:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-05 23:55:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-05 23:55:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-05 23:56:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-05 23:56:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdb9d23df28>,), **{}) took: 14.579310655593872 sec
[2020-02-05 23:56:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-05 23:56:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 00:05:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 00:05:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 00:06:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 00:06:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 00:06:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 00:06:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 00:06:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f25bf3561d0>,), **{}) took: 14.752898454666138 sec
[2020-02-06 00:07:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 00:07:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 01:04:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 01:04:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 01:49:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 01:49:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 02:16:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 02:16:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 02:17:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 02:17:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 02:17:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 02:18:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 02:18:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f17bb832240>,), **{}) took: 27.02204728126526 sec
[2020-02-06 02:18:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 02:18:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 02:20:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 02:20:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 02:20:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 02:20:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 02:20:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 02:20:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 02:20:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 02:20:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 02:20:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7feff2612240>,), **{}) took: 15.61784553527832 sec
[2020-02-06 02:21:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 02:21:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 02:29:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 02:29:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 02:41:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 02:41:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 02:41:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 02:41:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 02:41:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 02:42:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 02:42:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1d62a3e240>,), **{}) took: 24.615978956222534 sec
[2020-02-06 02:42:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 02:42:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 02:43:25] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 02:43:25] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-06 02:43:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 02:43:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 02:43:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 02:44:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 02:44:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7eb73fc240>,), **{}) took: 24.711315870285034 sec
[2020-02-06 02:44:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 02:44:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 04:21:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 04:21:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 06:22:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 06:22:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 11:10:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 11:10:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:24:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:24:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:26:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:26:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:26:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:26:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:26:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc882fd4278>,), **{}) took: 32.7521538734436 sec
[2020-02-06 12:27:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:27:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:28:35] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [1200 x 600], m2: [712 x 712] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-02-06 12:28:35] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [1200 x 600], m2: [712 x 712] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-02-06 12:28:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:28:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:28:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:29:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:29:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:29:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:29:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:29:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc302090dd8>,), **{}) took: 17.487666606903076 sec
[2020-02-06 12:29:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:29:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:30:04] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [160 x 600], m2: [712 x 712] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-02-06 12:30:04] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [160 x 600], m2: [712 x 712] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-02-06 12:30:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:30:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:30:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:30:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:30:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1a50bfe278>,), **{}) took: 18.621910095214844 sec
[2020-02-06 12:31:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:31:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:32:42] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 12:32:42] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 12:33:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:33:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:33:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:33:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:33:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd2c1ef1240>,), **{}) took: 27.980815887451172 sec
[2020-02-06 12:34:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:34:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:35:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 12:35:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 12:36:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:36:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:36:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:36:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:36:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0f4339f278>,), **{}) took: 25.71838116645813 sec
[2020-02-06 12:37:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:37:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:37:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 12:37:56] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 12:38:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:38:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:38:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:38:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:38:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6a1e3ef278>,), **{}) took: 25.40787672996521 sec
[2020-02-06 12:39:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:39:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:39:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 12:39:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 12:39:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:39:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:39:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:40:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:40:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbc73d93240>,), **{}) took: 25.367543697357178 sec
[2020-02-06 12:40:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:40:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:41:26] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 12:41:26] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 12:43:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:43:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:43:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:43:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:43:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f023bbac240>,), **{}) took: 25.24850630760193 sec
[2020-02-06 12:44:06] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:44:06] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 12:46:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 12:46:19] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-06 12:46:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 12:46:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 12:46:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 12:47:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 12:47:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc503970278>,), **{}) took: 25.15025544166565 sec
[2020-02-06 12:47:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 12:47:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
    (gate): Linear(in_features=112, out_features=600, bias=True)
    (sigmoid): Sigmoid()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 14:34:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 14:34:35] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-06 14:35:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 14:35:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(712, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=712, out_features=712, bias=True)
    (context): Linear(in_features=712, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
    (lexicons): LexiconFeatures()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:41:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:41:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:41:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:41:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:41:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f25c4705198>,), **{}) took: 35.814696311950684 sec
[2020-02-06 16:42:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:42:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:42:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:42:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:42:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6dc36d6208>,), **{}) took: 2.0531809329986572 sec
[2020-02-06 16:42:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:42:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:42:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:42:51] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-02-06 16:43:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:43:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:43:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:43:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:43:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f76a22b3208>,), **{}) took: 2.029998540878296 sec
[2020-02-06 16:43:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:43:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:43:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:43:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:43:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f77b3898208>,), **{}) took: 1.9882476329803467 sec
[2020-02-06 16:43:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:43:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:44:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:44:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 16:44:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:44:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:44:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:44:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:44:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdbff614208>,), **{}) took: 2.0264670848846436 sec
[2020-02-06 16:45:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:45:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:46:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:46:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 16:48:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:48:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:48:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:48:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:48:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f38924d0208>,), **{}) took: 2.203629493713379 sec
[2020-02-06 16:48:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:48:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:48:20] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:48:20] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 16:48:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:48:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:48:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:48:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:48:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe0bb437208>,), **{}) took: 2.9795682430267334 sec
[2020-02-06 16:48:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:48:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:49:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:49:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 16:50:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:50:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:50:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:50:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:50:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd8adcc1208>,), **{}) took: 4.980620861053467 sec
[2020-02-06 16:50:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:50:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:52:31] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:52:31] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 16:52:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:52:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:52:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:52:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:52:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f877cdf61d0>,), **{}) took: 4.405555963516235 sec
[2020-02-06 16:53:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:53:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:53:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:53:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 16:53:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 16:53:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 16:53:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 16:54:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 16:54:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2fea85d208>,), **{}) took: 4.796681642532349 sec
[2020-02-06 16:54:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 16:54:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 16:55:18] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 16:55:18] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 17:03:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 17:03:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 17:03:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 17:03:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 17:03:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fedb13a6208>,), **{}) took: 5.4949517250061035 sec
[2020-02-06 17:03:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 17:03:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 17:06:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 17:06:02] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 17:06:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 17:06:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 17:06:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 17:06:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 17:06:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fda47cfd208>,), **{}) took: 5.728274583816528 sec
[2020-02-06 17:06:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 17:06:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 17:08:58] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 17:08:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 20:53:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 20:53:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 20:53:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 20:53:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 20:53:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f76a2fae240>,), **{}) took: 11.75411868095398 sec
[2020-02-06 20:53:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 20:53:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (model): DRNN(
      (cells): Sequential(
        (0): GRU(300, 300)
        (1): GRU(300, 300)
      )
    )
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 20:54:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 20:54:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 20:58:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 20:58:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 20:58:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 20:58:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 20:58:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f29b4276240>,), **{}) took: 11.623600006103516 sec
[2020-02-06 21:10:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:10:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:10:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:10:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:10:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f03cc950240>,), **{}) took: 12.59947681427002 sec
[2020-02-06 21:10:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:10:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:13:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:13:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:15:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:15:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:15:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:15:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:15:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f05329b1240>,), **{}) took: 12.528153657913208 sec
[2020-02-06 21:15:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:15:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:15:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:15:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:15:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe45f5b5240>,), **{}) took: 12.619414567947388 sec
[2020-02-06 21:16:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:16:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:21:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:21:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:21:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:21:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:21:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:21:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:21:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbad7e97240>,), **{}) took: 12.588388442993164 sec
[2020-02-06 21:21:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:21:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:25:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:25:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:27:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:27:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:27:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:27:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:27:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbdb55cb240>,), **{}) took: 12.517778635025024 sec
[2020-02-06 21:27:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:27:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:28:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:28:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:29:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:29:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:29:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:30:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:30:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff74c520240>,), **{}) took: 23.003610849380493 sec
[2020-02-06 21:30:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:30:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:33:28] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:33:28] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:33:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:33:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:33:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:33:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:33:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f391f8e8278>,), **{}) took: 11.507363319396973 sec
[2020-02-06 21:34:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:34:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:35:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:35:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:36:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:36:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:36:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:36:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:36:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f73b0549278>,), **{}) took: 11.490503311157227 sec
[2020-02-06 21:36:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:36:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:37:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:37:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:37:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:37:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:37:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:37:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:37:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f95df4e6278>,), **{}) took: 11.540435791015625 sec
[2020-02-06 21:38:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:38:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:38:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:38:23] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:39:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:39:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:39:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:39:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:39:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd92675f208>,), **{}) took: 12.842383861541748 sec
[2020-02-06 21:41:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:41:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:41:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:41:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:41:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f59b4bbb208>,), **{}) took: 20.03447937965393 sec
[2020-02-06 21:41:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:41:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:42:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:42:59] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:49:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:49:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:49:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:49:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:49:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe3dad8a240>,), **{}) took: 13.324518203735352 sec
[2020-02-06 21:50:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:50:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:52:42] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:52:42] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:53:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:53:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:53:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:53:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:53:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f424cf31208>,), **{}) took: 17.562997341156006 sec
[2020-02-06 21:53:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:53:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:54:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:54:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-06 21:54:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-06 21:54:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-06 21:54:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-06 21:54:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-06 21:54:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff567301240>,), **{}) took: 18.706252813339233 sec
[2020-02-06 21:55:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-06 21:55:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-06 21:55:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-06 21:55:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 01:29:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:29:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:29:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:29:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:29:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcda89b9208>,), **{}) took: 1.9275891780853271 sec
[2020-02-07 01:29:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:29:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:31:14] - slp - ERROR -- Current run is terminating due to exception: 'HierAttNet' object has no attribute 'sent_att_net'.
[2020-02-07 01:31:14] - slp - ERROR -- Engine run is terminating due to exception: 'HierAttNet' object has no attribute 'sent_att_net'.
[2020-02-07 01:33:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:33:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:33:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:33:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:33:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7f60333240>,), **{}) took: 2.016108751296997 sec
[2020-02-07 01:33:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:33:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:33:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 01:33:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 01:33:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:33:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:33:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:33:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:33:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8850382dd8>,), **{}) took: 2.021761417388916 sec
[2020-02-07 01:33:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:33:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:33:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 01:33:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 01:34:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:34:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:34:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:34:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:34:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc6e3d0a240>,), **{}) took: 2.018551826477051 sec
[2020-02-07 01:34:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:34:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:34:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 01:34:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 01:34:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:34:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:34:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:34:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:34:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f480c363240>,), **{}) took: 1.9115667343139648 sec
[2020-02-07 01:35:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:35:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:42:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 01:42:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 01:43:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:43:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:43:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:43:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:43:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7feb52922eb8>,), **{}) took: 2.009385347366333 sec
[2020-02-07 01:43:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:43:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:43:15] - slp - ERROR -- Current run is terminating due to exception: too many indices for tensor of dimension 2.
[2020-02-07 01:43:15] - slp - ERROR -- Engine run is terminating due to exception: too many indices for tensor of dimension 2.
[2020-02-07 01:46:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:46:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:46:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:47:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:47:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:47:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:47:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:47:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff823a62e10>,), **{}) took: 2.0278570652008057 sec
[2020-02-07 01:47:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:47:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (sent): Linear(in_features=600, out_features=600, bias=True)
  (context): Linear(in_features=600, out_features=1, bias=False)
  (fc): Linear(in_features=600, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:47:31] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [248400 x 300], m2: [600 x 600] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-02-07 01:47:31] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [248400 x 300], m2: [600 x 600] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-02-07 01:51:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:51:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:51:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:51:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:51:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f89be779240>,), **{}) took: 2.018620014190674 sec
[2020-02-07 01:52:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:52:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:52:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:53:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:53:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f766761d208>,), **{}) took: 2.0242838859558105 sec
[2020-02-07 01:53:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:53:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (linear): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:54:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 01:54:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 01:54:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:54:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:54:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:54:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:54:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc19a14bef0>,), **{}) took: 1.955965518951416 sec
[2020-02-07 01:54:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:54:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (linear): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 01:55:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 01:55:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 01:55:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 01:55:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 01:55:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 01:55:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 01:55:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f76e037d208>,), **{}) took: 1.973539113998413 sec
[2020-02-07 01:55:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 01:55:42] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (linear): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 02:05:03] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 02:05:03] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 02:06:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 02:06:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 02:06:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 02:06:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 02:06:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa08b044208>,), **{}) took: 2.0063183307647705 sec
[2020-02-07 02:06:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 02:06:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (linear): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 02:07:58] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 02:07:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 02:08:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 02:08:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 02:08:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 02:08:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 02:08:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff97e0a6240>,), **{}) took: 1.9303598403930664 sec
[2020-02-07 02:08:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 02:08:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (linear): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 02:18:20] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 02:18:20] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 02:18:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 02:18:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 02:18:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 02:18:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 02:18:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3431ded240>,), **{}) took: 2.0901944637298584 sec
[2020-02-07 02:18:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 02:18:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (linear): Linear(in_features=300, out_features=2, bias=True)
  (sent): Linear(in_features=600, out_features=600, bias=True)
  (context): Linear(in_features=600, out_features=1, bias=False)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 02:18:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 02:18:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-02-07 02:19:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-02-07 02:19:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-02-07 02:19:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-02-07 02:19:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-02-07 02:19:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37c2ffba20>,), **{}) took: 1.9274003505706787 sec
[2020-02-07 02:19:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-02-07 02:19:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (model): DRNN(
    (cells): Sequential(
      (0): GRU(300, 300)
      (1): GRU(300, 300)
    )
  )
  (lookup): Embedding(2196024, 300)
  (linear): Linear(in_features=300, out_features=2, bias=True)
  (sent): Linear(in_features=300, out_features=300, bias=True)
  (context): Linear(in_features=300, out_features=1, bias=False)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-02-07 02:20:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-02-07 02:20:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-07 00:15:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:15:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:16:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:16:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:16:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:16:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:16:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37ab1a8358>,), **{}) took: 11.031548261642456 sec
[2020-03-07 00:21:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:21:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:21:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:21:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:21:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f19db556b38>,), **{}) took: 10.62618374824524 sec
[2020-03-07 00:22:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:22:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:22:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:22:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:22:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0e168a9320>,), **{}) took: 10.527326345443726 sec
[2020-03-07 00:23:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:23:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:23:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:24:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:24:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7fbe652320>,), **{}) took: 10.374846696853638 sec
[2020-03-07 00:24:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:24:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:24:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:24:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:24:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f89cbb3d320>,), **{}) took: 10.439955711364746 sec
[2020-03-07 00:26:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:26:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:26:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:26:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:26:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe6efc8aeb8>,), **{}) took: 10.440448999404907 sec
[2020-03-07 00:29:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:29:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:29:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:30:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:30:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f16ad88a320>,), **{}) took: 10.503740072250366 sec
[2020-03-07 00:30:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 00:30:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(760, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (fc): Linear(in_features=760, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=760, out_features=760, bias=True)
    (context): Linear(in_features=760, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 00:30:24] - slp - ERROR -- Current run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-03-07 00:30:24] - slp - ERROR -- Engine run is terminating due to exception: Expected hidden size (2, 8, 300), got (2, 8, 380).
[2020-03-07 00:30:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:30:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:30:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:31:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:31:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f34ab945320>,), **{}) took: 10.429361343383789 sec
[2020-03-07 00:31:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 00:31:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 00:31:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-07 00:31:38] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-07 00:32:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 00:32:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 00:32:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 00:32:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 00:32:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb559beea20>,), **{}) took: 10.491369247436523 sec
[2020-03-07 00:32:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 00:32:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 00:46:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-07 00:46:10] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-07 00:46:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 00:46:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 00:46:25] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-07 00:46:25] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-07 01:00:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 01:00:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 01:00:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 01:01:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 01:01:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 01:01:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 01:01:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 01:01:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6bcbc392e8>,), **{}) took: 11.23342776298523 sec
[2020-03-07 01:02:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 01:02:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 01:02:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-07 01:02:19] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-07 01:02:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 01:02:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 01:02:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 01:02:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 01:02:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0e4b7c7240>,), **{}) took: 11.361186265945435 sec
[2020-03-07 01:03:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 01:03:42] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 01:12:03] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-07 01:12:03] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-07 01:12:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 01:12:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 01:12:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 01:12:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 01:12:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4cec4f3240>,), **{}) took: 10.59517240524292 sec
[2020-03-07 01:13:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 01:13:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 02:31:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 02:31:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 03:21:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 03:21:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 03:57:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 03:57:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 05:06:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 05:06:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 17:05:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-07 17:05:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-07 17:05:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-07 17:05:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-07 17:05:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fac3111c2b0>,), **{}) took: 6.018030405044556 sec
[2020-03-07 17:06:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 17:06:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 18:06:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 18:06:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 18:53:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 18:53:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 20:03:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 20:03:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-07 20:46:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-07 20:46:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:12:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:12:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:12:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:12:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:12:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6779f142b0>,), **{}) took: 12.886795282363892 sec
[2020-03-09 21:13:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:13:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:13:31] - slp - ERROR -- Current run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-09 21:13:31] - slp - ERROR -- Engine run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-09 21:15:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:15:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:15:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:16:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:16:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe11afb4240>,), **{}) took: 5.128628492355347 sec
[2020-03-09 21:16:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:16:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:16:41] - slp - ERROR -- Current run is terminating due to exception: index out of range: Tried to access index 4 out of table with -1 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237.
[2020-03-09 21:16:41] - slp - ERROR -- Engine run is terminating due to exception: index out of range: Tried to access index 4 out of table with -1 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237.
[2020-03-09 21:19:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:19:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:19:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:19:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:19:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa15e611240>,), **{}) took: 5.161888360977173 sec
[2020-03-09 21:20:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:20:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:22:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:22:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:23:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:23:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:23:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:23:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:23:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1f74ce12b0>,), **{}) took: 5.1088690757751465 sec
[2020-03-09 21:24:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:24:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:24:02] - slp - ERROR -- Current run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-09 21:24:02] - slp - ERROR -- Engine run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-09 21:24:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:24:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:24:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:24:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:24:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1c1f5112b0>,), **{}) took: 4.022640943527222 sec
[2020-03-09 21:25:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:25:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:26:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:26:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:26:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:26:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:26:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:26:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:26:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0c27011278>,), **{}) took: 3.7007410526275635 sec
[2020-03-09 21:27:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:27:00] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:28:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:28:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:28:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:28:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:28:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:28:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:28:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff02eca2240>,), **{}) took: 3.6830966472625732 sec
[2020-03-09 21:29:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:29:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:29:40] - slp - ERROR -- Current run is terminating due to exception: index out of range: Tried to access index 3 out of table with -1 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237.
[2020-03-09 21:29:40] - slp - ERROR -- Engine run is terminating due to exception: index out of range: Tried to access index 3 out of table with -1 rows. at /pytorch/aten/src/TH/generic/THTensorEvenMoreMath.cpp:237.
[2020-03-09 21:30:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:30:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:30:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:30:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:30:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcd18657208>,), **{}) took: 3.704026460647583 sec
[2020-03-09 21:30:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:30:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:31:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:31:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:31:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:31:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:31:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:31:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:31:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc06bbfa240>,), **{}) took: 3.671821117401123 sec
[2020-03-09 21:31:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:31:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:32:48] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:32:48] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:32:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:32:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:32:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:32:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:32:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdfa53e2e10>,), **{}) took: 3.7266440391540527 sec
[2020-03-09 21:33:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:33:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:34:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:34:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:34:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:34:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:34:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:34:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:34:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5397cb5240>,), **{}) took: 3.6931827068328857 sec
[2020-03-09 21:34:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:34:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:35:06] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:35:06] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:35:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:35:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:35:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:35:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:35:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f784f179240>,), **{}) took: 3.6572771072387695 sec
[2020-03-09 21:35:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:35:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:36:25] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:36:25] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:38:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:38:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:38:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:38:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:38:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3e58469240>,), **{}) took: 3.781557559967041 sec
[2020-03-09 21:38:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:38:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 21:54:13] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 21:54:13] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-09 21:56:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:56:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:56:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:56:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:56:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd10803a240>,), **{}) took: 3.7017900943756104 sec
[2020-03-09 21:57:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 21:57:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 21:57:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 21:57:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 21:57:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff24aed1240>,), **{}) took: 3.810209035873413 sec
[2020-03-09 21:57:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-09 21:57:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 22:02:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-09 22:02:59] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-09 22:03:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-09 22:03:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-09 22:03:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-09 22:03:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-09 22:03:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f98893cf2b0>,), **{}) took: 3.756981134414673 sec
[2020-03-09 22:04:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-09 22:04:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-09 22:54:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-09 22:54:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-10 00:16:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-10 00:16:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-10 01:02:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-10 01:02:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-10 02:11:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-10 02:11:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 00:49:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 00:49:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 00:49:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 00:52:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 00:52:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 00:52:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 00:53:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 00:53:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdc7eee02e8>,), **{}) took: 20.00520610809326 sec
[2020-03-11 00:54:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 00:54:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 01:49:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 01:49:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 02:37:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 02:37:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 03:40:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 03:40:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 04:37:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 04:37:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 12:32:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 12:32:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 12:32:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 12:33:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 12:33:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4661c33390>,), **{}) took: 48.997353315353394 sec
[2020-03-11 12:34:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 12:34:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 13:56:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 13:56:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 14:55:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 14:55:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 15:58:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 15:58:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 17:13:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 17:13:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 21:07:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:07:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:07:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 21:08:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 21:08:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9f563962e8>,), **{}) took: 33.852195262908936 sec
[2020-03-11 21:09:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:09:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:09:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 21:10:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:10:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:10:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:10:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:10:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 21:10:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 21:10:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd375b992e8>,), **{}) took: 16.34976601600647 sec
[2020-03-11 21:18:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:18:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:18:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 21:18:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 21:18:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9f617702e8>,), **{}) took: 13.660611391067505 sec
[2020-03-11 21:20:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:20:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:20:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 21:20:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 21:20:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f09a329d2e8>,), **{}) took: 18.455732583999634 sec
[2020-03-11 21:21:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 21:21:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 21:25:21] - slp - ERROR -- Current run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-11 21:25:21] - slp - ERROR -- Engine run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-11 21:25:21] - slp - ERROR -- Engine run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-11 21:44:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:44:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:44:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 21:44:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 21:44:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd7810072e8>,), **{}) took: 21.798385858535767 sec
[2020-03-11 21:46:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 21:46:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 21:50:24] - slp - ERROR -- Current run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-11 21:50:24] - slp - ERROR -- Engine run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-11 21:50:24] - slp - ERROR -- Engine run is terminating due to exception: cuda runtime error (59) : device-side assert triggered at /pytorch/aten/src/THC/THCReduceAll.cuh:327.
[2020-03-11 21:55:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 21:55:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 21:55:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 21:55:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 21:55:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f133c7a12e8>,), **{}) took: 39.604928970336914 sec
[2020-03-11 21:56:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 21:56:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 22:01:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-11 22:01:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-11 22:01:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 22:01:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 22:01:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 22:02:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 22:02:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd6991a12e8>,), **{}) took: 22.507675647735596 sec
[2020-03-11 22:03:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-11 22:03:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 22:17:58] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-11 22:17:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-11 22:17:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-11 22:19:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 22:19:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 22:19:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 22:19:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 22:19:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f44a1fa6278>,), **{}) took: 32.78256273269653 sec
[2020-03-11 22:20:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-11 22:20:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 22:21:07] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-11 22:21:07] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-11 22:21:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 22:21:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 22:21:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 22:21:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 22:21:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f239bafe278>,), **{}) took: 22.90504765510559 sec
[2020-03-11 22:22:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-11 22:22:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 22:43:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-11 22:43:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-11 22:43:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-11 22:43:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 22:43:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 22:43:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 22:44:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 22:44:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6a498542b0>,), **{}) took: 28.68500590324402 sec
[2020-03-11 22:49:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 22:49:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 22:49:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 22:49:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 22:49:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5094b78278>,), **{}) took: 28.697965383529663 sec
[2020-03-11 22:58:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 22:58:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 22:58:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 22:58:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 22:58:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3b5fc962b0>,), **{}) took: 29.02314066886902 sec
[2020-03-11 23:00:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:00:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:00:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:00:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:00:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4c63c6f278>,), **{}) took: 24.179086685180664 sec
[2020-03-11 23:00:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-11 23:00:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-11 23:07:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-11 23:07:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-11 23:12:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:12:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:12:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:12:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:12:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1f9db59e48>,), **{}) took: 27.641322374343872 sec
[2020-03-11 23:13:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:13:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:13:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:14:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:14:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5dfbe3ce48>,), **{}) took: 23.647117853164673 sec
[2020-03-11 23:14:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:14:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:14:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:15:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:15:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9aa9d4d2b0>,), **{}) took: 23.781363248825073 sec
[2020-03-11 23:37:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:37:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:37:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:38:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:38:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8f2a0f1ef0>,), **{}) took: 30.193864107131958 sec
[2020-03-11 23:39:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:39:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:39:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:40:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:40:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1aa33122b0>,), **{}) took: 25.10624861717224 sec
[2020-03-11 23:48:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:48:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:48:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:48:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:48:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f61a92f02b0>,), **{}) took: 25.21772789955139 sec
[2020-03-11 23:56:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:56:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:56:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:57:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:57:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f482a1ad2b0>,), **{}) took: 30.061655521392822 sec
[2020-03-11 23:57:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-11 23:57:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-11 23:57:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-11 23:58:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-11 23:58:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faacaa8a2b0>,), **{}) took: 25.322187900543213 sec
[2020-03-12 00:00:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:00:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:00:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:00:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:00:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f034ef04278>,), **{}) took: 25.39650845527649 sec
[2020-03-12 00:01:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:01:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:01:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:02:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:02:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd503072278>,), **{}) took: 38.91417741775513 sec
[2020-03-12 00:05:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:05:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:05:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:05:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:05:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7feebea4e278>,), **{}) took: 25.38503909111023 sec
[2020-03-12 00:07:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:07:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:07:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:08:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:08:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6615a9c240>,), **{}) took: 31.097321033477783 sec
[2020-03-12 00:09:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:09:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:09:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:10:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:10:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa5458da278>,), **{}) took: 25.4531192779541 sec
[2020-03-12 00:12:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:12:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:12:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:13:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:13:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe50e0c6278>,), **{}) took: 79.3829357624054 sec
[2020-03-12 00:17:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:17:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:17:49] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:18:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:18:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa6a54e8240>,), **{}) took: 25.899489641189575 sec
[2020-03-12 00:20:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:20:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:20:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:20:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:20:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6351fa1240>,), **{}) took: 25.997810125350952 sec
[2020-03-12 00:24:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:24:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:24:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:25:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:25:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa6366c6278>,), **{}) took: 67.41398882865906 sec
[2020-03-12 00:27:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:27:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:27:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:28:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:28:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f04ccb1e278>,), **{}) took: 41.22636914253235 sec
[2020-03-12 00:30:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:30:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:30:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:30:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:30:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f51fb453278>,), **{}) took: 16.772769927978516 sec
[2020-03-12 00:32:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:32:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:32:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:32:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:32:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb6f12cf278>,), **{}) took: 16.53272032737732 sec
[2020-03-12 00:45:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:45:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:45:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:45:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:45:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9af702c278>,), **{}) took: 27.14362621307373 sec
[2020-03-12 00:46:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:46:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:46:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:47:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:47:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb5efcef668>,), **{}) took: 22.04293656349182 sec
[2020-03-12 00:48:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:48:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:48:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:49:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:49:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbfe7733278>,), **{}) took: 22.282864809036255 sec
[2020-03-12 00:51:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:51:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:51:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:51:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:51:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f21f3cc42e8>,), **{}) took: 22.904829740524292 sec
[2020-03-12 00:51:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 00:51:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 00:52:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 00:52:56] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 00:53:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:53:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:53:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:53:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:53:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f906af872e8>,), **{}) took: 24.704055786132812 sec
[2020-03-12 00:54:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 00:54:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 00:55:16] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 00:55:16] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 00:55:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:55:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:55:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 00:55:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 00:55:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5a491b32e8>,), **{}) took: 23.986830234527588 sec
[2020-03-12 00:56:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 00:56:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 00:59:34] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 00:59:34] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 00:59:34] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 00:59:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 00:59:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 00:59:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:00:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:00:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbd086652e8>,), **{}) took: 23.67877459526062 sec
[2020-03-12 01:00:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:00:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:03:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:03:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:03:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:03:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:03:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:03:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:04:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:04:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f41a97d52e8>,), **{}) took: 50.43446230888367 sec
[2020-03-12 01:05:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:05:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:07:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:07:56] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:07:56] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:08:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:08:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:08:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:08:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:08:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa51047ae80>,), **{}) took: 16.291722536087036 sec
[2020-03-12 01:09:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cpu
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:09:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:17:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:17:05] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-12 01:19:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:19:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:19:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:19:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:19:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1cd05192e8>,), **{}) took: 17.971151113510132 sec
[2020-03-12 01:20:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:20:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:22:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:22:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:22:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:23:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:23:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:23:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:23:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:23:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f26f6eaf2e8>,), **{}) took: 24.956040143966675 sec
[2020-03-12 01:23:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:23:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:27:43] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:27:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:27:43] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:29:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:29:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:29:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:29:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:29:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:29:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:30:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:30:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f18903422e8>,), **{}) took: 18.892784595489502 sec
[2020-03-12 01:30:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:30:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:32:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:32:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:32:35] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:33:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:33:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:33:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:33:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:33:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7e77e7b2e8>,), **{}) took: 18.882801294326782 sec
[2020-03-12 01:34:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:34:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:37:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:37:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:37:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:37:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:37:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:37:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:38:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:38:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f71d9d55ef0>,), **{}) took: 43.11267447471619 sec
[2020-03-12 01:38:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:38:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:40:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:40:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:40:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:41:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:41:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:41:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:41:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:41:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6097b9d2e8>,), **{}) took: 18.280582904815674 sec
[2020-03-12 01:41:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:41:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:46:31] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:46:31] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:46:31] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:46:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:46:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:46:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:48:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:48:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f40ed8382e8>,), **{}) took: 104.85589647293091 sec
[2020-03-12 01:49:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:49:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 01:52:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 01:52:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:52:11] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 01:56:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 01:56:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 01:56:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 01:56:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 01:56:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5c1234e2e8>,), **{}) took: 37.08364963531494 sec
[2020-03-12 01:57:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 01:57:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 02:01:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 02:01:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 02:01:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 02:01:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:01:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:01:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:01:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:01:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4cf05c92e8>,), **{}) took: 18.405773639678955 sec
[2020-03-12 02:05:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:05:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:05:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:06:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:06:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbaea7922e8>,), **{}) took: 77.59714150428772 sec
[2020-03-12 02:08:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:08:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:08:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:10:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:10:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6ad10b5a58>,), **{}) took: 110.64356112480164 sec
[2020-03-12 02:14:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:14:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:14:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:14:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:14:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37d82da7f0>,), **{}) took: 51.441020250320435 sec
[2020-03-12 02:15:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 02:15:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 02:18:08] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 02:18:08] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 02:18:08] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 02:18:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:18:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:18:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:18:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:18:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1810eb92e8>,), **{}) took: 17.92808961868286 sec
[2020-03-12 02:19:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 02:19:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 02:21:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 02:21:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 02:21:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 02:21:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:21:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:21:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:22:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:22:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f87414312b0>,), **{}) took: 32.32632780075073 sec
[2020-03-12 02:24:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:24:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:24:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:25:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:25:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faf589d1898>,), **{}) took: 25.055097103118896 sec
[2020-03-12 02:25:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 02:25:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 02:27:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 02:27:50] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-12 02:28:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 02:28:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 02:28:22] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 02:28:22] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-12 02:28:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:28:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:28:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:29:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:29:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f465e1b12e8>,), **{}) took: 82.87378072738647 sec
[2020-03-12 02:31:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:31:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:31:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:33:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:33:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f43079752b0>,), **{}) took: 101.99805498123169 sec
[2020-03-12 02:35:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 02:35:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 02:43:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:43:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:43:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:44:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:44:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa0df41f2e8>,), **{}) took: 89.51333332061768 sec
[2020-03-12 02:45:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:45:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:45:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:47:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:47:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2f4c7bd2b0>,), **{}) took: 92.47447228431702 sec
[2020-03-12 02:52:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 02:52:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 02:52:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 02:53:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 02:53:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff5ef0762b0>,), **{}) took: 87.75531792640686 sec
[2020-03-12 02:55:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 02:55:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 03:00:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 03:00:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 04:37:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 04:37:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 04:40:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 04:40:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 05:25:06] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 05:25:06] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 06:06:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 06:06:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 06:20:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 06:20:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 07:50:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 07:50:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 08:40:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 08:40:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 13:08:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 13:08:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 13:08:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 13:11:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 13:11:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe0ff5032e8>,), **{}) took: 141.58039689064026 sec
[2020-03-12 13:13:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 13:13:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 13:13:20] - slp - ERROR -- Current run is terminating due to exception: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first..
[2020-03-12 13:13:20] - slp - ERROR -- Engine run is terminating due to exception: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first..
[2020-03-12 13:13:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 13:13:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 13:13:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 13:14:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 13:14:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcdb58992e8>,), **{}) took: 38.49666476249695 sec
[2020-03-12 13:15:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 13:15:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 13:19:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 13:19:59] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 13:23:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 13:23:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 13:23:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 13:23:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 13:23:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9487f022b0>,), **{}) took: 24.855718851089478 sec
[2020-03-12 13:24:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 13:24:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 13:34:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 13:34:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 13:35:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 13:35:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 13:35:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 13:36:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 13:36:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc6777f32e8>,), **{}) took: 80.73234605789185 sec
[2020-03-12 13:38:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 13:38:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 13:54:15] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 13:54:15] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 13:54:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 13:54:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 13:54:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 13:54:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 13:54:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f49682192e8>,), **{}) took: 1.9255452156066895 sec
[2020-03-12 13:55:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 13:55:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 13:55:48] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 13:55:48] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 13:56:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 13:56:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 13:56:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 13:56:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 13:56:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f11672b72e8>,), **{}) took: 1.9477062225341797 sec
[2020-03-12 13:56:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 13:56:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 13:58:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 13:58:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 14:08:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 14:08:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 14:08:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 14:08:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 14:08:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb34943f2e8>,), **{}) took: 2.3447492122650146 sec
[2020-03-12 14:09:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 14:09:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 14:10:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 14:10:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 14:10:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 14:10:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 14:10:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 14:10:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 14:10:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdd2f75f2e8>,), **{}) took: 2.2504096031188965 sec
[2020-03-12 14:11:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 14:11:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 14:11:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 14:11:59] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 14:14:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 14:14:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 14:14:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 14:14:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 14:14:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa5f5feeef0>,), **{}) took: 1.9751684665679932 sec
[2020-03-12 14:14:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 14:14:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 14:17:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 14:17:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 14:18:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 14:18:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 14:18:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 14:18:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 14:18:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f92a93662e8>,), **{}) took: 2.0142242908477783 sec
[2020-03-12 14:18:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 14:18:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 14:29:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 14:29:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-12 14:36:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 14:36:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 14:36:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 14:36:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 14:36:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f406aa4b2e8>,), **{}) took: 2.7963316440582275 sec
[2020-03-12 14:37:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 14:37:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 14:40:19] - slp - ERROR -- Current run is terminating due to exception: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first..
[2020-03-12 14:40:19] - slp - ERROR -- Engine run is terminating due to exception: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first..
[2020-03-12 14:40:19] - slp - ERROR -- Engine run is terminating due to exception: can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first..
[2020-03-12 14:44:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 14:44:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 14:44:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 14:44:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 14:44:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe1154312e8>,), **{}) took: 2.7045087814331055 sec
[2020-03-12 14:45:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 14:45:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 15:05:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 15:05:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 15:05:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 15:05:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 15:05:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f533b56a2b0>,), **{}) took: 14.308877944946289 sec
[2020-03-12 15:06:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 15:06:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 15:19:03] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 4.85 GiB already allocated; 4.19 MiB free; 9.12 MiB cached).
[2020-03-12 15:19:03] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 10.92 GiB total capacity; 4.85 GiB already allocated; 4.19 MiB free; 9.12 MiB cached).
[2020-03-12 16:08:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 16:08:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 16:47:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 16:47:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 18:04:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 18:04:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 18:42:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 18:42:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 18:42:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 18:44:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 18:44:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f28c944d2b0>,), **{}) took: 107.73587226867676 sec
[2020-03-12 18:45:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 18:45:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 18:45:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 18:46:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 18:46:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd43ddf29e8>,), **{}) took: 29.706926345825195 sec
[2020-03-12 18:47:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 18:47:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 18:47:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 18:47:47] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-12 18:50:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 18:50:17] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-12 18:50:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 18:50:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 18:50:33] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-12 18:50:33] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-12 18:50:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-12 18:50:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-12 18:50:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-12 18:50:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-12 18:50:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efe0276e2e8>,), **{}) took: 7.119678497314453 sec
[2020-03-12 18:51:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 18:51:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 19:57:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 19:57:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 20:58:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 20:58:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 22:06:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 22:06:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-12 23:41:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-12 23:41:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 01:47:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 01:47:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 01:47:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 01:47:41] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 01:47:41] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5417abf438>,), **{}) took: 39.70657539367676 sec
[2020-03-13 01:48:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 01:48:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 01:55:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 01:55:54] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-13 01:57:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 01:57:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 01:57:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 01:57:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 01:57:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb7408d4438>,), **{}) took: 19.152825832366943 sec
[2020-03-13 02:00:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 02:00:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 02:00:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 02:01:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 02:01:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5cddcd54a8>,), **{}) took: 22.322761058807373 sec
[2020-03-13 02:02:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 02:02:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 03:19:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 03:19:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 04:36:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 04:36:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 05:45:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 05:45:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 07:11:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 07:11:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 12:00:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:00:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:00:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:01:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:01:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6d84e212e8>,), **{}) took: 31.406806230545044 sec
[2020-03-13 12:06:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:06:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:06:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:06:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:06:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0091535320>,), **{}) took: 24.949260473251343 sec
[2020-03-13 12:10:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:10:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:10:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:11:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:11:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbf4601d2e8>,), **{}) took: 27.805979251861572 sec
[2020-03-13 12:13:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:13:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:13:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:13:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:13:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f04e4bf2320>,), **{}) took: 20.886564254760742 sec
[2020-03-13 12:32:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:32:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:32:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:32:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:32:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f605a4b7320>,), **{}) took: 57.20705604553223 sec
[2020-03-13 12:34:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 12:34:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 12:34:39] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-03-13 12:34:39] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'to'.
[2020-03-13 12:36:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:36:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:36:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:36:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:36:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcf22599b38>,), **{}) took: 28.896708488464355 sec
[2020-03-13 12:38:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 12:38:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 12:38:04] - slp - ERROR -- Current run is terminating due to exception: name 'featuresn' is not defined.
[2020-03-13 12:38:04] - slp - ERROR -- Engine run is terminating due to exception: name 'featuresn' is not defined.
[2020-03-13 12:38:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:38:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:38:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:38:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:38:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8abddb1a90>,), **{}) took: 24.63299012184143 sec
[2020-03-13 12:40:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 12:40:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 12:41:39] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 12:41:39] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 12:41:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:41:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:41:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:42:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:42:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f59d7ef1a58>,), **{}) took: 23.61699366569519 sec
[2020-03-13 12:43:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 12:43:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 12:44:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 12:44:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 12:45:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:45:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:45:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:46:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:46:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fea72c9e2e8>,), **{}) took: 29.41541075706482 sec
[2020-03-13 12:47:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 12:47:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 12:49:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 12:49:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 12:50:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 12:50:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 12:50:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 12:50:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 12:50:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe41b573e48>,), **{}) took: 20.97673726081848 sec
[2020-03-13 12:52:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 12:52:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 12:53:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 12:53:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 13:53:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 13:53:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 13:53:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 13:54:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 13:54:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa395d37320>,), **{}) took: 71.40449261665344 sec
[2020-03-13 13:55:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 13:55:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 13:56:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 13:56:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 13:57:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 13:57:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 13:57:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 13:58:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 13:58:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f08e4cb5320>,), **{}) took: 29.1915545463562 sec
[2020-03-13 13:59:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 13:59:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 14:00:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 14:00:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 14:01:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:01:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:01:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:01:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:01:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd58f065f60>,), **{}) took: 20.99164581298828 sec
[2020-03-13 14:02:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 14:02:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 14:03:42] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 14:03:42] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 14:04:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:04:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:04:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:04:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:04:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f91e0aa92e8>,), **{}) took: 27.716106414794922 sec
[2020-03-13 14:05:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 14:05:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 14:06:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 14:06:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 14:06:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:06:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:06:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:07:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:07:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff6f08862e8>,), **{}) took: 27.061368227005005 sec
[2020-03-13 14:08:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:08:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:08:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:08:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:08:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3a0c3872e8>,), **{}) took: 24.70119833946228 sec
[2020-03-13 14:09:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 14:09:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 14:11:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 14:11:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 14:11:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:11:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:11:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:12:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:12:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f18a91e22e8>,), **{}) took: 49.97649073600769 sec
[2020-03-13 14:13:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 14:13:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 14:15:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 14:15:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 14:16:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:16:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:16:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:16:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:16:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3d41a042e8>,), **{}) took: 40.163039207458496 sec
[2020-03-13 14:17:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 14:17:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 14:21:16] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 14:21:16] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-13 14:21:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:21:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:21:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:21:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:21:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb7b7c772e8>,), **{}) took: 27.620264768600464 sec
[2020-03-13 14:22:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 14:22:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 14:22:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-13 14:22:44] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-03-13 14:23:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-13 14:23:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-13 14:23:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-13 14:23:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-13 14:23:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f10af2c92e8>,), **{}) took: 17.18244242668152 sec
[2020-03-13 14:24:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 14:24:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 15:37:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 15:37:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 16:31:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 16:31:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 18:28:06] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 18:28:06] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-13 19:48:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-13 19:48:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-17 23:55:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-17 23:55:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-17 23:55:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-17 23:56:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-17 23:56:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6f391062e8>,), **{}) took: 59.335005044937134 sec
[2020-03-17 23:59:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-17 23:59:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-17 23:59:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-17 23:59:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-17 23:59:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4b14ef6320>,), **{}) took: 17.876832962036133 sec
[2020-03-18 00:01:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-18 00:01:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-18 00:01:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-18 00:02:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-18 00:02:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa003d8b320>,), **{}) took: 27.631118535995483 sec
[2020-03-18 00:03:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-18 00:03:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-18 01:04:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-18 01:04:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-18 02:24:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-18 02:24:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-18 03:36:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-18 03:36:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-18 04:43:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-18 04:43:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-20 00:33:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-20 00:33:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-20 00:33:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-20 00:34:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-20 00:34:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcccb09bbe0>,), **{}) took: 44.16627883911133 sec
[2020-03-20 00:37:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-20 00:37:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-20 00:37:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-20 00:37:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-20 00:37:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f13899fb2e8>,), **{}) took: 11.98336124420166 sec
[2020-03-20 00:40:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-20 00:40:45] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=603, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-20 00:40:50] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [8 x 600], m2: [603 x 2] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-03-20 00:40:50] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [8 x 600], m2: [603 x 2] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-03-20 00:42:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-20 00:42:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-20 00:42:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-20 00:42:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-20 00:42:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f53206362b0>,), **{}) took: 12.73164415359497 sec
[2020-03-20 00:46:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-20 00:46:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-20 03:13:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-20 03:13:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-20 06:05:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-20 06:05:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-20 10:04:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-20 10:04:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-20 12:43:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-20 12:43:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-24 20:16:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-24 20:16:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-24 20:16:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-24 20:16:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-24 20:16:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-24 20:16:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-24 20:16:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-24 20:16:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb1f65381d0>,), **{}) took: 17.45446515083313 sec
[2020-03-24 20:52:26] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-24 20:54:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-24 20:54:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-24 20:54:46] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-24 20:54:46] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-24 21:06:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-24 21:06:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-24 21:06:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-24 21:06:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-24 21:06:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f04c81502e8>,), **{}) took: 2.0615878105163574 sec
[2020-03-24 21:10:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-24 21:10:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-24 21:10:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-24 21:10:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-24 21:10:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4a6a8452b0>,), **{}) took: 1.9824917316436768 sec
[2020-03-24 21:11:48] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-24 21:11:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-24 21:11:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-24 21:12:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-24 21:12:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-24 21:12:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-24 21:12:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-24 21:12:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-24 21:12:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-24 21:12:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faeb66972b0>,), **{}) took: 1.9947230815887451 sec
[2020-03-24 21:12:50] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-24 21:12:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-24 21:12:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-24 21:13:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-24 21:13:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-25 11:44:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 11:44:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 11:44:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 11:44:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 11:44:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7ab8b58320>,), **{}) took: 1.9521057605743408 sec
[2020-03-25 11:45:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 11:45:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 11:45:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 11:45:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 11:45:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcba3e94668>,), **{}) took: 1.9282939434051514 sec
[2020-03-25 11:52:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 11:52:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 11:52:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 11:52:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 11:52:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe467ae2198>,), **{}) took: 1.943321704864502 sec
[2020-03-25 11:55:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 11:55:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 11:55:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 11:55:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 11:55:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1df57e6668>,), **{}) took: 1.9540071487426758 sec
[2020-03-25 11:56:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 11:56:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 11:56:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 11:56:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 11:56:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7bd6c556a0>,), **{}) took: 1.9094576835632324 sec
[2020-03-25 20:19:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 20:19:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 20:19:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 20:19:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 20:19:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd19a7dd518>,), **{}) took: 1.948610782623291 sec
[2020-03-25 20:21:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 20:21:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 20:21:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 20:21:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 20:21:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f597db44518>,), **{}) took: 2.033285140991211 sec
[2020-03-25 20:28:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 20:28:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 20:28:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 20:28:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 20:28:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb36123d4e0>,), **{}) took: 1.95731782913208 sec
[2020-03-25 20:28:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-25 20:28:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-25 20:29:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-25 20:29:02] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-25 20:30:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 20:30:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 20:30:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 20:30:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 20:30:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc4b7a55518>,), **{}) took: 2.1557066440582275 sec
[2020-03-25 20:30:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-25 20:30:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-25 20:30:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-25 20:30:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-25 20:30:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 20:30:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 20:30:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 20:30:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 20:30:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9a15acfc88>,), **{}) took: 2.4905338287353516 sec
[2020-03-25 20:31:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-25 20:31:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-25 20:31:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-03-25 20:31:09] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-03-25 20:31:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-25 20:31:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-25 20:31:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-25 20:31:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-25 20:31:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6031bad518>,), **{}) took: 2.1948299407958984 sec
[2020-03-25 20:31:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-25 20:31:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-26 02:30:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-26 02:30:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-26 07:17:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-26 07:17:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-26 11:36:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-26 11:36:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-26 18:32:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-26 18:32:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-27 17:01:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-27 17:01:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-27 17:01:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-27 17:01:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-27 17:01:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff83a924320>,), **{}) took: 38.47487664222717 sec
[2020-03-30 18:08:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-30 18:08:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-30 18:08:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-30 18:08:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-30 18:08:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37351a8e48>,), **{}) took: 7.7660510540008545 sec
[2020-03-30 18:46:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-30 18:46:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-30 18:46:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-30 18:46:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-30 18:46:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe435415518>,), **{}) took: 3.4968295097351074 sec
[2020-03-30 18:55:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-30 18:55:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-30 18:55:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-30 18:55:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-30 18:55:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f48c49a54a8>,), **{}) took: 3.6831679344177246 sec
[2020-03-30 18:57:10] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-30 18:57:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-30 18:57:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-30 18:57:13] - slp - ERROR -- Current run is terminating due to exception: 'HierAttNet' object has no attribute 'idx2word'.
[2020-03-30 18:57:13] - slp - ERROR -- Engine run is terminating due to exception: 'HierAttNet' object has no attribute 'idx2word'.
[2020-03-30 20:40:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-03-30 20:40:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-03-30 20:40:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-03-30 20:41:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-03-30 20:41:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbf55c7b4e0>,), **{}) took: 3.5157384872436523 sec
[2020-03-30 20:42:49] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-30 20:42:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-30 20:42:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-30 22:38:41] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-30 22:38:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-30 22:38:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-31 01:57:33] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-31 01:57:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-31 01:57:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-31 04:28:02] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-31 04:28:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-31 04:28:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-03-31 06:23:33] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-03-31 06:23:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-03-31 06:23:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-01 17:10:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:10:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:10:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:10:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:10:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8ae29d0e10>,), **{}) took: 24.645050525665283 sec
[2020-04-01 17:11:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:11:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:11:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:11:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:11:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f22b09d4da0>,), **{}) took: 21.117297887802124 sec
[2020-04-01 17:11:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:11:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:11:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:12:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:12:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f521f589da0>,), **{}) took: 20.757016897201538 sec
[2020-04-01 17:12:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:12:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:12:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:12:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:12:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3b18006da0>,), **{}) took: 21.64350962638855 sec
[2020-04-01 17:13:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:13:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:13:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:13:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:13:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4588692da0>,), **{}) took: 20.648139238357544 sec
[2020-04-01 17:15:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:15:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:15:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:15:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:15:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5c67a0bda0>,), **{}) took: 20.85776138305664 sec
[2020-04-01 17:17:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:17:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:17:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:18:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:18:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc9492f9da0>,), **{}) took: 20.66523790359497 sec
[2020-04-01 17:18:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:18:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:18:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:18:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:18:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efd0be19da0>,), **{}) took: 20.394526720046997 sec
[2020-04-01 17:22:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:22:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:22:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:23:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:23:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9fbc133da0>,), **{}) took: 20.706265449523926 sec
[2020-04-01 17:26:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:26:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:26:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:26:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:26:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f88a62a4da0>,), **{}) took: 21.359740257263184 sec
[2020-04-01 17:42:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:42:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:42:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:43:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:43:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9600fceda0>,), **{}) took: 21.480824947357178 sec
[2020-04-01 17:44:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-01 17:44:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-01 17:44:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-01 17:44:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-01 17:44:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4e4a72ed68>,), **{}) took: 19.56525707244873 sec
[2020-04-02 14:28:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 14:28:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 14:28:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 14:28:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 14:28:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f45fd9ed5f8>,), **{}) took: 13.225441694259644 sec
[2020-04-02 14:29:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 14:29:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 14:29:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 14:29:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 14:29:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faddf4d55f8>,), **{}) took: 11.557718515396118 sec
[2020-04-02 14:38:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 14:38:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 14:38:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 14:38:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 14:38:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f93209ab588>,), **{}) took: 49.87525200843811 sec
[2020-04-02 14:40:50] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-04-02 14:40:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 14:40:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 20:45:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 20:45:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 20:45:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 20:45:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 20:45:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7effe9e3d470>,), **{}) took: 14.271941184997559 sec
[2020-04-02 20:46:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 20:46:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 20:46:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 20:46:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 20:46:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7988420ba8>,), **{}) took: 11.467259645462036 sec
[2020-04-02 20:47:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 20:47:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 20:47:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 20:47:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 20:47:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f79482e1470>,), **{}) took: 11.421144723892212 sec
[2020-04-02 20:48:10] - slp - WARNING -- The checkpoint ../checkpoints/../experiment_model.best.pth you are trying to load does not exist. Continuing without loading...
[2020-04-02 20:51:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 20:51:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 20:51:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 20:52:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 20:52:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe2118fe048>,), **{}) took: 11.425321102142334 sec
[2020-04-02 20:58:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 20:58:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 20:58:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 20:58:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 20:58:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0b053ff470>,), **{}) took: 11.136032104492188 sec
[2020-04-02 21:03:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:03:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:03:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:03:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:03:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa69be5d470>,), **{}) took: 11.165436744689941 sec
[2020-04-02 21:06:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:06:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:06:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:06:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:06:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f25ebe52470>,), **{}) took: 11.14754867553711 sec
[2020-04-02 21:08:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:08:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:08:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:08:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:08:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe6977f4be0>,), **{}) took: 11.249746322631836 sec
[2020-04-02 21:11:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:11:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:11:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:11:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:11:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4269979080>,), **{}) took: 11.132725715637207 sec
[2020-04-02 21:12:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:12:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:12:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:12:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:12:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff955cf4be0>,), **{}) took: 11.192872047424316 sec
[2020-04-02 21:16:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:16:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:16:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:16:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:16:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fea28bba470>,), **{}) took: 11.08301854133606 sec
[2020-04-02 21:17:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:17:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:17:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:18:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:18:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fabc9f0b470>,), **{}) took: 11.150333642959595 sec
[2020-04-02 21:25:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:25:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:25:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:25:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:25:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f63ee458470>,), **{}) took: 10.436940431594849 sec
[2020-04-02 21:27:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:27:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:27:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:27:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:27:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5ec9d6a470>,), **{}) took: 10.431727409362793 sec
[2020-04-02 21:28:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:28:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:28:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:28:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:28:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa49458a470>,), **{}) took: 10.440248727798462 sec
[2020-04-02 21:33:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:33:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:33:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:33:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:33:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc2d6a69470>,), **{}) took: 10.428467512130737 sec
[2020-04-02 21:34:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 21:40:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:40:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:40:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:40:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:40:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f31a1545438>,), **{}) took: 10.43272614479065 sec
[2020-04-02 21:41:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 21:41:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 21:43:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:43:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:44:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:44:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:44:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:44:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:44:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f62082a4470>,), **{}) took: 10.429742097854614 sec
[2020-04-02 21:46:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:46:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:46:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:46:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:46:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1e460f9d68>,), **{}) took: 10.321081399917603 sec
[2020-04-02 21:47:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 21:55:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:55:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:55:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:55:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:55:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f79638b4470>,), **{}) took: 10.500010013580322 sec
[2020-04-02 21:56:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 21:56:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 21:56:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:56:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:56:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:57:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:57:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fccebcf4470>,), **{}) took: 10.359456777572632 sec
[2020-04-02 21:57:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 21:57:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 21:58:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 21:58:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 21:58:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 21:58:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 21:58:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f778884d470>,), **{}) took: 10.28511905670166 sec
[2020-04-02 21:59:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 21:59:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 22:01:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 22:01:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 22:01:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 22:01:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 22:01:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7bb31aed68>,), **{}) took: 10.254258871078491 sec
[2020-04-02 22:04:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 22:04:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 22:04:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 22:04:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 22:04:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 22:05:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 22:05:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb17ac60470>,), **{}) took: 10.487287044525146 sec
[2020-04-02 22:05:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 22:05:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 22:06:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 22:06:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 22:06:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 22:06:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 22:06:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4e18bfdf98>,), **{}) took: 10.433488607406616 sec
[2020-04-02 22:07:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 22:07:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-02 22:25:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 22:25:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 22:25:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 22:25:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 22:25:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7e6285a470>,), **{}) took: 12.365019798278809 sec
[2020-04-02 22:26:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 22:26:21] - slp - ERROR -- Current run is terminating due to exception: too many values to unpack (expected 2).
[2020-04-02 22:26:21] - slp - ERROR -- Engine run is terminating due to exception: too many values to unpack (expected 2).
[2020-04-02 22:29:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 22:29:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 22:29:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 22:29:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 22:29:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5cd0bdf470>,), **{}) took: 10.758844375610352 sec
[2020-04-02 22:30:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 22:32:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 22:32:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 22:32:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 22:32:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 22:32:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 22:32:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 22:33:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 22:33:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f412d43d438>,), **{}) took: 13.49204969406128 sec
[2020-04-02 22:33:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 23:45:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-02 23:45:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-02 23:45:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-02 23:45:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-02 23:45:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb9e75fb4a8>,), **{}) took: 10.514938354492188 sec
[2020-04-02 23:47:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-02 23:47:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 00:29:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 00:29:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 00:29:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 00:29:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 00:29:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 00:29:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 00:29:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcc88c4f4a8>,), **{}) took: 6.225415945053101 sec
[2020-04-03 00:31:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 00:31:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 00:34:17] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 00:34:17] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 00:47:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 00:47:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 00:47:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 00:47:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 00:47:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4c6e7ec048>,), **{}) took: 5.56566596031189 sec
[2020-04-03 00:49:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 00:49:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=10, out_features=10, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 00:52:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 00:52:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:02:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:02:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:02:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:02:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:02:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2dee161c88>,), **{}) took: 10.964056968688965 sec
[2020-04-03 01:06:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:06:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=600, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:10:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:10:23] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:10:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:10:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:10:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:11:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:11:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f15f7fe83c8>,), **{}) took: 10.992849349975586 sec
[2020-04-03 01:11:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:11:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:17:10] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:17:10] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:17:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:17:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:17:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:17:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:17:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f73add593c8>,), **{}) took: 10.997326135635376 sec
[2020-04-03 01:17:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:17:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:19:29] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:19:29] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:19:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:19:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:19:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:19:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:19:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f62f586b400>,), **{}) took: 11.022432327270508 sec
[2020-04-03 01:20:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:20:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:21:00] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:21:00] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:21:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:21:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:21:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:21:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:21:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f116f150400>,), **{}) took: 11.093986511230469 sec
[2020-04-03 01:21:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:21:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:22:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:22:21] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:22:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:22:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:22:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:22:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:22:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f02f8d63400>,), **{}) took: 11.13437271118164 sec
[2020-04-03 01:22:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:22:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:23:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:23:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:23:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:23:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:23:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:23:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:23:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2cf28dabe0>,), **{}) took: 11.147188901901245 sec
[2020-04-03 01:23:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:23:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:24:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:24:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:24:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:24:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:24:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:25:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:25:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1d1e958400>,), **{}) took: 11.035516262054443 sec
[2020-04-03 01:25:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:25:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:25:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:25:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-03 01:25:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-03 01:25:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-03 01:25:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-03 01:26:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-03 01:26:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1f5ecae400>,), **{}) took: 11.088457584381104 sec
[2020-04-03 01:26:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-03 01:26:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (attention): LuongAttention(
      (W): Linear(in_features=300, out_features=1, bias=False)
    )
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-03 01:26:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-03 01:26:59] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-04 23:26:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-04 23:26:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-04 23:26:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-04 23:27:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-04 23:27:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-04 23:27:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-04 23:27:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-04 23:27:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f02fa012400>,), **{}) took: 24.854501008987427 sec
[2020-04-04 23:28:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-04 23:28:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-04 23:28:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-04 23:29:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-04 23:29:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc7f988cfd0>,), **{}) took: 20.379032373428345 sec
[2020-04-06 13:57:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 13:57:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 13:57:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 13:59:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 13:59:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fda17921390>,), **{}) took: 88.50520586967468 sec
[2020-04-06 14:00:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:09:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:09:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:09:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:10:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:10:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb4283c5390>,), **{}) took: 84.1248767375946 sec
[2020-04-06 14:11:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:17:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:17:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:17:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:19:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:19:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f92f5da5390>,), **{}) took: 79.71777868270874 sec
[2020-04-06 14:19:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:21:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:21:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:21:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:22:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:22:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9cd6e96390>,), **{}) took: 27.51290488243103 sec
[2020-04-06 14:22:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:24:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:24:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:24:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:25:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:25:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff2607ae400>,), **{}) took: 25.281737804412842 sec
[2020-04-06 14:25:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:27:44] - slp - ERROR -- Current run is terminating due to exception: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number.
[2020-04-06 14:27:44] - slp - ERROR -- Engine run is terminating due to exception: invalid index of a 0-dim tensor. Use tensor.item() to convert a 0-dim tensor to a Python number.
[2020-04-06 14:30:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:30:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:30:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:32:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:32:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcab9c4f4a8>,), **{}) took: 67.10111737251282 sec
[2020-04-06 14:35:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:35:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:35:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:36:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:36:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7381d064a8>,), **{}) took: 28.773897886276245 sec
[2020-04-06 14:38:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:38:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:38:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:38:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:38:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f66d6a24400>,), **{}) took: 24.46036124229431 sec
[2020-04-06 14:39:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:40:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-06 14:41:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:41:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:41:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:41:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:41:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f48c431d3c8>,), **{}) took: 28.098191022872925 sec
[2020-04-06 14:42:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:44:58] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-06 14:44:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-06 14:45:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:45:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:45:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:45:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:45:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0b718d4d68>,), **{}) took: 23.355475664138794 sec
[2020-04-06 14:46:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:47:07] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-06 14:47:07] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-06 14:48:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:48:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:48:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:49:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:49:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f20fe523be0>,), **{}) took: 20.34245538711548 sec
[2020-04-06 14:49:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:50:17] - slp - ERROR -- Current run is terminating due to exception: zip argument #1 must support iteration.
[2020-04-06 14:50:17] - slp - ERROR -- Engine run is terminating due to exception: zip argument #1 must support iteration.
[2020-04-06 14:50:46] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:50:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:50:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:51:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:51:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f92db1be400>,), **{}) took: 20.143847465515137 sec
[2020-04-06 14:51:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:52:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:52:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:52:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:52:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:52:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd6ae01ff98>,), **{}) took: 20.272549867630005 sec
[2020-04-06 14:53:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:53:40] - slp - ERROR -- Current run is terminating due to exception: zip argument #1 must support iteration.
[2020-04-06 14:53:40] - slp - ERROR -- Engine run is terminating due to exception: zip argument #1 must support iteration.
[2020-04-06 14:53:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 14:53:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 14:53:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 14:54:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 14:54:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0819850400>,), **{}) took: 20.25831937789917 sec
[2020-04-06 14:54:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: ../checkpoints/../checkpoints/experiment_model.best.pth None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-06 14:55:54] - slp - ERROR -- Current run is terminating due to exception: zip argument #1 must support iteration.
[2020-04-06 14:55:54] - slp - ERROR -- Engine run is terminating due to exception: zip argument #1 must support iteration.
[2020-04-06 18:44:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:44:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:49:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:49:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:49:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 18:50:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 18:50:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7abfc34dd8>,), **{}) took: 15.167743682861328 sec
[2020-04-06 18:50:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:50:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:50:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 18:51:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 18:51:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7e59bdddd8>,), **{}) took: 15.255702495574951 sec
[2020-04-06 18:51:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:51:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:54:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:54:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:55:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:55:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:57:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:57:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:58:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:58:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:59:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:59:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:59:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:59:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 18:59:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 18:59:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:00:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:00:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:01:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:01:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:41:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:41:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:41:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:41:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:41:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa9de8afd30>,), **{}) took: 15.28157114982605 sec
[2020-04-06 19:41:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:42:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:42:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:42:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:42:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f06bfb21d30>,), **{}) took: 15.237019538879395 sec
[2020-04-06 19:45:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:45:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:45:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:45:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:45:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe824469d30>,), **{}) took: 15.345885276794434 sec
[2020-04-06 19:45:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:45:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:45:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:46:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:46:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1887ffb550>,), **{}) took: 15.280782699584961 sec
[2020-04-06 19:46:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:46:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:46:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:47:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:47:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f007ecffd30>,), **{}) took: 15.252458333969116 sec
[2020-04-06 19:48:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:48:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:48:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:48:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:48:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5adfe20940>,), **{}) took: 15.26029372215271 sec
[2020-04-06 19:49:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:49:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:49:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:49:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:49:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ffbd127bd30>,), **{}) took: 15.219166278839111 sec
[2020-04-06 19:51:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:51:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:51:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:51:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:51:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb900ef71d0>,), **{}) took: 15.320033073425293 sec
[2020-04-06 19:56:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 19:56:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 19:56:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 19:57:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 19:57:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0566ad8dd8>,), **{}) took: 15.241878271102905 sec
[2020-04-06 20:34:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:34:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:34:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:34:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:34:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7aed291dd8>,), **{}) took: 15.922586679458618 sec
[2020-04-06 20:42:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:42:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:42:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:42:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:42:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe817f80dd8>,), **{}) took: 15.768505573272705 sec
[2020-04-06 20:45:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:45:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:45:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:45:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:45:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f41e7223fd0>,), **{}) took: 15.734684228897095 sec
[2020-04-06 20:47:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:47:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:47:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:47:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:47:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f98df55de10>,), **{}) took: 15.859655618667603 sec
[2020-04-06 20:50:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:50:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:50:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:51:11] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:51:11] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7c6c222e10>,), **{}) took: 15.753422737121582 sec
[2020-04-06 20:53:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:53:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:54:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:54:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:55:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:55:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:55:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcc24ee2ef0>,), **{}) took: 16.587517738342285 sec
[2020-04-06 20:58:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:58:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:58:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:58:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:58:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efcfa683eb8>,), **{}) took: 15.949033737182617 sec
[2020-04-06 20:59:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 20:59:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 20:59:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 20:59:51] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 20:59:51] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0b254756a0>,), **{}) took: 16.02111291885376 sec
[2020-04-06 21:01:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:01:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:01:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:01:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:01:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb665725780>,), **{}) took: 15.94777798652649 sec
[2020-04-06 21:02:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:02:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:03:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:03:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:03:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1ccb02d5c0>,), **{}) took: 16.17419743537903 sec
[2020-04-06 21:05:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:05:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:05:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:05:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:05:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1e60e40e48>,), **{}) took: 16.227277040481567 sec
[2020-04-06 21:06:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:06:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:06:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:06:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:06:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f87e22a4eb8>,), **{}) took: 16.3015034198761 sec
[2020-04-06 21:09:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:09:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:10:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:10:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:10:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f25dd4d85f8>,), **{}) took: 17.2876398563385 sec
[2020-04-06 21:16:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:16:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:16:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:17:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:17:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f41868d7f28>,), **{}) took: 17.845487117767334 sec
[2020-04-06 21:18:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:18:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:18:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:18:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:18:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fad2fd983c8>,), **{}) took: 23.916036128997803 sec
[2020-04-06 21:19:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:19:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:19:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:19:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:19:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5b7b729e48>,), **{}) took: 17.211228609085083 sec
[2020-04-06 21:20:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:20:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:20:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:20:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:20:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb028e40e48>,), **{}) took: 17.479202270507812 sec
[2020-04-06 21:22:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:22:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:22:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:22:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:22:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f99647fce48>,), **{}) took: 17.11220908164978 sec
[2020-04-06 21:27:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:27:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:27:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:27:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:27:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc40ac90e48>,), **{}) took: 19.85257601737976 sec
[2020-04-06 21:28:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 21:28:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 21:28:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 21:28:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 21:28:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3d449cee10>,), **{}) took: 16.24615716934204 sec
[2020-04-06 22:50:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 22:50:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 22:50:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 22:51:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 22:51:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd9449d6e48>,), **{}) took: 17.553762674331665 sec
[2020-04-06 23:02:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 23:02:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 23:02:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 23:03:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 23:03:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f655ae1be80>,), **{}) took: 39.87860369682312 sec
[2020-04-06 23:05:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-06 23:05:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-06 23:05:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-06 23:05:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-06 23:05:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd374a55e48>,), **{}) took: 17.034918785095215 sec
[2020-04-07 00:43:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-07 00:43:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-07 00:43:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-07 00:43:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-07 00:43:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37396e5dd8>,), **{}) took: 13.595282793045044 sec
[2020-04-07 00:46:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-07 00:46:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-07 00:46:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-07 00:46:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-07 00:46:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f62cbec0e10>,), **{}) took: 13.579146146774292 sec
[2020-04-08 21:47:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 21:47:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 21:47:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 21:47:51] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 21:47:51] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fda569b2e10>,), **{}) took: 18.671454906463623 sec
[2020-04-08 21:50:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 21:50:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 21:50:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 21:50:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 21:50:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5669741e80>,), **{}) took: 22.553723335266113 sec
[2020-04-08 21:51:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 21:51:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 21:51:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 21:52:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 21:52:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fec126e5e80>,), **{}) took: 23.10308003425598 sec
[2020-04-08 21:53:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 21:53:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 21:53:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 21:53:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 21:53:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f755463fe80>,), **{}) took: 22.838276863098145 sec
[2020-04-08 21:56:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 21:56:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 21:56:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 21:56:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 21:56:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5ad963be48>,), **{}) took: 22.386170387268066 sec
[2020-04-08 21:57:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 21:57:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 21:57:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 21:58:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 21:58:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f452e6cae48>,), **{}) took: 22.61599349975586 sec
[2020-04-08 22:01:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:01:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:01:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:02:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 22:02:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0d17523e80>,), **{}) took: 22.7198007106781 sec
[2020-04-08 22:11:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:11:32] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:11:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:18:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:18:33] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:18:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:19:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:19:45] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:19:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:19:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:20:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 22:20:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f01c054dbe0>,), **{}) took: 22.950300693511963 sec
[2020-04-08 22:21:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:21:08] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:21:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:21:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:21:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 22:21:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efcbb156c18>,), **{}) took: 22.794614791870117 sec
[2020-04-08 22:22:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:22:09] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:22:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:22:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:22:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 22:22:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9e6692dc18>,), **{}) took: 22.508853435516357 sec
[2020-04-08 22:26:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:26:28] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:26:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:26:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:27:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:27:10] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:27:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:27:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:27:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 22:27:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3541f56c18>,), **{}) took: 22.575371026992798 sec
[2020-04-08 22:28:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:28:17] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:28:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:28:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:28:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 22:28:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0103c8aba8>,), **{}) took: 22.753357410430908 sec
[2020-04-08 22:30:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-08 22:30:41] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-08 22:30:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-08 22:30:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-08 22:31:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-08 22:31:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe99eb6dc18>,), **{}) took: 22.7914617061615 sec
[2020-04-09 00:03:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 00:03:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 00:03:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 00:04:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 00:04:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6b57bbae80>,), **{}) took: 20.63639998435974 sec
[2020-04-09 00:06:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 00:06:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 00:06:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 00:06:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 00:06:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9f742fde80>,), **{}) took: 24.014161348342896 sec
[2020-04-09 00:22:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 00:22:45] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-09 00:22:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 00:23:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 00:23:25] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-09 00:23:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 14:08:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 14:08:22] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-09 14:08:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 14:13:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 14:13:53] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-09 14:13:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 14:14:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 14:14:36] - slp - WARNING -- To use data.metrics please install scikit-learn. See https://scikit-learn.org/stable/index.html
[2020-04-09 14:14:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 14:14:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 14:14:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 14:14:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:15:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:15:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f24895feef0>,), **{}) took: 18.874268770217896 sec
[2020-04-09 14:18:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 14:18:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 14:18:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:19:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:19:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fac85258048>,), **{}) took: 18.853614568710327 sec
[2020-04-09 14:20:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:21:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:21:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7b0c8a1fd0>,), **{}) took: 18.8629367351532 sec
[2020-04-09 14:31:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:31:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:31:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3e4d01a048>,), **{}) took: 18.98450016975403 sec
[2020-04-09 14:33:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:33:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:33:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc67a449080>,), **{}) took: 18.748916387557983 sec
[2020-04-09 14:36:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:36:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:36:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6c261f4080>,), **{}) took: 18.993122816085815 sec
[2020-04-09 14:40:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:41:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:41:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6163e21080>,), **{}) took: 20.788575172424316 sec
[2020-04-09 14:43:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:43:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:43:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4f04bff080>,), **{}) took: 20.53290820121765 sec
[2020-04-09 14:52:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 14:53:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 14:53:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb953d35048>,), **{}) took: 19.38155460357666 sec
[2020-04-09 15:02:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 15:02:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 15:02:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f079c85e080>,), **{}) took: 19.460092544555664 sec
[2020-04-09 15:07:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 15:07:51] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 15:07:51] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f33c7bd6080>,), **{}) took: 19.724886894226074 sec
[2020-04-09 15:15:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 15:15:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 15:15:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd901c16080>,), **{}) took: 19.29436159133911 sec
[2020-04-09 15:19:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 15:19:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 15:19:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1414ce7080>,), **{}) took: 19.28264045715332 sec
[2020-04-09 15:21:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 15:21:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 15:21:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6ad85c6080>,), **{}) took: 19.341560125350952 sec
[2020-04-09 15:25:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 15:26:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 15:26:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f33187ce080>,), **{}) took: 19.355175971984863 sec
[2020-04-09 16:22:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 16:23:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 16:23:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4de914f080>,), **{}) took: 19.448994159698486 sec
[2020-04-09 16:26:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 16:27:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 16:27:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6862414080>,), **{}) took: 19.30510115623474 sec
[2020-04-09 16:48:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 16:49:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 16:49:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff54db990f0>,), **{}) took: 19.47469735145569 sec
[2020-04-09 17:02:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 17:02:37] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 17:02:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1bca35a1d0>,), **{}) took: 19.536666870117188 sec
[2020-04-09 19:27:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 19:27:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 19:27:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 19:28:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 19:28:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3faaa83518>,), **{}) took: 19.600327968597412 sec
[2020-04-09 19:28:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-09 19:28:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-09 19:28:49] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-09 19:28:49] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-09 19:29:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-09 19:29:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-09 19:29:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-09 19:29:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-09 19:29:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faad7f170b8>,), **{}) took: 19.395885705947876 sec
[2020-04-09 19:29:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-09 19:29:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-09 19:30:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-09 19:30:09] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-10 14:02:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:02:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:02:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:03:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:03:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbbf8890780>,), **{}) took: 29.55148220062256 sec
[2020-04-10 14:03:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-10 14:03:19] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-10 14:03:21] - slp - ERROR -- Current run is terminating due to exception: forward() takes from 2 to 3 positional arguments but 6 were given.
[2020-04-10 14:03:21] - slp - ERROR -- Engine run is terminating due to exception: forward() takes from 2 to 3 positional arguments but 6 were given.
[2020-04-10 14:11:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:11:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:11:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:12:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:12:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f94ad8c94e0>,), **{}) took: 62.93668222427368 sec
[2020-04-10 14:13:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:13:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:13:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:13:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:13:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe3d31534e0>,), **{}) took: 23.91708993911743 sec
[2020-04-10 14:13:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-10 14:13:37] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-10 14:13:37] - slp - ERROR -- Current run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 150.
[2020-04-10 14:13:37] - slp - ERROR -- Engine run is terminating due to exception: input.size(-1) must be equal to input_size. Expected 300, got 150.
[2020-04-10 14:16:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:16:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:16:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:16:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:16:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcef139d4e0>,), **{}) took: 23.855624437332153 sec
[2020-04-10 14:16:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-10 14:16:51] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-10 14:17:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-10 14:17:54] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-10 14:18:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:18:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:18:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:18:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:18:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa9d31574a8>,), **{}) took: 24.40857720375061 sec
[2020-04-10 14:18:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-10 14:18:47] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-10 14:19:50] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-10 14:19:50] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-10 14:40:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:40:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:40:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:42:55] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:42:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe1a84fd4e0>,), **{}) took: 119.36988258361816 sec
[2020-04-10 14:49:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:49:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:49:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:50:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:50:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9fc12e6518>,), **{}) took: 93.29353642463684 sec
[2020-04-10 14:52:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:52:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:52:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:52:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:52:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7feb723fe4a8>,), **{}) took: 32.06675601005554 sec
[2020-04-10 14:53:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:53:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:53:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:54:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:54:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f94768c64e0>,), **{}) took: 24.31277060508728 sec
[2020-04-10 14:54:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:54:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:54:49] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:55:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:55:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5d47f97518>,), **{}) took: 30.682292222976685 sec
[2020-04-10 14:55:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-10 14:55:46] - slp - INFO -- Trainer will run for
model: DRNN(
  (embed): Embedding(2196024, 300)
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-10 14:57:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-10 14:57:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-10 14:57:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 14:57:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 14:57:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 14:58:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 14:58:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd187e5d518>,), **{}) took: 24.402031183242798 sec
[2020-04-10 14:58:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-10 14:58:36] - slp - INFO -- Trainer will run for
model: DRNN(
  (embed): Embedding(2196024, 300)
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-10 15:00:30] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-10 15:00:30] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-10 15:28:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-10 15:28:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-10 15:28:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-10 15:29:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-10 15:29:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f954c7cd518>,), **{}) took: 24.28266215324402 sec
[2020-04-10 15:29:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-10 15:29:42] - slp - INFO -- Trainer will run for
model: DRNN(
  (embed): Embedding(2196024, 300)
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-10 15:30:45] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-10 15:30:45] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 00:02:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 00:03:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 00:03:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6c8f1870f0>,), **{}) took: 44.22288179397583 sec
[2020-04-15 00:10:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 00:10:45] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 00:10:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe5a18c3e10>,), **{}) took: 27.048004865646362 sec
[2020-04-15 00:11:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 00:12:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 00:12:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1f2df5c0f0>,), **{}) took: 24.64434552192688 sec
[2020-04-15 00:13:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 00:13:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 00:13:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8cb3983128>,), **{}) took: 17.424815893173218 sec
[2020-04-15 00:18:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 00:19:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 00:19:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2c775a5128>,), **{}) took: 17.916656732559204 sec
[2020-04-15 00:25:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 00:25:26] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 00:25:26] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f36dbdd8cc0>,), **{}) took: 17.470729112625122 sec
[2020-04-15 14:00:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:00:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:02:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:02:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:02:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:02:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:02:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9d111534e0>,), **{}) took: 18.511913776397705 sec
[2020-04-15 14:05:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:05:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:05:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:05:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:05:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbdc466a4e0>,), **{}) took: 25.642610788345337 sec
[2020-04-15 14:05:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:05:41] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:05:41] - slp - ERROR -- Current run is terminating due to exception: list index out of range.
[2020-04-15 14:05:41] - slp - ERROR -- Engine run is terminating due to exception: list index out of range.
[2020-04-15 14:07:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:07:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:07:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:07:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:07:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdc52f5a518>,), **{}) took: 25.317464351654053 sec
[2020-04-15 14:09:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:09:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:09:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:09:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:09:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f95eef184e0>,), **{}) took: 24.970491886138916 sec
[2020-04-15 14:09:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:09:43] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:09:44] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:09:44] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:17:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:17:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:17:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:18:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:18:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f785d20c4e0>,), **{}) took: 23.87056827545166 sec
[2020-04-15 14:18:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:18:27] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:18:31] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:18:31] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:23:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:23:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:23:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:23:46] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:23:46] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f630d5ab4e0>,), **{}) took: 22.841413259506226 sec
[2020-04-15 14:23:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:23:58] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:24:00] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:24:00] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:24:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:24:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:24:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:24:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:24:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa4d6ed94e0>,), **{}) took: 22.91495633125305 sec
[2020-04-15 14:25:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:25:01] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:25:04] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:25:04] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:27:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:27:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:27:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:27:58] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:27:58] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb4285f14e0>,), **{}) took: 22.830332040786743 sec
[2020-04-15 14:28:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:28:08] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:28:13] - slp - ERROR -- Current run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:28:13] - slp - ERROR -- Engine run is terminating due to exception: 'list' object has no attribute 'size'.
[2020-04-15 14:29:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:29:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:29:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:29:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:29:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f985e130c18>,), **{}) took: 23.147033214569092 sec
[2020-04-15 14:32:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:32:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:32:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:32:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:32:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7688d83518>,), **{}) took: 15.73567795753479 sec
[2020-04-15 14:33:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:36:28] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:40:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 14:40:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 14:42:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:42:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:42:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:42:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:42:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcca72d1518>,), **{}) took: 16.15578842163086 sec
[2020-04-15 14:44:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:44:14] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:44:18] - slp - ERROR -- Current run is terminating due to exception: object of type 'ToTensor' has no len().
[2020-04-15 14:44:18] - slp - ERROR -- Engine run is terminating due to exception: object of type 'ToTensor' has no len().
[2020-04-15 14:44:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:44:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:44:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:45:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:45:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fae5abcf518>,), **{}) took: 15.898777961730957 sec
[2020-04-15 14:45:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:45:25] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:47:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 14:47:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 14:47:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:47:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:47:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:48:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:48:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc8329cc518>,), **{}) took: 15.971454620361328 sec
[2020-04-15 14:51:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 14:51:56] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 14:52:53] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 14:52:53] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 14:53:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 14:53:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 14:53:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 14:54:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 14:54:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4715e6b518>,), **{}) took: 15.905125856399536 sec
[2020-04-15 15:01:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 15:01:39] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 15:22:32] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 15:22:32] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 15:22:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 15:22:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 15:22:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 15:22:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 15:22:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f29c4a58518>,), **{}) took: 15.780061960220337 sec
[2020-04-15 15:24:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 15:24:49] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 15:25:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 15:25:52] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 15:25:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 15:25:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 15:25:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 15:26:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 15:26:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f95ef552518>,), **{}) took: 28.1479332447052 sec
[2020-04-15 15:26:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 15:27:15] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 15:28:06] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 15:28:06] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 15:28:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 15:28:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 15:28:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 15:29:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 15:29:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7097bdc0f0>,), **{}) took: 22.70924139022827 sec
[2020-04-15 15:29:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 15:46:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 15:46:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 15:46:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 15:46:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 15:46:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2489506518>,), **{}) took: 24.52631187438965 sec
[2020-04-15 15:49:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 15:49:59] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 15:52:14] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 15:52:14] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 15:52:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 15:52:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 15:52:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 15:52:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 15:52:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fee05533cf8>,), **{}) took: 26.847506046295166 sec
[2020-04-15 15:53:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 15:53:56] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 15:55:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 15:55:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 15:59:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 15:59:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 15:59:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 16:00:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 16:00:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbe494da4e0>,), **{}) took: 45.841116428375244 sec
[2020-04-15 16:03:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 16:03:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 16:03:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 16:03:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 16:03:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8f457f0d30>,), **{}) took: 17.307143688201904 sec
[2020-04-15 16:04:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 16:05:22] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 16:06:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 16:06:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 16:07:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 16:07:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 16:07:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 16:07:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 16:07:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb3a8ce7550>,), **{}) took: 27.851167678833008 sec
[2020-04-15 16:10:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 16:10:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 16:10:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 16:10:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 16:10:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3b75fa7518>,), **{}) took: 18.02457094192505 sec
[2020-04-15 16:11:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 16:11:04] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 16:11:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 16:11:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 16:12:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 16:12:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 16:12:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 16:12:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 16:12:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc10d486518>,), **{}) took: 18.754628896713257 sec
[2020-04-15 16:13:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 16:13:15] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 16:13:18] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 11.93 GiB total capacity; 6.58 GiB already allocated; 61.31 MiB free; 12.17 MiB cached).
[2020-04-15 16:13:18] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 3.19 GiB (GPU 0; 11.93 GiB total capacity; 6.58 GiB already allocated; 61.31 MiB free; 12.17 MiB cached).
[2020-04-15 16:14:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 16:14:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 16:14:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 16:14:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 16:14:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7174678518>,), **{}) took: 24.226879596710205 sec
[2020-04-15 16:14:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 16:14:54] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 16:14:56] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.46 GiB (GPU 0; 11.93 GiB total capacity; 5.64 GiB already allocated; 1023.31 MiB free; 20.82 MiB cached).
[2020-04-15 16:14:56] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.46 GiB (GPU 0; 11.93 GiB total capacity; 5.64 GiB already allocated; 1023.31 MiB free; 20.82 MiB cached).
[2020-04-15 16:15:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 16:15:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 16:15:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 16:15:44] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 16:15:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f76fa7e1518>,), **{}) took: 19.16051697731018 sec
[2020-04-15 16:16:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 16:16:09] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 16:16:10] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.93 GiB total capacity; 5.51 GiB already allocated; 625.31 MiB free; 548.85 MiB cached).
[2020-04-15 16:16:10] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 1.56 GiB (GPU 0; 11.93 GiB total capacity; 5.51 GiB already allocated; 625.31 MiB free; 548.85 MiB cached).
[2020-04-15 18:04:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:04:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:04:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:04:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:04:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fac6db5dda0>,), **{}) took: 36.9804482460022 sec
[2020-04-15 18:05:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:05:07] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:05:08] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 1.42 GiB (GPU 0; 11.93 GiB total capacity; 9.70 GiB already allocated; 742.06 MiB free; 992.09 MiB cached).
[2020-04-15 18:05:08] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 1.42 GiB (GPU 0; 11.93 GiB total capacity; 9.70 GiB already allocated; 742.06 MiB free; 992.09 MiB cached).
[2020-04-15 18:06:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:06:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:06:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:06:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:06:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa827d8e518>,), **{}) took: 16.32077932357788 sec
[2020-04-15 18:06:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:06:45] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:06:45] - slp - ERROR -- Current run is terminating due to exception: 'tuple' object has no attribute 'log_softmax'.
[2020-04-15 18:06:45] - slp - ERROR -- Engine run is terminating due to exception: 'tuple' object has no attribute 'log_softmax'.
[2020-04-15 18:09:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:09:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:09:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:09:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:09:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa4fa0ee080>,), **{}) took: 15.311490535736084 sec
[2020-04-15 18:09:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:09:47] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:09:48] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.77 GiB (GPU 0; 11.93 GiB total capacity; 9.11 GiB already allocated; 1.03 GiB free; 1.25 GiB cached).
[2020-04-15 18:09:49] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.77 GiB (GPU 0; 11.93 GiB total capacity; 9.11 GiB already allocated; 1.03 GiB free; 1.25 GiB cached).
[2020-04-15 18:10:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:10:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:10:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:10:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:10:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3cce8d3518>,), **{}) took: 16.045496463775635 sec
[2020-04-15 18:11:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:11:02] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:11:03] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.43 GiB (GPU 0; 11.93 GiB total capacity; 8.29 GiB already allocated; 1.99 GiB free; 1.11 GiB cached).
[2020-04-15 18:11:03] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.43 GiB (GPU 0; 11.93 GiB total capacity; 8.29 GiB already allocated; 1.99 GiB free; 1.11 GiB cached).
[2020-04-15 18:33:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:33:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:33:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:33:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:33:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f06a2b13518>,), **{}) took: 15.744361162185669 sec
[2020-04-15 18:33:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:33:38] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:33:39] - slp - ERROR -- Current run is terminating due to exception: not enough values to unpack (expected 4, got 3).
[2020-04-15 18:33:39] - slp - ERROR -- Engine run is terminating due to exception: not enough values to unpack (expected 4, got 3).
[2020-04-15 18:35:57] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:35:57] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:35:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:36:13] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:36:13] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff9269fe518>,), **{}) took: 15.973966121673584 sec
[2020-04-15 18:36:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:36:32] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:39:42] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 18:39:42] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 18:40:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:40:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:40:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:40:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:40:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f072f677518>,), **{}) took: 15.672510862350464 sec
[2020-04-15 18:41:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:41:01] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:41:02] - slp - ERROR -- Current run is terminating due to exception: name 'torch' is not defined.
[2020-04-15 18:41:02] - slp - ERROR -- Engine run is terminating due to exception: name 'torch' is not defined.
[2020-04-15 18:41:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:41:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:41:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:41:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:41:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd792941518>,), **{}) took: 15.576791048049927 sec
[2020-04-15 18:41:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:41:49] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:44:38] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 18:44:38] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 18:50:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:50:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:50:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:50:27] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:50:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f80922e3518>,), **{}) took: 15.058562517166138 sec
[2020-04-15 18:50:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:50:47] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 18:50:48] - slp - ERROR -- Current run is terminating due to exception: 'tuple' object has no attribute 'log_softmax'.
[2020-04-15 18:50:48] - slp - ERROR -- Engine run is terminating due to exception: 'tuple' object has no attribute 'log_softmax'.
[2020-04-15 18:58:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 18:58:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 18:58:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 18:58:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 18:58:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3f91c4a518>,), **{}) took: 16.026633739471436 sec
[2020-04-15 18:58:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 18:58:38] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:15:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:15:44] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:15:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:15:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:15:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:16:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:16:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7cc249b518>,), **{}) took: 24.829163789749146 sec
[2020-04-15 19:16:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:16:37] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:21:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:21:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:21:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:21:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:21:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:22:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:22:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faca908e518>,), **{}) took: 18.738078594207764 sec
[2020-04-15 19:22:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:22:33] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:23:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:23:51] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:24:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:24:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:24:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:25:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:25:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f822e322518>,), **{}) took: 15.755704402923584 sec
[2020-04-15 19:25:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:25:48] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:27:16] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:27:16] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:29:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:29:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:29:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:29:50] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:29:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f99f756a518>,), **{}) took: 16.620237588882446 sec
[2020-04-15 19:30:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:30:09] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:32:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:32:52] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:32:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:32:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:32:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:33:10] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:33:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f45d8207518>,), **{}) took: 16.13858723640442 sec
[2020-04-15 19:33:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:33:31] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:34:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:34:24] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:36:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:36:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:36:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:36:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:36:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd854666518>,), **{}) took: 16.170487642288208 sec
[2020-04-15 19:36:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:36:36] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:38:12] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:38:12] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:38:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:38:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:38:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:38:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:38:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1457e6c518>,), **{}) took: 26.408469438552856 sec
[2020-04-15 19:39:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:39:17] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:40:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:40:01] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:40:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:40:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:40:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:40:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:40:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc3c8df9518>,), **{}) took: 24.40776038169861 sec
[2020-04-15 19:41:00] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:41:00] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:42:27] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:42:27] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:42:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:42:32] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:42:32] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:42:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:42:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3570381518>,), **{}) took: 24.313231229782104 sec
[2020-04-15 19:43:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:43:47] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:45:05] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:45:05] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:45:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:45:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:45:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:45:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:45:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ffab6113160>,), **{}) took: 24.698282718658447 sec
[2020-04-15 19:45:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:45:51] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:46:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 19:46:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 19:46:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:46:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:46:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:46:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:46:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4f52e72518>,), **{}) took: 15.599548101425171 sec
[2020-04-15 19:47:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:47:08] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 19:53:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 19:53:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 19:53:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 19:54:03] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 19:54:03] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f3553492518>,), **{}) took: 26.069398403167725 sec
[2020-04-15 19:54:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 19:54:28] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 20:02:55] - slp - ERROR -- Current run is terminating due to exception: [Errno 5] Input/output error.
[2020-04-15 20:02:55] - slp - ERROR -- Engine run is terminating due to exception: [Errno 5] Input/output error.
[2020-04-15 20:03:41] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-15 20:03:41] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-15 20:03:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-15 20:03:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-15 20:03:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-15 20:04:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-15 20:04:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb4df8e4518>,), **{}) took: 23.49129295349121 sec
[2020-04-15 20:04:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-15 20:04:33] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-15 20:20:20] - slp - ERROR -- Current run is terminating due to exception: [Errno 5] Input/output error.
[2020-04-15 20:20:20] - slp - ERROR -- Engine run is terminating due to exception: [Errno 5] Input/output error.
[2020-04-16 01:16:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:16:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:16:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:16:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:16:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9f84956518>,), **{}) took: 16.813215255737305 sec
[2020-04-16 01:17:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:17:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:17:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:17:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:17:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd671e55518>,), **{}) took: 16.906742095947266 sec
[2020-04-16 01:17:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:17:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:18:00] - slp - ERROR -- Current run is terminating due to exception: tuple index out of range.
[2020-04-16 01:18:00] - slp - ERROR -- Engine run is terminating due to exception: tuple index out of range.
[2020-04-16 01:21:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:21:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:21:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:21:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:21:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2c24e31518>,), **{}) took: 17.832737922668457 sec
[2020-04-16 01:21:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:21:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:21:50] - slp - ERROR -- Current run is terminating due to exception: name 'features' is not defined.
[2020-04-16 01:21:50] - slp - ERROR -- Engine run is terminating due to exception: name 'features' is not defined.
[2020-04-16 01:23:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:23:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:23:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:23:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:23:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f179d9c8518>,), **{}) took: 16.830281734466553 sec
[2020-04-16 01:24:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:24:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:25:11] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-16 01:25:11] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-16 01:27:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:27:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:27:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:27:35] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:27:35] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f24eed354a8>,), **{}) took: 17.056522369384766 sec
[2020-04-16 01:27:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:27:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:28:21] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.93 GiB total capacity; 4.74 GiB already allocated; 1.31 MiB free; 10.53 MiB cached).
[2020-04-16 01:28:21] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.93 GiB total capacity; 4.74 GiB already allocated; 1.31 MiB free; 10.53 MiB cached).
[2020-04-16 01:30:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:30:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:30:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:30:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:30:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f658115f4a8>,), **{}) took: 17.356837511062622 sec
[2020-04-16 01:30:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:30:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:31:05] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.68 GiB already allocated; 19.31 MiB free; 4.52 MiB cached).
[2020-04-16 01:31:05] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.68 GiB already allocated; 19.31 MiB free; 4.52 MiB cached).
[2020-04-16 01:33:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:33:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:33:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:34:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:34:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1e80fb5da0>,), **{}) took: 18.50231671333313 sec
[2020-04-16 01:34:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:34:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:34:24] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-16 01:34:24] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-16 01:34:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:34:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:34:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:35:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:35:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7c62e1c4a8>,), **{}) took: 11.693137407302856 sec
[2020-04-16 01:35:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:35:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:42:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:42:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:42:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:42:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:42:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2a3ea2ac50>,), **{}) took: 17.471192836761475 sec
[2020-04-16 01:43:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:43:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:43:10] - slp - ERROR -- Current run is terminating due to exception: forward() missing 1 required positional argument: 'title_lengths'.
[2020-04-16 01:43:10] - slp - ERROR -- Engine run is terminating due to exception: forward() missing 1 required positional argument: 'title_lengths'.
[2020-04-16 01:47:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:47:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:47:24] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:47:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:47:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe5768924e0>,), **{}) took: 17.40204691886902 sec
[2020-04-16 01:48:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:48:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:48:08] - slp - ERROR -- Current run is terminating due to exception: name 'output_title' is not defined.
[2020-04-16 01:48:08] - slp - ERROR -- Engine run is terminating due to exception: name 'output_title' is not defined.
[2020-04-16 01:51:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:51:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:51:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:51:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:51:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f93042cc4e0>,), **{}) took: 17.341361045837402 sec
[2020-04-16 01:51:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:51:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:52:22] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.69 GiB already allocated; 19.31 MiB free; 6.24 MiB cached).
[2020-04-16 01:52:22] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.69 GiB already allocated; 19.31 MiB free; 6.24 MiB cached).
[2020-04-16 01:52:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 01:52:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 01:52:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 01:53:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 01:53:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fae821294e0>,), **{}) took: 16.472786903381348 sec
[2020-04-16 01:53:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 01:53:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 01:54:23] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.69 GiB already allocated; 19.31 MiB free; 5.35 MiB cached).
[2020-04-16 01:54:23] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.69 GiB already allocated; 19.31 MiB free; 5.35 MiB cached).
[2020-04-16 08:33:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 08:33:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 18:13:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 18:13:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 20:55:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 20:55:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 21:50:47] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 21:50:47] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 21:50:47] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 21:51:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 21:51:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f16e4726518>,), **{}) took: 31.662853956222534 sec
[2020-04-16 21:51:45] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 21:51:45] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 22:45:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 22:45:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 22:45:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 22:47:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 22:47:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe7397ab518>,), **{}) took: 122.41167402267456 sec
[2020-04-16 23:01:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:01:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:01:21] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:01:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:01:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f35bb31d518>,), **{}) took: 17.75021982192993 sec
[2020-04-16 23:01:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:01:59] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:07:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-16 23:07:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-16 23:09:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:09:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:09:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:11:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:11:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f88c13784e0>,), **{}) took: 121.69506192207336 sec
[2020-04-16 23:12:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:12:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:12:38] - slp - ERROR -- Current run is terminating due to exception: name 'pbd' is not defined.
[2020-04-16 23:12:38] - slp - ERROR -- Engine run is terminating due to exception: name 'pbd' is not defined.
[2020-04-16 23:14:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:14:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:14:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:16:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:16:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faed8d2d4e0>,), **{}) took: 127.15731763839722 sec
[2020-04-16 23:16:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:16:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:16:44] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.27 GiB already allocated; 19.31 MiB free; 2.72 MiB cached).
[2020-04-16 23:16:44] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.27 GiB already allocated; 19.31 MiB free; 2.72 MiB cached).
[2020-04-16 23:18:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:18:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:18:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:19:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:19:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fbf997254a8>,), **{}) took: 26.670564889907837 sec
[2020-04-16 23:19:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:19:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:22:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-16 23:22:52] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-16 23:25:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:25:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:25:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:25:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:25:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd36243d4e0>,), **{}) took: 25.298991918563843 sec
[2020-04-16 23:25:55] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:25:55] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:29:48] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 11.93 GiB total capacity; 4.25 GiB already allocated; 5.31 MiB free; 36.93 MiB cached).
[2020-04-16 23:29:49] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-16 23:29:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:29:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:29:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:30:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:30:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f92d857f4e0>,), **{}) took: 17.590651512145996 sec
[2020-04-16 23:30:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:30:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:30:47] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.93 GiB total capacity; 4.28 GiB already allocated; 1.31 MiB free; 17.03 MiB cached).
[2020-04-16 23:30:47] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 11.93 GiB total capacity; 4.28 GiB already allocated; 1.31 MiB free; 17.03 MiB cached).
[2020-04-16 23:30:52] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:30:52] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:30:52] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:32:57] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:32:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8c1d9fb4e0>,), **{}) took: 125.35305714607239 sec
[2020-04-16 23:34:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:34:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:38:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:38:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:44:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-16 23:44:23] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-16 23:44:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:44:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:44:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:44:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:44:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f174887e518>,), **{}) took: 17.879050731658936 sec
[2020-04-16 23:45:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:45:12] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:49:15] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 566.00 MiB (GPU 0; 11.93 GiB total capacity; 3.75 GiB already allocated; 523.31 MiB free; 33.91 MiB cached).
[2020-04-16 23:49:15] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 566.00 MiB (GPU 0; 11.93 GiB total capacity; 3.75 GiB already allocated; 523.31 MiB free; 33.91 MiB cached).
[2020-04-16 23:49:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:49:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:49:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:50:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:50:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9617725518>,), **{}) took: 14.052825689315796 sec
[2020-04-16 23:50:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:50:23] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:58:18] - slp - ERROR -- Current run is terminating due to exception: Expected input batch_size (1) to match target batch_size (8)..
[2020-04-16 23:58:18] - slp - ERROR -- Engine run is terminating due to exception: Expected input batch_size (1) to match target batch_size (8)..
[2020-04-16 23:59:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:59:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:59:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-16 23:59:24] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-16 23:59:24] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f54ede82518>,), **{}) took: 14.222066640853882 sec
[2020-04-16 23:59:42] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-16 23:59:42] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-16 23:59:51] - slp - ERROR -- Current run is terminating due to exception: Expected input batch_size (1) to match target batch_size (8)..
[2020-04-16 23:59:51] - slp - ERROR -- Engine run is terminating due to exception: Expected input batch_size (1) to match target batch_size (8)..
[2020-04-16 23:59:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-16 23:59:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-16 23:59:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-17 00:00:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-17 00:00:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff77be8d518>,), **{}) took: 14.141299486160278 sec
[2020-04-17 00:00:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-17 00:00:26] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-17 00:02:59] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-17 00:02:59] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-17 00:04:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-17 00:04:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-17 00:04:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-17 00:04:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-17 00:04:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f13af592518>,), **{}) took: 14.061581134796143 sec
[2020-04-17 00:04:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-17 00:04:33] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-17 00:05:58] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-17 00:05:58] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-17 00:06:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-17 00:06:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-17 00:06:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-17 00:06:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-17 00:06:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f56cc402518>,), **{}) took: 15.766521692276001 sec
[2020-04-17 00:06:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-17 00:06:43] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-17 00:06:45] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 610.00 MiB (GPU 0; 11.93 GiB total capacity; 3.76 GiB already allocated; 533.31 MiB free; 17.54 MiB cached).
[2020-04-17 00:06:45] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 610.00 MiB (GPU 0; 11.93 GiB total capacity; 3.76 GiB already allocated; 533.31 MiB free; 17.54 MiB cached).
[2020-04-17 16:33:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-17 16:33:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-17 16:33:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-17 16:34:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-17 16:34:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb2f7f35518>,), **{}) took: 31.70283079147339 sec
[2020-04-17 16:34:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-17 16:34:40] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-17 16:35:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-17 16:35:35] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-17 16:39:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-17 16:39:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-17 16:39:44] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-17 16:40:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-17 16:40:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5eaa978518>,), **{}) took: 22.19778299331665 sec
[2020-04-17 16:40:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-17 16:40:37] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-17 20:42:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-17 20:42:44] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-17 23:31:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-17 23:31:41] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-18 02:21:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-18 02:21:16] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-18 04:34:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-18 04:34:43] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 02:46:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-19 02:46:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-19 02:46:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-19 02:47:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-19 02:47:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f33798a1da0>,), **{}) took: 41.4998779296875 sec
[2020-04-19 02:48:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 02:48:01] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 02:51:59] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 874.00 MiB (GPU 0; 11.93 GiB total capacity; 5.61 GiB already allocated; 662.31 MiB free; 313.46 MiB cached).
[2020-04-19 02:51:59] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 874.00 MiB (GPU 0; 11.93 GiB total capacity; 5.61 GiB already allocated; 662.31 MiB free; 313.46 MiB cached).
[2020-04-19 02:53:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-19 02:53:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-19 02:53:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-19 02:53:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-19 02:53:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc6ea522cc0>,), **{}) took: 14.588500022888184 sec
[2020-04-19 02:53:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 02:53:50] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 02:55:20] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 11.93 GiB total capacity; 5.72 GiB already allocated; 526.31 MiB free; 338.21 MiB cached).
[2020-04-19 02:55:20] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 906.00 MiB (GPU 0; 11.93 GiB total capacity; 5.72 GiB already allocated; 526.31 MiB free; 338.21 MiB cached).
[2020-04-19 02:56:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-19 02:56:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-19 02:56:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-19 02:56:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-19 02:56:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fed1a4da518>,), **{}) took: 13.934355974197388 sec
[2020-04-19 02:56:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 02:56:35] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 02:57:16] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 874.00 MiB (GPU 0; 11.93 GiB total capacity; 5.61 GiB already allocated; 242.31 MiB free; 732.33 MiB cached).
[2020-04-19 02:57:16] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 874.00 MiB (GPU 0; 11.93 GiB total capacity; 5.61 GiB already allocated; 242.31 MiB free; 732.33 MiB cached).
[2020-04-19 03:00:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-19 03:00:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-19 03:00:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-19 03:00:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-19 03:00:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f08aa8c3518>,), **{}) took: 14.078237533569336 sec
[2020-04-19 03:00:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 03:00:50] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 03:01:14] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 11.93 GiB total capacity; 5.84 GiB already allocated; 194.31 MiB free; 551.16 MiB cached).
[2020-04-19 03:01:14] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 1.55 GiB (GPU 0; 11.93 GiB total capacity; 5.84 GiB already allocated; 194.31 MiB free; 551.16 MiB cached).
[2020-04-19 03:04:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-19 03:04:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-19 03:04:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-19 03:04:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-19 03:04:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa8ebe11b00>,), **{}) took: 14.377066135406494 sec
[2020-04-19 03:04:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 03:04:36] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 03:06:25] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 874.00 MiB (GPU 0; 11.93 GiB total capacity; 5.61 GiB already allocated; 556.31 MiB free; 418.37 MiB cached).
[2020-04-19 03:06:25] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 874.00 MiB (GPU 0; 11.93 GiB total capacity; 5.61 GiB already allocated; 556.31 MiB free; 418.37 MiB cached).
[2020-04-19 03:10:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-19 03:10:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-19 03:10:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-19 03:10:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-19 03:10:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcaf020e518>,), **{}) took: 13.983858346939087 sec
[2020-04-19 03:11:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-19 03:11:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-19 03:11:26] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-19 03:11:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-19 03:11:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f12fbb534e0>,), **{}) took: 13.79853630065918 sec
[2020-04-19 03:17:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 03:17:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 10:43:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 10:43:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 15:46:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 15:46:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 18:53:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 18:53:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-19 20:55:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-19 20:55:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 03:39:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:39:46] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:39:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:39:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:39:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd433eb5518>,), **{}) took: 9.697084188461304 sec
[2020-04-20 03:40:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:40:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:40:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:40:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:40:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa60feb6518>,), **{}) took: 5.800757884979248 sec
[2020-04-20 03:43:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 03:43:24] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 03:45:20] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:45:20] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:45:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:45:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:45:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdc69689a90>,), **{}) took: 19.177573680877686 sec
[2020-04-20 03:48:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:48:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:48:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:49:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:49:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f08f1ae5d68>,), **{}) took: 18.985485792160034 sec
[2020-04-20 03:50:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-20 03:50:19] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-20 03:51:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:51:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:51:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:51:07] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:51:07] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7156cee518>,), **{}) took: 5.8346171379089355 sec
[2020-04-20 03:51:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:51:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:51:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:51:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:51:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2dbbcc54e0>,), **{}) took: 18.648160934448242 sec
[2020-04-20 03:54:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 03:54:10] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 03:56:52] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-20 03:56:52] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-20 03:57:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:57:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:57:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:57:23] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:57:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc942b1d518>,), **{}) took: 4.406558036804199 sec
[2020-04-20 03:57:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 03:57:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 03:57:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 03:57:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 03:57:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f05cb5d24e0>,), **{}) took: 12.21288013458252 sec
[2020-04-20 04:00:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 04:00:19] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 04:01:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 04:01:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 04:01:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 04:01:31] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-20 04:01:31] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-20 04:01:32] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 04:01:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff58b0f74e0>,), **{}) took: 4.779270172119141 sec
[2020-04-20 04:10:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 04:10:00] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 04:10:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 04:10:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-20 04:10:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-20 04:10:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-20 04:10:05] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 04:10:05] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f12431e94a8>,), **{}) took: 4.8507914543151855 sec
[2020-04-20 04:10:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-20 04:10:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f71fcc0d518>,), **{}) took: 10.797621488571167 sec
[2020-04-20 04:12:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 04:12:37] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 04:13:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 04:13:23] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 05:19:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 05:19:04] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 06:39:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 06:39:54] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 06:49:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 06:49:37] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 07:46:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 07:46:52] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 09:13:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 09:13:23] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 09:59:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 09:59:03] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 11:46:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 11:46:58] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-20 16:49:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-20 16:49:04] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 00:10:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-21 00:10:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-21 00:10:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-21 00:10:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-21 00:10:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdaa981c4e0>,), **{}) took: 20.484355688095093 sec
[2020-04-21 00:11:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-21 00:11:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-21 00:11:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-21 00:11:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-21 00:11:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8f5b5b14e0>,), **{}) took: 16.615146160125732 sec
[2020-04-21 00:14:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-21 00:14:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-21 00:14:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-21 00:14:34] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-21 00:14:34] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa3f4ffa4e0>,), **{}) took: 16.648940801620483 sec
[2020-04-21 00:17:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-21 00:17:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-21 00:17:06] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-21 00:17:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-21 00:17:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcc736114e0>,), **{}) took: 16.541960954666138 sec
[2020-04-21 00:19:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 00:19:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 00:23:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-21 00:23:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-21 00:23:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-21 00:23:59] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-21 00:23:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1b143e8518>,), **{}) took: 25.17018222808838 sec
[2020-04-21 00:24:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 00:24:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 03:33:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 03:33:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 03:36:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 03:36:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 06:20:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 06:20:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 06:39:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 06:39:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 08:45:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 08:45:54] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 09:01:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 09:01:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 11:35:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 11:35:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 12:08:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 12:08:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 21:43:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-21 21:43:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-21 21:43:29] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-21 21:43:40] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-21 21:43:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe6fcf654e0>,), **{}) took: 11.806633472442627 sec
[2020-04-21 21:45:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 21:45:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 21:46:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-21 21:46:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-21 21:46:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-21 21:47:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-21 21:47:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdbb1d774a8>,), **{}) took: 20.705811500549316 sec
[2020-04-21 21:47:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-21 21:47:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-21 21:53:50] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 11.93 GiB total capacity; 5.17 GiB already allocated; 9.31 MiB free; 64.07 MiB cached).
[2020-04-21 21:53:50] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 22.00 MiB (GPU 0; 11.93 GiB total capacity; 5.17 GiB already allocated; 9.31 MiB free; 64.07 MiB cached).
[2020-04-22 00:06:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 00:06:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-22 03:50:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 03:50:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-22 06:33:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 06:33:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-22 08:57:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 08:57:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-22 12:48:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-22 12:48:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-22 12:48:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-22 12:48:36] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-22 12:48:36] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4104b984e0>,), **{}) took: 5.748044013977051 sec
[2020-04-22 12:50:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 12:50:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-22 16:04:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 16:04:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-22 18:57:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 18:57:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-22 21:45:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-22 21:45:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 00:44:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 00:44:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 16:00:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 16:00:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 16:00:49] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 16:01:30] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 16:01:30] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7c3800d5f8>,), **{}) took: 40.33012008666992 sec
[2020-04-23 16:04:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 16:04:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 16:04:57] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 16:05:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 16:05:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4f945f45f8>,), **{}) took: 10.533153057098389 sec
[2020-04-23 16:09:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 16:09:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 16:09:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 16:10:04] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 16:10:04] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc0cfbbd5c0>,), **{}) took: 10.590137481689453 sec
[2020-04-23 16:12:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 16:12:37] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
    (3): GRU(300, 300)
    (4): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (attention): Attention(
    (k): Linear(in_features=300, out_features=300, bias=False)
    (q): Linear(in_features=300, out_features=300, bias=False)
    (v): Linear(in_features=300, out_features=300, bias=False)
    (drop): Dropout(p=0, inplace=False)
  )
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 16:12:39] - slp - ERROR -- Current run is terminating due to exception: Expected input batch_size (9183) to match target batch_size (8)..
[2020-04-23 16:12:39] - slp - ERROR -- Engine run is terminating due to exception: Expected input batch_size (9183) to match target batch_size (8)..
[2020-04-23 16:15:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 16:15:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 16:15:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 16:15:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 16:15:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f185900c630>,), **{}) took: 10.474504470825195 sec
[2020-04-23 16:18:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 16:18:12] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
    (3): GRU(300, 300)
    (4): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (attention): Attention(
    (k): Linear(in_features=300, out_features=300, bias=False)
    (q): Linear(in_features=300, out_features=300, bias=False)
    (v): Linear(in_features=300, out_features=300, bias=False)
    (drop): Dropout(p=0, inplace=False)
  )
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 16:19:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 16:19:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 16:19:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 16:19:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 16:19:42] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 16:19:53] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 16:19:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc9cf4b1dd8>,), **{}) took: 11.252274990081787 sec
[2020-04-23 16:22:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 16:22:29] - slp - INFO -- Trainer will run for
model: DRNN(
  (cells): Sequential(
    (0): GRU(300, 300)
    (1): GRU(300, 300)
    (2): GRU(300, 300)
    (3): GRU(300, 300)
    (4): GRU(300, 300)
  )
  (lookup): Embedding(2196024, 300)
  (attention): Attention(
    (k): Linear(in_features=300, out_features=300, bias=False)
    (q): Linear(in_features=300, out_features=300, bias=False)
    (v): Linear(in_features=300, out_features=300, bias=False)
    (drop): Dropout(p=0, inplace=False)
  )
  (fc): Linear(in_features=300, out_features=2, bias=True)
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 16:25:36] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 16:25:36] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 23:16:19] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:16:19] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:16:20] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:16:29] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:16:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8651bec4e0>,), **{}) took: 8.524169921875 sec
[2020-04-23 23:16:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 23:16:54] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 23:17:04] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 23:17:04] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 23:18:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:18:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:18:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:18:48] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:18:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f517741d4e0>,), **{}) took: 10.029122829437256 sec
[2020-04-23 23:19:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:19:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:19:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:19:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:19:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1ae6f384e0>,), **{}) took: 10.06208610534668 sec
[2020-04-23 23:20:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:20:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:20:04] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:20:14] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:20:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7faf1e9de4e0>,), **{}) took: 9.956044673919678 sec
[2020-04-23 23:20:31] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 23:20:31] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 23:21:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 23:21:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 23:23:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:23:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:23:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:24:08] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:24:08] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f21eb06f518>,), **{}) took: 10.025901794433594 sec
[2020-04-23 23:24:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 23:24:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 23:24:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 23:24:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 23:24:50] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:24:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:24:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:25:00] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:25:00] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f40a9fe9518>,), **{}) took: 9.896078109741211 sec
[2020-04-23 23:25:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 23:25:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 23:26:40] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 23:26:40] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 23:29:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:29:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:29:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:29:21] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:29:21] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc4ee559cc0>,), **{}) took: 10.094735145568848 sec
[2020-04-23 23:29:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 23:29:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 23:32:31] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 23:32:31] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 23:32:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:32:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:32:33] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:32:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:32:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f7ccf186518>,), **{}) took: 9.934646844863892 sec
[2020-04-23 23:33:03] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 23:33:03] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 23:33:47] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 23:33:47] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-23 23:34:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-23 23:34:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-23 23:34:08] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-23 23:34:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-23 23:34:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0a4b546518>,), **{}) took: 10.388787746429443 sec
[2020-04-23 23:34:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-23 23:34:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-23 23:35:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-23 23:35:23] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-24 00:50:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-24 00:50:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-24 00:50:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-24 00:50:17] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-24 00:50:17] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd2e85f3518>,), **{}) took: 10.258521556854248 sec
[2020-04-24 00:50:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 00:50:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 01:22:09] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-24 01:22:09] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-24 01:22:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 01:22:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 01:23:21] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-24 01:23:21] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-24 01:55:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-24 01:55:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-24 01:55:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-24 01:55:56] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-24 01:55:56] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa01b9b5550>,), **{}) took: 1.9729454517364502 sec
[2020-04-24 01:56:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 01:56:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 01:56:05] - slp - ERROR -- Current run is terminating due to exception: forward() takes 3 positional arguments but 5 were given.
[2020-04-24 01:56:05] - slp - ERROR -- Engine run is terminating due to exception: forward() takes 3 positional arguments but 5 were given.
[2020-04-24 01:57:35] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-24 01:57:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-24 01:57:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-24 01:57:38] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-24 01:57:38] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4536e8d550>,), **{}) took: 1.9630606174468994 sec
[2020-04-24 01:57:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 01:57:47] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 05:42:39] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 05:42:39] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 07:26:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 07:26:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 09:10:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 09:10:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 10:46:44] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 10:46:44] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 11:23:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-24 11:23:26] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-24 11:23:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-24 11:24:02] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-24 11:24:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1172835518>,), **{}) took: 35.432140827178955 sec
[2020-04-24 11:24:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 11:24:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 11:24:29] - slp - ERROR -- Current run is terminating due to exception: local variable 'titles' referenced before assignment.
[2020-04-24 11:24:29] - slp - ERROR -- Engine run is terminating due to exception: local variable 'titles' referenced before assignment.
[2020-04-24 11:29:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-24 11:29:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-24 11:29:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-24 11:29:47] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-24 11:29:47] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f80fc873550>,), **{}) took: 1.9494473934173584 sec
[2020-04-24 11:29:56] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 11:29:56] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 11:29:57] - slp - ERROR -- Current run is terminating due to exception: forward() takes 3 positional arguments but 5 were given.
[2020-04-24 11:29:57] - slp - ERROR -- Engine run is terminating due to exception: forward() takes 3 positional arguments but 5 were given.
[2020-04-24 11:38:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-24 11:38:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-24 11:38:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-24 11:38:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-24 11:38:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f07fae72550>,), **{}) took: 2.2362475395202637 sec
[2020-04-24 11:38:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 11:38:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 12:08:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-24 12:08:35] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-24 13:14:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 13:14:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 17:35:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 17:35:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 19:00:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 19:00:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 20:43:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 20:43:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-24 23:35:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-24 23:35:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-24 23:35:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-24 23:36:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-24 23:36:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc47c7db518>,), **{}) took: 28.835841417312622 sec
[2020-04-24 23:38:16] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-24 23:38:16] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 01:15:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 01:15:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 02:57:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 02:57:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 04:45:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 04:45:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 06:07:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 06:07:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 12:03:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-25 12:03:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-25 12:03:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-25 12:03:43] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-25 12:03:43] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f05f1d7d518>,), **{}) took: 2.072089433670044 sec
[2020-04-25 12:05:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 12:05:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 12:06:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-25 12:06:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-25 12:06:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-25 12:06:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-25 12:06:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-25 12:07:01] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-25 12:07:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe51bc6d4e0>,), **{}) took: 2.0544214248657227 sec
[2020-04-25 12:08:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 12:08:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 12:09:31] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-25 12:09:31] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-25 12:11:09] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-25 12:11:09] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-25 12:11:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-25 12:11:12] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-25 12:11:12] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fdd97216518>,), **{}) took: 2.0955543518066406 sec
[2020-04-25 12:12:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 12:12:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 14:31:46] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 14:31:46] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 18:03:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 18:03:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 20:23:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 20:23:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-25 23:29:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-25 23:29:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-28 17:58:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 17:58:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 17:58:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 17:59:28] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 17:59:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f73b961f518>,), **{}) took: 49.11378049850464 sec
[2020-04-28 18:06:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:06:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:06:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:06:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:06:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff4c1c62d68>,), **{}) took: 24.08171010017395 sec
[2020-04-28 18:10:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:10:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:10:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:11:06] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:11:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe19e80c5c0>,), **{}) took: 24.795397520065308 sec
[2020-04-28 18:18:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:18:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:18:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:19:15] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:19:15] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fba8b955e48>,), **{}) took: 23.906137466430664 sec
[2020-04-28 18:27:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:27:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:27:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:28:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:28:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6790b58550>,), **{}) took: 24.445566415786743 sec
[2020-04-28 18:34:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:34:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:34:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:35:19] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:35:19] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1118465518>,), **{}) took: 25.084503650665283 sec
[2020-04-28 18:37:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:37:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:37:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:38:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:38:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ffb29b66550>,), **{}) took: 24.209203004837036 sec
[2020-04-28 18:40:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:40:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:40:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:41:22] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:41:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6028179518>,), **{}) took: 24.50832986831665 sec
[2020-04-28 18:43:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:43:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:43:53] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:44:18] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:44:18] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f16d9cca550>,), **{}) took: 24.37280535697937 sec
[2020-04-28 18:53:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 18:53:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 18:53:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 18:53:52] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 18:53:52] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8f3322c518>,), **{}) took: 24.085564374923706 sec
[2020-04-28 22:05:43] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-28 22:05:43] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-28 22:05:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-28 22:05:49] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-28 22:05:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd9070444e0>,), **{}) took: 5.429734468460083 sec
[2020-04-28 22:07:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-28 22:07:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-28 22:28:19] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-28 22:28:19] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-29 11:18:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:18:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:26:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:26:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:27:45] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:27:45] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:27:46] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-29 11:28:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-29 11:28:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1d534454e0>,), **{}) took: 23.055185317993164 sec
[2020-04-29 11:30:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:30:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:30:37] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-29 11:30:39] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-29 11:30:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6d29337470>,), **{}) took: 2.090552568435669 sec
[2020-04-29 11:37:39] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:37:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:37:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-29 11:37:42] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-29 11:37:42] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f264caa8470>,), **{}) took: 2.082444429397583 sec
[2020-04-29 11:40:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:40:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:40:14] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-29 11:40:16] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-29 11:40:16] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5e5725a4e0>,), **{}) took: 1.996291160583496 sec
[2020-04-29 11:40:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:40:39] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:40:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-29 11:40:54] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-29 11:40:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f646816f470>,), **{}) took: 15.243759155273438 sec
[2020-04-29 11:41:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 11:41:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 11:42:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 11:42:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 11:42:06] - slp - ERROR -- Current run is terminating due to exception: invalid argument 0: Tensors must have same number of dimensions: got 2 and 3 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:62.
[2020-04-29 11:42:06] - slp - ERROR -- Engine run is terminating due to exception: invalid argument 0: Tensors must have same number of dimensions: got 2 and 3 at /pytorch/aten/src/THC/generic/THCTensorMath.cu:62.
[2020-04-29 11:42:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:42:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:42:59] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-29 11:43:09] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-29 11:43:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcf338884e0>,), **{}) took: 10.89807391166687 sec
[2020-04-29 11:44:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 11:44:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 11:45:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-29 11:45:37] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-04-29 11:46:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-29 11:46:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-29 11:46:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-29 11:46:20] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-29 11:46:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fed98588cc0>,), **{}) took: 11.888773679733276 sec
[2020-04-29 11:48:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 11:48:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 12:46:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 12:46:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 12:54:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 12:54:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 13:57:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 13:57:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 14:29:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 14:29:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 15:05:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 15:05:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 15:51:24] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 15:51:24] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 16:17:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 16:17:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-29 17:20:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-29 17:20:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-30 00:39:05] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-30 00:39:05] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-30 00:39:05] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-30 00:39:31] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-30 00:39:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe25281edd8>,), **{}) took: 25.38302516937256 sec
[2020-04-30 00:42:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-30 00:42:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-30 03:08:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-30 03:08:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-30 04:44:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-30 04:44:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-30 07:06:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-30 07:06:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-30 10:10:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-30 10:10:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-30 23:37:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-30 23:37:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-30 23:37:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-30 23:37:25] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-30 23:37:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f31c09765c0>,), **{}) took: 16.166470289230347 sec
[2020-04-30 23:39:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-30 23:39:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-04-30 23:39:44] - slp - ERROR -- Current run is terminating due to exception: .
[2020-04-30 23:39:44] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-04-30 23:40:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-04-30 23:40:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-04-30 23:40:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-04-30 23:40:33] - slp - INFO -- Loaded word embeddings from cache.
[2020-04-30 23:40:33] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fc9eef49128>,), **{}) took: 16.469404458999634 sec
[2020-04-30 23:42:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-04-30 23:42:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-01 01:37:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-01 01:37:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-01 04:41:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-01 04:41:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-01 06:16:12] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-01 06:16:12] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-01 08:20:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-01 08:20:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196024, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-01 23:45:07] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-01 23:45:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-01 23:48:03] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-01 23:48:03] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-01 23:48:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-01 23:48:03] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-01 23:48:57] - slp - INFO -- Found 26806 word vectors.
[2020-05-01 23:48:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe447efc5c0>,), **{}) took: 54.069472551345825 sec
[2020-05-01 23:52:08] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-01 23:52:08] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-01 23:52:09] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-01 23:52:09] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-01 23:53:02] - slp - INFO -- Found 26806 word vectors.
[2020-05-01 23:53:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb88c66b5c0>,), **{}) took: 53.82314682006836 sec
[2020-05-01 23:59:28] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-01 23:59:28] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-01 23:59:28] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-01 23:59:28] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:01:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:01:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:01:02] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:01:02] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:01:57] - slp - INFO -- Found 26806 word vectors.
[2020-05-02 00:01:57] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0e3a0955c0>,), **{}) took: 54.833317279815674 sec
[2020-05-02 00:03:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:03:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:03:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:03:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:03:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:03:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:03:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:03:51] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:04:45] - slp - INFO -- Found 26806 word vectors.
[2020-05-02 00:04:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f09443b75c0>,), **{}) took: 53.40246558189392 sec
[2020-05-02 00:10:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:10:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:10:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:10:54] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:11:48] - slp - INFO -- Found 26806 word vectors.
[2020-05-02 00:11:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7ff942ca5eb8>,), **{}) took: 53.604559898376465 sec
[2020-05-02 00:17:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:17:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:17:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:17:01] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:17:55] - slp - INFO -- Found 26806 word vectors.
[2020-05-02 00:17:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f07ac2535c0>,), **{}) took: 53.89158248901367 sec
[2020-05-02 00:22:13] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 00:22:13] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 00:40:17] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:40:17] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:40:17] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:40:17] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:43:56] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:43:56] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:43:56] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:43:56] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:44:51] - slp - INFO -- Found 26806 word vectors.
[2020-05-02 00:44:51] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f16aa7ca4e0>,), **{}) took: 55.35317039489746 sec
[2020-05-02 00:48:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 00:48:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 00:54:54] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-02 00:54:54] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-02 00:54:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 00:54:54] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 00:54:56] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-02 00:54:56] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-02 00:54:59] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:54:59] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:55:00] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:55:00] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:55:35] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-02 00:55:35] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-02 00:55:35] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 00:55:35] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 00:55:37] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-02 00:55:37] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-02 00:55:40] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 00:55:40] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 00:55:40] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 00:55:40] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 00:56:02] - slp - INFO -- Found 26806 word vectors.
[2020-05-02 00:56:02] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f974ac365c0>,), **{}) took: 62.77829670906067 sec
[2020-05-02 00:56:39] - slp - INFO -- Found 26806 word vectors.
[2020-05-02 00:56:39] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f39c430e4e0>,), **{}) took: 58.23394179344177 sec
[2020-05-02 00:58:29] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 00:58:29] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 00:58:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 00:58:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 01:36:26] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 01:36:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 01:36:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 01:36:27] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 01:36:58] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 01:36:58] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 01:36:58] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 01:36:58] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 01:38:14] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 01:38:14] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fe284557550>,), **{}) took: 76.0596935749054 sec
[2020-05-02 01:38:37] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 01:38:37] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 01:38:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 01:38:38] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 01:39:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 01:39:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 01:39:18] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 01:39:18] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 01:40:37] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 01:40:37] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f6a3cffb588>,), **{}) took: 78.92726397514343 sec
[2020-05-02 01:49:01] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 01:49:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 01:49:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 01:49:01] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 01:50:31] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 01:50:31] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 01:50:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 01:50:31] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 01:51:53] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 01:51:53] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa117e53588>,), **{}) took: 81.82872414588928 sec
[2020-05-02 01:57:28] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 01:57:28] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 03:10:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 03:10:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 03:43:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 03:43:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 04:13:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 04:13:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 05:37:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 05:37:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 05:44:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 05:44:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 07:09:43] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 07:09:43] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 08:40:27] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 08:40:27] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 10:19:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 10:19:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 14:20:36] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 14:20:36] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 14:30:20] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 14:30:20] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 14:32:23] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 14:32:23] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 14:32:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 14:32:25] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 14:35:14] - slp - INFO -- Found 2196022 word vectors.
[2020-05-02 14:35:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f08bbaa6240>,), **{}) took: 182.0811264514923 sec
[2020-05-02 14:37:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 14:37:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 14:37:16] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 14:37:16] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 14:39:50] - slp - INFO -- Found 2196022 word vectors.
[2020-05-02 14:40:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f64e0b9c240>,), **{}) took: 164.75098991394043 sec
[2020-05-02 14:40:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 14:40:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 14:40:38] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 14:40:38] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 14:43:10] - slp - INFO -- Found 2196022 word vectors.
[2020-05-02 14:43:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37b1100278>,), **{}) took: 161.09823894500732 sec
[2020-05-02 14:43:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 14:43:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.001
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 14:43:42] - slp - ERROR -- Current run is terminating due to exception: forward() missing 3 required positional arguments: 'katiallo', 'katiallo1', and 'katiallo2'.
[2020-05-02 14:43:42] - slp - ERROR -- Engine run is terminating due to exception: forward() missing 3 required positional arguments: 'katiallo', 'katiallo1', and 'katiallo2'.
[2020-05-02 14:44:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 14:44:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 14:44:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 14:44:35] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 14:47:08] - slp - INFO -- Found 2196022 word vectors.
[2020-05-02 14:47:20] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f56e71a9518>,), **{}) took: 165.26743912696838 sec
[2020-05-02 14:47:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 14:47:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 14:47:49] - slp - ERROR -- Current run is terminating due to exception: forward() missing 1 required positional argument: 'katiallo2'.
[2020-05-02 14:47:49] - slp - ERROR -- Engine run is terminating due to exception: forward() missing 1 required positional argument: 'katiallo2'.
[2020-05-02 14:50:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 14:50:24] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 14:50:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 14:50:25] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 14:52:57] - slp - INFO -- Found 2196022 word vectors.
[2020-05-02 14:53:06] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2a49411198>,), **{}) took: 161.4918110370636 sec
[2020-05-02 14:53:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 14:53:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 14:56:07] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-02 14:56:07] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-05-02 17:49:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 17:49:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26806, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 19:32:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 19:32:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 20:22:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 20:22:07] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 20:22:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 20:22:07] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 20:22:21] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 20:22:21] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 20:22:22] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 20:22:22] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 20:22:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 20:22:49] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 20:22:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 20:22:50] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 20:23:49] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 20:23:49] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4b249824e0>,), **{}) took: 59.843358278274536 sec
[2020-05-02 20:24:32] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 20:24:32] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 20:27:51] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-02 20:27:51] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-02 20:28:00] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 20:28:01] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 20:28:01] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 20:28:01] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 20:28:55] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 20:28:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f331e0964e0>,), **{}) took: 54.64507293701172 sec
[2020-05-02 20:29:27] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 20:29:27] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 20:29:27] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 20:29:27] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 20:30:23] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 20:30:23] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fb5c0b0abe0>,), **{}) took: 55.68532872200012 sec
[2020-05-02 20:30:50] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 20:30:50] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-02 20:35:43] - slp - ERROR -- Current run is terminating due to exception: Dimension out of range (expected to be in range of [-1, 0], but got 1).
[2020-05-02 20:35:43] - slp - ERROR -- Engine run is terminating due to exception: Dimension out of range (expected to be in range of [-1, 0], but got 1).
[2020-05-02 20:39:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 20:39:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 20:39:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 20:39:11] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 20:41:42] - slp - INFO -- Found 2196022 word vectors.
[2020-05-02 20:42:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f2c7cc17278>,), **{}) took: 178.25170040130615 sec
[2020-05-02 20:55:55] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 20:55:55] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 20:55:55] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 20:55:55] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 20:57:01] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 20:57:01] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f537a2034e0>,), **{}) took: 66.36943006515503 sec
[2020-05-02 21:33:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-02 21:33:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-02 21:33:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-02 21:33:48] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-02 21:34:55] - slp - INFO -- Found 26608 word vectors.
[2020-05-02 21:34:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcf4963f4e0>,), **{}) took: 66.59992718696594 sec
[2020-05-02 21:39:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-02 21:39:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 01:29:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 01:29:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 05:20:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 05:20:54] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 09:18:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 09:18:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 12:32:32] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-03 12:32:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-03 12:32:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-03 12:32:34] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-03 12:32:42] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-03 12:32:42] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-03 12:32:43] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-03 12:32:43] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-03 12:33:44] - slp - INFO -- Found 26608 word vectors.
[2020-05-03 12:33:44] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fafa815c470>,), **{}) took: 61.35695719718933 sec
[2020-05-03 12:34:22] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 12:34:22] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 12:52:51] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 12:52:51] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 13:57:26] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 13:57:26] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 15:47:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 15:47:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 16:30:49] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-03 16:30:50] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-03 16:30:50] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-03 16:30:50] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-03 16:31:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-03 16:31:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-03 16:31:10] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-03 16:31:10] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-03 16:31:30] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-03 16:31:30] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-03 16:31:31] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-03 16:31:31] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-03 16:32:40] - slp - INFO -- Found 26608 word vectors.
[2020-05-03 16:32:40] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f1e786b14e0>,), **{}) took: 69.82774877548218 sec
[2020-05-03 16:34:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 16:34:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 17:19:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 17:19:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 18:38:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 18:38:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 19:15:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 19:15:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 20:03:11] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 20:03:11] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 20:48:15] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-03 20:48:15] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-03 20:48:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-03 20:48:15] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-03 20:48:22] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-03 20:48:22] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-03 20:48:23] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-03 20:48:23] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-03 20:49:22] - slp - INFO -- Found 26608 word vectors.
[2020-05-03 20:49:22] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd7fade8c50>,), **{}) took: 59.20630145072937 sec
[2020-05-03 20:50:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 20:50:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 22:16:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 22:16:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-03 23:34:18] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-03 23:34:18] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 00:19:21] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 00:19:21] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 01:31:13] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-04 01:31:13] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-04 01:31:13] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-04 01:31:13] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-04 01:31:25] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-04 01:31:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-04 01:31:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-04 01:31:25] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-04 01:32:28] - slp - INFO -- Found 26608 word vectors.
[2020-05-04 01:32:28] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fd31e51f4e0>,), **{}) took: 62.620824337005615 sec
[2020-05-04 01:34:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 01:34:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 06:15:54] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 06:15:54] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 09:48:41] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 09:48:41] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 12:02:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 12:02:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 13:37:10] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 13:37:10] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 14:42:01] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-04 14:42:01] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-04 14:42:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 14:42:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 14:42:02] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-04 14:42:02] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-04 15:38:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 15:38:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 17:56:23] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-04 17:56:23] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-04 17:56:23] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 17:56:23] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(26608, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 20:54:36] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-04 20:54:36] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-04 20:54:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-04 20:54:36] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-04 20:55:14] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-04 20:55:14] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-04 20:55:15] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-04 20:55:15] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-04 20:57:49] - slp - INFO -- Found 2196022 word vectors.
[2020-05-04 20:57:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7efd22a135f8>,), **{}) took: 164.21823453903198 sec
[2020-05-04 21:00:15] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 21:00:15] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 21:43:41] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-04 21:43:41] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-04 21:43:41] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-04 21:43:41] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-04 21:46:09] - slp - INFO -- Found 2196022 word vectors.
[2020-05-04 21:46:32] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f666c1454e0>,), **{}) took: 170.51605248451233 sec
[2020-05-04 21:48:40] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 21:48:40] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-04 22:56:52] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-04 22:56:52] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 01:10:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 01:10:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 02:02:34] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 02:02:34] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 03:49:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 03:49:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 03:50:05] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 03:50:05] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 05:37:08] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 05:37:08] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 07:31:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 07:31:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 08:16:48] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-05 08:16:48] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-05 08:16:48] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-05 08:16:48] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-05 08:17:11] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-05 08:17:11] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-05 08:17:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-05 08:17:11] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-05 08:19:41] - slp - INFO -- Found 2196022 word vectors.
[2020-05-05 08:19:54] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa493a4a198>,), **{}) took: 163.0961833000183 sec
[2020-05-05 08:25:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 08:25:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 08:25:12] - slp - ERROR -- Current run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.14 GiB already allocated; 9.31 MiB free; 9.63 MiB cached).
[2020-05-05 08:25:12] - slp - ERROR -- Engine run is terminating due to exception: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 11.93 GiB total capacity; 4.14 GiB already allocated; 9.31 MiB free; 9.63 MiB cached).
[2020-05-05 09:56:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 09:56:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 12:33:33] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-05 12:33:33] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-05 12:33:34] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-05 12:33:34] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-05 12:36:04] - slp - INFO -- Found 2196022 word vectors.
[2020-05-05 12:36:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fa3dc4104e0>,), **{}) took: 156.20729851722717 sec
[2020-05-05 12:37:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 12:37:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 14:00:33] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 14:00:33] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 15:11:38] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 15:11:38] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 16:07:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 16:07:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 16:18:24] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-05 16:18:25] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-05 16:18:25] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-05 16:18:25] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-05 16:20:53] - slp - INFO -- Found 2196022 word vectors.
[2020-05-05 16:20:59] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f5e962b05f8>,), **{}) took: 154.132417678833 sec
[2020-05-05 16:22:59] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 16:22:59] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 17:00:48] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 17:00:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 17:49:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 17:49:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 20:30:25] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 20:30:25] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-05 22:32:01] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-05 22:32:01] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-06 00:15:47] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: None
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-06 00:15:48] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-07 16:05:16] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-07 16:05:16] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-07 16:07:04] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-07 16:07:04] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-07 16:08:18] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-07 16:08:18] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-07 16:08:19] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-07 16:08:19] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-07 16:10:45] - slp - INFO -- Found 2196022 word vectors.
[2020-05-07 16:10:55] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f10bbad26d8>,), **{}) took: 155.6139702796936 sec
[2020-05-07 16:21:51] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-07 16:21:51] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-07 16:21:51] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-07 16:21:51] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-07 16:24:17] - slp - INFO -- Found 2196022 word vectors.
[2020-05-07 16:24:27] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f81c92bf6d8>,), **{}) took: 155.7155418395996 sec
[2020-05-07 16:28:53] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-07 16:28:53] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-07 16:28:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-07 16:28:54] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-07 16:31:21] - slp - INFO -- Found 2196022 word vectors.
[2020-05-07 16:31:31] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f0f57b6ce10>,), **{}) took: 157.59252953529358 sec
[2020-05-07 16:31:57] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 30
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-07 16:31:57] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(699, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=699, out_features=699, bias=True)
    (context): Linear(in_features=699, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-07 16:31:58] - slp - ERROR -- Current run is terminating due to exception: size mismatch, m1: [40 x 600], m2: [699 x 699] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-05-07 16:31:58] - slp - ERROR -- Engine run is terminating due to exception: size mismatch, m1: [40 x 600], m2: [699 x 699] at /pytorch/aten/src/THC/generic/THCTensorMathBlas.cu:273.
[2020-05-07 16:54:10] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-07 16:54:10] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-07 16:54:11] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-07 16:54:11] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-07 16:56:37] - slp - INFO -- Found 2196022 word vectors.
[2020-05-07 16:56:48] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f524e6126d8>,), **{}) took: 157.1900908946991 sec
[2020-05-07 16:57:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 30
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-07 16:57:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-15 14:12:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-15 14:12:35] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-15 14:12:36] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-15 14:12:37] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-15 14:15:44] - slp - INFO -- Found 2196022 word vectors.
[2020-05-15 14:16:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fefef7376d8>,), **{}) took: 212.88414406776428 sec
[2020-05-15 14:17:53] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 30
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-15 14:17:53] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-15 15:16:22] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-05-15 15:16:22] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-15 15:16:26] - slp - ERROR -- Engine run is terminating due to exception: .
[2020-05-15 15:16:26] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-15 15:16:38] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-15 15:16:38] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-15 15:16:39] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-15 15:16:39] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-15 15:19:16] - slp - INFO -- Found 2196022 word vectors.
[2020-05-15 15:19:29] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f102358c6d8>,), **{}) took: 169.77061653137207 sec
[2020-05-15 15:19:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 30
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-15 15:19:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-15 16:06:55] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-15 16:06:55] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-15 16:06:55] - slp - ERROR -- Engine run is terminating due to exception: 'loss'.
[2020-05-15 16:06:55] - slp - ERROR -- Engine run is terminating due to exception: 'loss'.
[2020-05-15 16:07:06] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-15 16:07:06] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-15 16:07:07] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-15 16:07:07] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-15 16:09:58] - slp - INFO -- Found 2196022 word vectors.
[2020-05-15 16:10:10] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f4b3bb08710>,), **{}) took: 182.57317733764648 sec
[2020-05-15 16:10:58] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 30
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-15 16:10:58] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-05-15 18:55:07] - slp - ERROR -- Current run is terminating due to exception: .
[2020-05-15 18:55:07] - slp - WARNING -- CTRL-C caught. Exiting gracefully...
[2020-05-15 18:55:12] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-05-15 18:55:12] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-05-15 18:55:12] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-05-15 18:55:12] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-05-15 18:57:40] - slp - INFO -- Found 2196022 word vectors.
[2020-05-15 18:57:51] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f9e0a465710>,), **{}) took: 158.64904737472534 sec
[2020-05-15 18:58:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 30
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-05-15 18:58:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-02 20:09:29] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-07-02 20:09:29] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-07-02 20:09:30] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-07-02 20:09:30] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-07-02 20:09:54] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-07-02 20:09:54] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-07-02 20:09:54] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-07-02 20:09:54] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-07-02 20:12:38] - slp - INFO -- Found 2196022 word vectors.
[2020-07-02 20:12:50] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f8a135ad470>,), **{}) took: 176.18988394737244 sec
[2020-07-02 20:14:09] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-02 20:14:09] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-02 20:14:09] - slp - ERROR -- Current run is terminating due to exception: too many values to unpack (expected 3).
[2020-07-02 20:14:09] - slp - ERROR -- Engine run is terminating due to exception: too many values to unpack (expected 3).
[2020-07-02 20:26:44] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-07-02 20:26:44] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-07-02 20:26:45] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-07-02 20:26:45] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-07-02 20:29:11] - slp - INFO -- Found 2196022 word vectors.
[2020-07-02 20:29:25] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f37560f2470>,), **{}) took: 160.03247475624084 sec
[2020-07-02 20:31:02] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-07-02 20:31:02] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-07-02 20:31:03] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-07-02 20:31:03] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-07-02 20:33:32] - slp - INFO -- Found 2196022 word vectors.
[2020-07-02 20:33:45] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7fcfdda07470>,), **{}) took: 162.69465017318726 sec
[2020-07-02 20:34:17] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-02 20:34:17] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-02 22:06:02] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-02 22:06:02] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-02 23:19:37] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-02 23:19:37] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-03 00:10:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-03 00:10:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-03 01:47:49] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-03 01:47:49] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-03 14:45:34] - slp - INFO -- PyTorch version 1.2.0 available.
[2020-07-03 14:45:34] - slp - INFO -- Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex .
[2020-07-03 14:45:35] - slp - INFO -- Cache: /data/embeddings/glove.840B.300d.p
[2020-07-03 14:45:35] - slp - INFO -- Indexing file /data/embeddings/glove.840B.300d.txt ...
[2020-07-03 14:48:08] - slp - INFO -- Found 2196022 word vectors.
[2020-07-03 14:48:09] - slp - INFO -- BENCHMARK: load(*(<slp.util.embeddings.EmbeddingsLoader object at 0x7f666a76c908>,), **{}) took: 154.0473027229309 sec
[2020-07-03 14:48:30] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-03 14:48:30] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-03 14:58:04] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-03 14:58:04] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-03 15:14:07] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-03 15:14:07] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-03 15:27:14] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-03 15:27:14] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
[2020-07-03 15:41:19] - slp - INFO -- Trainer configured to run experiment
	pretrained model: None None
	checkpoint directory: ../checkpoints
	patience: 10
	accumulation steps: 1
	non blocking: True
	retain graph: False
	device: cuda
	model dtype: torch.float32
	parallel: False
[2020-07-03 15:41:19] - slp - INFO -- Trainer will run for
model: HierAttNet(
  (sent_att_net): SentAttNet(
    (gru): GRU(600, 300, batch_first=True, bidirectional=True)
    (sent): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (fc): Linear(in_features=600, out_features=2, bias=True)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
  (word_att_net_text): WordAttNet(
    (gru): GRU(300, 300, batch_first=True, bidirectional=True)
    (word): Linear(in_features=600, out_features=600, bias=True)
    (context): Linear(in_features=600, out_features=1, bias=False)
    (lookup): Embedding(2196022, 300)
    (pack): PackSequence()
    (unpack): PadPackedSequence()
  )
)
optimizer: Adam (
Parameter Group 0
    amsgrad: False
    betas: (0.9, 0.999)
    eps: 1e-08
    lr: 0.0005
    weight_decay: 0
)
loss: CrossEntropyLoss()
